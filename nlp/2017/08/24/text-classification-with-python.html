<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Text Classification with Python | notebooks</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Text Classification with Python" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="If you are already familiar with what text classification is, you might want to jump to this part, or get the code here." />
<meta property="og:description" content="If you are already familiar with what text classification is, you might want to jump to this part, or get the code here." />
<link rel="canonical" href="https://joaorafaelm.github.io/notebook/nlp/2017/08/24/text-classification-with-python.html" />
<meta property="og:url" content="https://joaorafaelm.github.io/notebook/nlp/2017/08/24/text-classification-with-python.html" />
<meta property="og:site_name" content="notebooks" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-08-24T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://joaorafaelm.github.io/notebook/nlp/2017/08/24/text-classification-with-python.html","@type":"BlogPosting","headline":"Text Classification with Python","dateModified":"2017-08-24T00:00:00-05:00","datePublished":"2017-08-24T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://joaorafaelm.github.io/notebook/nlp/2017/08/24/text-classification-with-python.html"},"description":"If you are already familiar with what text classification is, you might want to jump to this part, or get the code here.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/notebook/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://joaorafaelm.github.io/notebook/feed.xml" title="notebooks" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-104187041-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-104187041-1');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/notebook/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/notebook/">notebooks</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/notebook/about/">About Me</a><a class="page-link" href="/notebook/search/">Search</a><a class="page-link" href="/notebook/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Text Classification with Python</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2017-08-24T00:00:00-05:00" itemprop="datePublished">
        Aug 24, 2017
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notebook/categories/#nlp">nlp</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#what-is-text-classification">What is Text Classification?</a></li>
<li class="toc-entry toc-h1"><a href="#getting-started">Getting started</a></li>
<li class="toc-entry toc-h1"><a href="#setting-up-the-environment">Setting up the environment</a></li>
<li class="toc-entry toc-h1"><a href="#gathering-the-data">Gathering the data</a></li>
<li class="toc-entry toc-h1"><a href="#extracting-features-from-the-dataset">Extracting features from the dataset</a></li>
<li class="toc-entry toc-h1"><a href="#testing-the-algorithms">Testing the algorithms</a></li>
<li class="toc-entry toc-h1"><a href="#conclusion">Conclusion</a></li>
<li class="toc-entry toc-h1"><a href="#further-reading">Further reading</a></li>
</ul><blockquote>
  <p>If you are already familiar with what text classification is, you might want to jump to <a href="#testing-the-algorithms">this part</a>, or get the code <a href="https://github.com/joaorafaelm/text-classification-python">here</a>.</p>
</blockquote>

<h1 id="what-is-text-classification">
<a class="anchor" href="#what-is-text-classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is Text Classification?</h1>
<p>Document or text classification is used to classify information, that is, assign a category to a text; it can be a document, a tweet, a simple message, an email, and so on.
In this article, I will show how you can classify retail products into categories. Although in this example the categories are structured in a hierarchy, to keep it simple I will consider all subcategories as top-level.</p>

<p><em>If you are looking for complex implementations of large scale hierarchical text classification, I will leave links to some really good papers and projects at the <a href="#conclusion">end</a> of this post.</em></p>

<h1 id="getting-started">
<a class="anchor" href="#getting-started" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting started</h1>
<p>Now, before you go any further, make sure you have installed <a href="https://www.python.org/downloads/">Python3+</a> and <a href="https://virtualenv.pypa.io/en/stable/">virtualenv</a> <em>(optional, but I highly recommend you to use it)</em>.</p>

<p>Let’s break down the problem into steps:</p>
<ul>
  <li><a href="#setting-up-the-environment">Setting up the environment</a></li>
  <li><a href="#gathering-the-data">Gathering the data</a></li>
  <li><a href="#extracting-features-from-the-dataset">Extracting features from the dataset</a></li>
  <li><a href="#testing-the-algorithms">Testing the algorithms</a></li>
</ul>

<h1 id="setting-up-the-environment">
<a class="anchor" href="#setting-up-the-environment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Setting up the environment</h1>
<p>The main packages used in this projects are: <a href="http://scikit-learn.org">sklearn</a>, <a href="http://www.nltk.org">nltk</a> and <a href="https://dataset.readthedocs.io/en/latest/">dataset</a>.
Due to the size of the data-set, it might take some time to clone/download the repository; NLTK data is also considerably big.
Run the following commands to setup the project structure and download the required packages:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Clone the repo</span>
git clone https://github.com/joaorafaelm/text-classification-python<span class="p">;</span>
<span class="nb">cd </span>text-classification-python<span class="p">;</span>

<span class="c"># Create virtualenv; skip this one if you dont have virtualenv.</span>
virtualenv venv <span class="o">&amp;&amp;</span> <span class="nb">source </span>venv/bin/activate<span class="p">;</span>

<span class="c"># Install all requirements</span>
pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt<span class="p">;</span>

<span class="c"># Download all data that NLTK uses</span>
python <span class="nt">-m</span> nltk.downloader all<span class="p">;</span>
</code></pre></div></div>

<h1 id="gathering-the-data">
<a class="anchor" href="#gathering-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gathering the data</h1>
<p>The dataset that will be used was created by <a href="https://en.wikipedia.org/wiki/Web_scraping">scraping</a> some products from Amazon. Scraping might be fine for projects where only a small amount of data is required, but it can be a really slow process since it is very simple for a server to detect a robot, unless you are rotating over a list of proxies, which can slow the process even more.</p>

<p>Using <a href="https://github.com/joaorafaelm/text-classification-python/blob/master/amazon_scrape.py">this script</a>, I downloaded information of over 22,000 products, organized into 42 top-level categories, and a total of 6233 subcategories. See the whole category tree structure <a href="https://github.com/joaorafaelm/text-classification-python/blob/master/category_tree.txt">here</a>.</p>

<p>Again, to keep it simple I will be using only 3 top-level categories: Automotive, Home &amp; Kitchen and Industrial &amp; Scientific. Including the subcategories, there are 36 categories in total.</p>

<p>To extract the data from database, run the command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># dump from db to dumps/all_products.json</span>
datafreeze .datafreeze.yaml<span class="p">;</span>
</code></pre></div></div>

<p>Inside the project you will also find a file called <a href="https://github.com/joaorafaelm/text-classification-python/blob/master/data_prep.py">data_prep.py</a>, in this file you can set the categories you want to use, the minimum amount of samples per category and the depth of a category. As I said before, only 3 categories are going to be used: <em>Home &amp; Kitchen, Industrial &amp; Scientific and Automotive</em>. I did not specify the depth of the subcategories, but I did specify 50 as the minimum amount of samples (is this case, products) per category.
To transform the data dumped from the database into this “filtered” data, just execute the file:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python data_prep.py
</code></pre></div></div>
<p>The script will create a new file called <strong>products.json</strong> at the root of the project, and print out the category tree structure. Change the value of the variables <code class="language-plaintext highlighter-rouge">default_depth</code>, <code class="language-plaintext highlighter-rouge">min_samples</code> and <code class="language-plaintext highlighter-rouge">domain</code> if you need more data.</p>

<h1 id="extracting-features-from-the-dataset">
<a class="anchor" href="#extracting-features-from-the-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Extracting features from the dataset</h1>
<p>In order to run machine learning algorithms, we need to transform the text into numerical vectors. <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag-of-words</a> is one of the most used models, it assigns a numerical value to a word, creating a list of numbers. It can also assign a value to a set of words, known as <a href="https://en.wikipedia.org/wiki/N-gram">N-gram</a>.</p>

<p>Scikit provides a vectorizer called <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">TfidfVectorizer</a> which transforms the text based on the bag-of-words/n-gram model, additionally, it computes term frequencies and evaluate each word using the <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf-idf</a> weighting scheme.</p>

<p>Counting terms frequencies might not be enough sometimes. Take the words ‘cars’ and ‘car’ for example, by only using <em>tf-idf</em>, they are considered different words. This problem can be solved using <a href="https://en.wikipedia.org/wiki/Stemming">Stemming</a> and/or <a href="https://en.wikipedia.org/wiki/Lemmatisation">Lemmatisation</a>. And there is where <a href="http://www.nltk.org">NLTK</a> comes into play.</p>

<p>NLTK offers some pretty useful tools for NLP. For this project I used it to perform <em>Lemmatisation</em> and <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">Part-of-speech tagging</a>.</p>

<p>With <em>Lemmatisation</em> we can group together the inflected forms of a word. For example, the words ‘walked’, ‘walks’ and ‘walking’, can be grouped into their base form, the verb ‘walk’. That is why we need to <em>POS tag</em> each word as a noun, verb, adverb, and so on.</p>

<p>It is also worth noting that some words despite the fact that they appear frequently, they do not really make any difference for classification, in fact they could even help misclassify a text. Words like ‘a’, ‘an’, ‘the’, ‘to’, ‘or’ etc, are known as <a href="https://en.wikipedia.org/wiki/Stop_words">stop-words</a>. These words can be ignored during the <a href="https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html">tokenization</a> process.</p>

<h1 id="testing-the-algorithms">
<a class="anchor" href="#testing-the-algorithms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Testing the algorithms</h1>
<p>Now that we have all the features and labels, it is time to train the classifiers. There are a number of algorithms you can use for this type of problem, for example: Multinomial Naive Bayes, Linear SVC, SGD Classifier, K-Neighbors Classifier, Random Forest Classifier.
Inside the file <a href="https://github.com/joaorafaelm/text-classification-python/blob/master/classify.py">classify.py</a> you can find an example using the SGDClassifier.
Run it yourself using the command:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python classify.py
</code></pre></div></div>
<p>It will print out the accuracy of each category, along with the <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a>.</p>

<p>Here is how it is implemented:
load the dataset, initiate WordNetLemmatizer and PerceptronTagger from NLTK.
As I was only interested in nouns, verbs, adverbs and adjectives, I created a lookup dict to quicken up the process. Although NLTK is great, its aim is not performance, so I also implemented python’s <a href="https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_Recently_Used_.28LRU.29">LRU Cache</a> for both lemmatize and tagger functions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load data
</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">'products.json'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'utf-8'</span><span class="p">))</span>

<span class="c1"># Initiate lemmatizer
</span><span class="n">wnl</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>

<span class="c1"># Load tagger pickle
</span><span class="n">tagger</span> <span class="o">=</span> <span class="n">PerceptronTagger</span><span class="p">()</span>

<span class="c1"># Lookup if tag is noun, verb, adverb or an adjective
</span><span class="n">tags</span> <span class="o">=</span> <span class="p">{</span><span class="s">'N'</span><span class="p">:</span> <span class="n">wn</span><span class="p">.</span><span class="n">NOUN</span><span class="p">,</span> <span class="s">'V'</span><span class="p">:</span> <span class="n">wn</span><span class="p">.</span><span class="n">VERB</span><span class="p">,</span> <span class="s">'R'</span><span class="p">:</span> <span class="n">wn</span><span class="p">.</span><span class="n">ADV</span><span class="p">,</span> <span class="s">'J'</span><span class="p">:</span> <span class="n">wn</span><span class="p">.</span><span class="n">ADJ</span><span class="p">}</span>

<span class="c1"># Memoization of POS tagging and Lemmatizer
</span><span class="n">lemmatize_mem</span> <span class="o">=</span> <span class="n">lru_cache</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="mi">10000</span><span class="p">)(</span><span class="n">wnl</span><span class="p">.</span><span class="n">lemmatize</span><span class="p">)</span>
<span class="n">tagger_mem</span> <span class="o">=</span> <span class="n">lru_cache</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="mi">10000</span><span class="p">)(</span><span class="n">tagger</span><span class="p">.</span><span class="n">tag</span><span class="p">)</span>
</code></pre></div></div>

<p>Next, the tokenizer function was created. It breaks the text into words and iterate over them, ignoring the stop-words and POS-tagging/Lemmatising the rest. This function will receive all documents from the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># POS tag sentences and lemmatize each word
</span><span class="k">def</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">wordpunct_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ENGLISH_STOP_WORDS</span><span class="p">:</span>
            <span class="n">tag</span> <span class="o">=</span> <span class="n">tagger_mem</span><span class="p">(</span><span class="nb">frozenset</span><span class="p">({</span><span class="n">token</span><span class="p">}))</span>
            <span class="k">yield</span> <span class="n">lemmatize_mem</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">tags</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">tag</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>  <span class="n">wn</span><span class="p">.</span><span class="n">NOUN</span><span class="p">))</span>

</code></pre></div></div>

<p>At last the pipeline is defined; the first step is to call TfidfVectorizer, with the tokenizer function preprocessing each document, and then pass through the SGDClassifier.
The classifier is trained and tested using <a href="http://statweb.stanford.edu/~tibs/sta306bfiles/cvwrong.pdf">10-fold Cross-Validation</a> provided by the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html">cross_val_predict</a> method from scikit-learn.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pipeline definition
</span><span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s">'vectorizer'</span><span class="p">,</span> <span class="n">TfidfVectorizer</span><span class="p">(</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="n">stop_words</span><span class="o">=</span><span class="n">ENGLISH_STOP_WORDS</span><span class="p">,</span>
        <span class="n">sublinear_tf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">min_df</span><span class="o">=</span><span class="mf">0.00009</span>
    <span class="p">)),</span>
    <span class="p">(</span><span class="s">'classifier'</span><span class="p">,</span> <span class="n">SGDClassifier</span><span class="p">(</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
    <span class="p">)),</span>
<span class="p">])</span>

<span class="c1"># Cross validate using k-fold
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">cross_val_predict</span><span class="p">(</span>
    <span class="n">pipeline</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'data'</span><span class="p">),</span>
    <span class="n">y</span><span class="o">=</span><span class="n">dataset</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'target'</span><span class="p">),</span>
    <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">20</span>
<span class="p">)</span>

<span class="c1"># Print out precison, recall and f1 scode.
</span><span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'target'</span><span class="p">),</span> <span class="n">y_pred</span><span class="p">,</span>
    <span class="n">target_names</span><span class="o">=</span><span class="n">dataset</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'target_names'</span><span class="p">),</span>
    <span class="n">digits</span><span class="o">=</span><span class="mi">3</span>
<span class="p">))</span>
</code></pre></div></div>
<p>And here are the accuracy results for each algorithm I tested (all algorithms were tested with their default parameters):</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Algorithms</th>
      <th style="text-align: right">Precision</th>
      <th style="text-align: right">Recall</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">SGDClassifier</td>
      <td style="text-align: right">0.975</td>
      <td style="text-align: right">0.975</td>
    </tr>
    <tr>
      <td style="text-align: center">LinearSVC</td>
      <td style="text-align: right">0.972</td>
      <td style="text-align: right">0.971</td>
    </tr>
    <tr>
      <td style="text-align: center">RandomForest</td>
      <td style="text-align: right">0.938</td>
      <td style="text-align: right">0.936</td>
    </tr>
    <tr>
      <td style="text-align: center">MultinomialNB</td>
      <td style="text-align: right">0.882</td>
      <td style="text-align: right">0.851</td>
    </tr>
  </tbody>
</table>

<p>The <em>precision</em> is the percentage of the test samples that were classified to the category and actually belonged to the category.</p>

<p>The <em>recall</em> is the percentage of all the test samples that originally belonged to the category and in the evaluation process were correctly classified to the category.</p>

<h1 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h1>
<p>As the category tree gets bigger, and you have more and more data to classify, you cannot use a model as simple as the one above (well, you can but its precision will be very low, not to mention the computational cost). Another important thing to notice, is how you structure the categories, in amazon category structure, a lot of subcategories are so confused that I doubt even humans could correctly classify products to them.
The full code of this post can be found <a href="https://github.com/joaorafaelm/text-classification-python">here</a>.</p>

<p>If you noticed something wrong, or you know something that can make the algorithms better, please do comment bellow. Thanks for reading!</p>

<h1 id="further-reading">
<a class="anchor" href="#further-reading" aria-hidden="true"><span class="octicon octicon-link"></span></a>Further reading</h1>
<ul>
  <li>
    <p><a href="https://monkeylearn.com/docs/article/classifier-statistics/">Classifier Statistics</a></p>
  </li>
  <li>
    <p><a href="http://ieeexplore.ieee.org/document/6522404/?reload=true">A Meta-Top-Down Method for Large-Scale Hierarchical Classification</a></p>
  </li>
  <li>
    <p><a href="https://link.springer.com/article/10.1007/s10618-010-0175-9">A survey of hierarchical classification across different application domains</a></p>
  </li>
  <li>
    <p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.130.4074&amp;rep=rep1&amp;type=pdf">Hierarchical Text Categorization and Its Application to Bioinformatics</a></p>
  </li>
  <li>
    <p><a href="https://link.springer.com/chapter/10.1007/978-3-540-73731-5_12">Comparing Several Approaches for Hierarchical Classification of Proteins with Decision Trees</a></p>
  </li>
  <li>
    <p><a href="https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/">Tokenizing Words and Sentences with NLTK</a></p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=OQQ-W_63UgQ">Natural Language Processing with Deep Learning</a></p>
  </li>
  <li>
    <p><a href="https://www.3pillarglobal.com/insights/document-classification-using-multinomial-naive-bayes-classifier">Document Classification using Multinomial Naive Bayes Classifier</a></p>
  </li>
</ul>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="joaorafaelm/notebook"
        issue-term="title"
        label="blogpost-comment"
        theme="github-dark"
        crossorigin="anonymous"
        async>
</script>
<a class="u-url" href="/notebook/nlp/2017/08/24/text-classification-with-python.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/notebook/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/notebook/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/notebook/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/joaorafaelm" title="joaorafaelm"><svg class="svg-icon grey"><use xlink:href="/notebook/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
