{
  
    
        "post0": {
            "title": "Incremental training with XGBoost",
            "content": "Install dependencies . pip install scikit-learn xgboost . Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1) Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1) Requirement already satisfied: numpy&gt;=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5) Requirement already satisfied: scipy&gt;=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1) . . Save your model after you train on the first batch. Then, on successive runs, provide the xgb.train method with the filepath of the saved model. . First, split the boston dataset into training and testing sets. Then split the training set into halves. Fit a model with the first half and get a score that will serve as a benchmark. Then fit two models with the second half; one model will have the additional parameter xgb_model. If passing in the extra parameter didn&#39;t make a difference, then we would expect their scores to be similar.. But, fortunately, the new model seems to perform much better than the first. . import xgboost as xgb from sklearn.model_selection import train_test_split from sklearn.datasets import load_boston from sklearn.metrics import mean_squared_error X = load_boston()[&#39;data&#39;] y = load_boston()[&#39;target&#39;] # split data into training and testing sets # then split training set in half X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0) X_train_1, X_train_2, y_train_1, y_train_2 = train_test_split( X_train, y_train, test_size=0.5, random_state=0 ) xg_train_1 = xgb.DMatrix(X_train_1, label=y_train_1) xg_train_2 = xgb.DMatrix(X_train_2, label=y_train_2) xg_test = xgb.DMatrix(X_test, label=y_test) params = {&#39;objective&#39;: &#39;reg:squarederror&#39;, &#39;verbose&#39;: False} model_1 = xgb.train(params, xg_train_1, 30) model_1.save_model(&#39;model_1.model&#39;) # ================= train two versions of the model =====================# model_2_v1 = xgb.train(params, xg_train_2, 30) model_2_v2 = xgb.train(params, xg_train_2, 30, xgb_model=&#39;model_1.model&#39;) print(mean_squared_error(model_1.predict(xg_test), y_test)) # benchmark print(mean_squared_error(model_2_v1.predict(xg_test), y_test)) # &quot;before&quot; print(mean_squared_error(model_2_v2.predict(xg_test), y_test)) # &quot;after&quot; . 21.988532050893138 39.677688213388755 23.092057209292484 .",
            "url": "https://joaorafaelm.github.io/notebook/xgboost/2021/05/30/_05_31_incremental_training_xgboost.html",
            "relUrl": "/xgboost/2021/05/30/_05_31_incremental_training_xgboost.html",
            "date": " 2021 May 30"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://joaorafaelm.github.io/notebook/jupyter/2021/05/30/_05_27_getting_started_with_freqtrade.html",
            "relUrl": "/jupyter/2021/05/30/_05_27_getting_started_with_freqtrade.html",
            "date": " 2021 May 30"
        }
        
    
  
    
        ,"post2": {
            "title": "Text Classification with Python",
            "content": "If you are already familiar with what text classification is, you might want to jump to this part, or get the code here. . What is Text Classification? . Document or text classification is used to classify information, that is, assign a category to a text; it can be a document, a tweet, a simple message, an email, and so on. In this article, I will show how you can classify retail products into categories. Although in this example the categories are structured in a hierarchy, to keep it simple I will consider all subcategories as top-level. . If you are looking for complex implementations of large scale hierarchical text classification, I will leave links to some really good papers and projects at the end of this post. . Getting started . Now, before you go any further, make sure you have installed Python3+ and virtualenv (optional, but I highly recommend you to use it). . Let’s break down the problem into steps: . Setting up the environment | Gathering the data | Extracting features from the dataset | Testing the algorithms | . Setting up the environment . The main packages used in this projects are: sklearn, nltk and dataset. Due to the size of the data-set, it might take some time to clone/download the repository; NLTK data is also considerably big. Run the following commands to setup the project structure and download the required packages: . # Clone the repo git clone https://github.com/joaorafaelm/text-classification-python; cd text-classification-python; # Create virtualenv; skip this one if you dont have virtualenv. virtualenv venv &amp;&amp; source venv/bin/activate; # Install all requirements pip install -r requirements.txt; # Download all data that NLTK uses python -m nltk.downloader all; . Gathering the data . The dataset that will be used was created by scraping some products from Amazon. Scraping might be fine for projects where only a small amount of data is required, but it can be a really slow process since it is very simple for a server to detect a robot, unless you are rotating over a list of proxies, which can slow the process even more. . Using this script, I downloaded information of over 22,000 products, organized into 42 top-level categories, and a total of 6233 subcategories. See the whole category tree structure here. . Again, to keep it simple I will be using only 3 top-level categories: Automotive, Home &amp; Kitchen and Industrial &amp; Scientific. Including the subcategories, there are 36 categories in total. . To extract the data from database, run the command: . # dump from db to dumps/all_products.json datafreeze .datafreeze.yaml; . Inside the project you will also find a file called data_prep.py, in this file you can set the categories you want to use, the minimum amount of samples per category and the depth of a category. As I said before, only 3 categories are going to be used: Home &amp; Kitchen, Industrial &amp; Scientific and Automotive. I did not specify the depth of the subcategories, but I did specify 50 as the minimum amount of samples (is this case, products) per category. To transform the data dumped from the database into this “filtered” data, just execute the file: . python data_prep.py . The script will create a new file called products.json at the root of the project, and print out the category tree structure. Change the value of the variables default_depth, min_samples and domain if you need more data. . Extracting features from the dataset . In order to run machine learning algorithms, we need to transform the text into numerical vectors. Bag-of-words is one of the most used models, it assigns a numerical value to a word, creating a list of numbers. It can also assign a value to a set of words, known as N-gram. . Scikit provides a vectorizer called TfidfVectorizer which transforms the text based on the bag-of-words/n-gram model, additionally, it computes term frequencies and evaluate each word using the tf-idf weighting scheme. . Counting terms frequencies might not be enough sometimes. Take the words ‘cars’ and ‘car’ for example, by only using tf-idf, they are considered different words. This problem can be solved using Stemming and/or Lemmatisation. And there is where NLTK comes into play. . NLTK offers some pretty useful tools for NLP. For this project I used it to perform Lemmatisation and Part-of-speech tagging. . With Lemmatisation we can group together the inflected forms of a word. For example, the words ‘walked’, ‘walks’ and ‘walking’, can be grouped into their base form, the verb ‘walk’. That is why we need to POS tag each word as a noun, verb, adverb, and so on. . It is also worth noting that some words despite the fact that they appear frequently, they do not really make any difference for classification, in fact they could even help misclassify a text. Words like ‘a’, ‘an’, ‘the’, ‘to’, ‘or’ etc, are known as stop-words. These words can be ignored during the tokenization process. . Testing the algorithms . Now that we have all the features and labels, it is time to train the classifiers. There are a number of algorithms you can use for this type of problem, for example: Multinomial Naive Bayes, Linear SVC, SGD Classifier, K-Neighbors Classifier, Random Forest Classifier. Inside the file classify.py you can find an example using the SGDClassifier. Run it yourself using the command: . python classify.py . It will print out the accuracy of each category, along with the confusion matrix. . Here is how it is implemented: load the dataset, initiate WordNetLemmatizer and PerceptronTagger from NLTK. As I was only interested in nouns, verbs, adverbs and adjectives, I created a lookup dict to quicken up the process. Although NLTK is great, its aim is not performance, so I also implemented python’s LRU Cache for both lemmatize and tagger functions. . # Load data dataset = json.load(open(&#39;products.json&#39;, encoding=&#39;utf-8&#39;)) # Initiate lemmatizer wnl = WordNetLemmatizer() # Load tagger pickle tagger = PerceptronTagger() # Lookup if tag is noun, verb, adverb or an adjective tags = {&#39;N&#39;: wn.NOUN, &#39;V&#39;: wn.VERB, &#39;R&#39;: wn.ADV, &#39;J&#39;: wn.ADJ} # Memoization of POS tagging and Lemmatizer lemmatize_mem = lru_cache(maxsize=10000)(wnl.lemmatize) tagger_mem = lru_cache(maxsize=10000)(tagger.tag) . Next, the tokenizer function was created. It breaks the text into words and iterate over them, ignoring the stop-words and POS-tagging/Lemmatising the rest. This function will receive all documents from the dataset. . # POS tag sentences and lemmatize each word def tokenizer(text): for token in wordpunct_tokenize(text): if token not in ENGLISH_STOP_WORDS: tag = tagger_mem(frozenset({token})) yield lemmatize_mem(token, tags.get(tag[0][1], wn.NOUN)) . At last the pipeline is defined; the first step is to call TfidfVectorizer, with the tokenizer function preprocessing each document, and then pass through the SGDClassifier. The classifier is trained and tested using 10-fold Cross-Validation provided by the cross_val_predict method from scikit-learn. . # Pipeline definition pipeline = Pipeline([ (&#39;vectorizer&#39;, TfidfVectorizer( tokenizer=tokenizer, ngram_range=(1, 2), stop_words=ENGLISH_STOP_WORDS, sublinear_tf=True, min_df=0.00009 )), (&#39;classifier&#39;, SGDClassifier( alpha=1e-4, n_jobs=-1 )), ]) # Cross validate using k-fold y_pred = cross_val_predict( pipeline, dataset.get(&#39;data&#39;), y=dataset.get(&#39;target&#39;), cv=10, n_jobs=-1, verbose=20 ) # Print out precison, recall and f1 scode. print(classification_report( dataset.get(&#39;target&#39;), y_pred, target_names=dataset.get(&#39;target_names&#39;), digits=3 )) . And here are the accuracy results for each algorithm I tested (all algorithms were tested with their default parameters): . Algorithms Precision Recall . SGDClassifier | 0.975 | 0.975 | . LinearSVC | 0.972 | 0.971 | . RandomForest | 0.938 | 0.936 | . MultinomialNB | 0.882 | 0.851 | . The precision is the percentage of the test samples that were classified to the category and actually belonged to the category. . The recall is the percentage of all the test samples that originally belonged to the category and in the evaluation process were correctly classified to the category. . Conclusion . As the category tree gets bigger, and you have more and more data to classify, you cannot use a model as simple as the one above (well, you can but its precision will be very low, not to mention the computational cost). Another important thing to notice, is how you structure the categories, in amazon category structure, a lot of subcategories are so confused that I doubt even humans could correctly classify products to them. The full code of this post can be found here. . If you noticed something wrong, or you know something that can make the algorithms better, please do comment bellow. Thanks for reading! . Further reading . Classifier Statistics . | A Meta-Top-Down Method for Large-Scale Hierarchical Classification . | A survey of hierarchical classification across different application domains . | Hierarchical Text Categorization and Its Application to Bioinformatics . | Comparing Several Approaches for Hierarchical Classification of Proteins with Decision Trees . | Tokenizing Words and Sentences with NLTK . | Natural Language Processing with Deep Learning . | Document Classification using Multinomial Naive Bayes Classifier . | .",
            "url": "https://joaorafaelm.github.io/notebook/nlp/2017/08/24/text-classification-with-python.html",
            "relUrl": "/nlp/2017/08/24/text-classification-with-python.html",
            "date": " 2017 Aug 24"
        }
        
    
  
    
        ,"post3": {
            "title": "GraphQL and Django in 5 minutes",
            "content": "TL;DR Jump to the coding part or get the code here. . What is GraphQL? . GraphQL query is a string that is sent to a server to be interpreted and fulfilled, which then returns JSON back to the client. It was created by Facebook in 2012 and the first specification draft was made public in 2015. . In this tutorial I will cover the basics of working with GraphQL and Django. . Getting started . Before creating the project and all, make sure you have virtualenv installed, so that the packages used in this tutorial won’t be installed system-wide. . # Clone the repo git clone https://github.com/joaorafaelm/graphql-django-example; cd graphql-django-example; # Create virtualenv virtualenv venv &amp;&amp; source venv/bin/activate; # Install django and graphene pip install -r requirements.txt; # Setup db python manage.py migrate; . Run python manage.py loaddata books.json to populate the db, or run python manage.py createsuperuser and then add some data using the admin interface. . Models and GraphQL Schema . This example is going to use the models Author, Book and Publisher. . # bookstore/store/models.py from django.db import models class Publisher(models.Model): name = models.CharField(max_length=30) website = models.URLField() def __str__(self): return self.name class Author(models.Model): first_name = models.CharField(max_length=30) last_name = models.CharField(max_length=40) email = models.EmailField() def __str__(self): return &#39;%s %s&#39; % (self.first_name, self.last_name) class Book(models.Model): title = models.CharField(max_length=100) authors = models.ManyToManyField(Author) publisher = models.ForeignKey(Publisher) publication_date = models.DateField() def __str__(self): return self.title . After creating the models, the Schema should be created, which will be used to serve the API. . At the time of writing this post, graphene-django version is 1.3 and it does not handle ManyToMany fields properly, that is why the resolve_authors method was added. This issue has been resolved for the next release. . # bookstore/schema.py import graphene from graphene_django.types import DjangoObjectType from graphene_django.debug import DjangoDebug from bookstore.store.models import Author, Book, Publisher class AuthorType(DjangoObjectType): class Meta: model = Author class BookType(DjangoObjectType): authors = graphene.List(AuthorType) # Many To Many fix until next release. # https://github.com/graphql-python/graphene-django/issues/155 @graphene.resolve_only_args def resolve_authors(self): return self.authors.all() class Meta: model = Book class PublisherType(DjangoObjectType): class Meta: model = Publisher class Query(graphene.ObjectType): all_authors = graphene.List(AuthorType) all_books = graphene.List(BookType) all_publishers = graphene.List(PublisherType) # Debug field (rawSql, parameters etc). debug = graphene.Field(DjangoDebug, name=&#39;__debug&#39;) def resolve_all_authors(self, args, context, info): return Author.objects.all() def resolve_all_books(self, args, context, info): return Book.objects.select_related(&#39;publisher&#39;).all() def resolve_all_publishers(self, args, context, info): return Publisher.objects.all() schema = graphene.Schema(query=Query) . Last but not least, the GraphQL URL must be added into the urls.py file. . # bookstore/urls.py from django.conf.urls import url from django.contrib import admin from graphene_django.views import GraphQLView from bookstore.schema import schema urlpatterns = [ url(r&#39;^admin/&#39;, admin.site.urls), url(r&#39;^graphql&#39;, GraphQLView.as_view(graphiql=True, schema=schema)), ] . You can now run python manage.py runserver and start using the API at http://localhost:8000/graphql. . Querying and debugging . GraphiQL provides a graphical interactive in-browser GraphQL IDE, including some features such as syntax highlighting, real-time error reporting, automatic query completion etc. . I will show some query examples, but you can learn more about querying at graphql.org/learn/queries/. . Given the following query, we can retrieve all books registered along with their authors. . { allBooks { title, authors { firstName, lastName } } } . And the response… . { &quot;data&quot;: { &quot;allBooks&quot;: [ { &quot;title&quot;: &quot;Resurrection&quot;, &quot;authors&quot;: [ { &quot;firstName&quot;: &quot;Leo&quot;, &quot;lastName&quot;: &quot;Tolstoy&quot; } ] }, { &quot;title&quot;: &quot;Childhood&quot;, &quot;authors&quot;: [ { &quot;firstName&quot;: &quot;Leo&quot;, &quot;lastName&quot;: &quot;Tolstoy&quot; } ] } ] } } . Using the __debug field you can get information about the actual SQL query. . { allAuthors {lastName} __debug { sql {rawSql, duration} } } . Response: . { &quot;data&quot;: { &quot;allAuthors&quot;: [ { &quot;lastName&quot;: &quot;King&quot; }, { &quot;lastName&quot;: &quot;Tolstoy&quot; }, { &quot;lastName&quot;: &quot;Gaiman&quot; }, { &quot;lastName&quot;: &quot;Pratchett&quot; } ], &quot;__debug&quot;: { &quot;sql&quot;: [ { &quot;rawSql&quot;: &quot;SELECT &quot;store_author &quot;. &quot;id &quot;, &quot;store_author &quot;. &quot;first_name &quot;, &quot;store_author &quot;. &quot;last_name &quot;, &quot;store_author &quot;. &quot;email &quot; FROM &quot;store_author &quot;&quot;, &quot;duration&quot;: 0.0009260177612304688 } ] } } } . All this code is on my Github. Please do fork it and make pull requests regarding any issues or improvements you may have with my code. .",
            "url": "https://joaorafaelm.github.io/notebook/graphql/django/2017/08/05/graphql-and-django-in-5-minutes.html",
            "relUrl": "/graphql/django/2017/08/05/graphql-and-django-in-5-minutes.html",
            "date": " 2017 Aug 05"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Hi 👋 . I’m a remote software developer, currently living in Brazil. You can find some open source projects I’ve been working on in my github. Reach me at [     joaoraf@me.com,     linkedin,     @jrmol, ]. .",
          "url": "https://joaorafaelm.github.io/notebook/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://joaorafaelm.github.io/notebook/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
