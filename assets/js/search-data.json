{
  
    
        "post0": {
            "title": "Deploying a XGBoost model",
            "content": "import pandas as pd import numpy as np import xgboost as xgb import re from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error np.random.seed(42) . Load data and build a model . X, y = fetch_openml(&quot;titanic&quot;, version=1, as_frame=True, return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2) . X_y_train = xgb.DMatrix(data=X_train[[&quot;pclass&quot;, &quot;age&quot;, &quot;fare&quot;, &quot;sibsp&quot;, &quot;parch&quot;]], label=y_train.astype(int)) X_test = xgb.DMatrix(data=X_test[[&quot;pclass&quot;, &quot;age&quot;, &quot;fare&quot;, &quot;sibsp&quot;, &quot;parch&quot;]]) . X_train[[&quot;pclass&quot;, &quot;age&quot;, &quot;fare&quot;, &quot;sibsp&quot;, &quot;parch&quot;]].head() . pclass age fare sibsp parch . 662 3.0 | 40.0 | 7.2250 | 0.0 | 0.0 | . 164 1.0 | 35.0 | 26.5500 | 0.0 | 0.0 | . 871 3.0 | NaN | 7.7500 | 0.0 | 0.0 | . 1298 3.0 | 36.0 | 9.5000 | 0.0 | 0.0 | . 1004 3.0 | NaN | 7.7875 | 0.0 | 0.0 | . params = { &quot;base_score&quot;: np.mean(y_train.astype(int)), &quot;eta&quot;: 0.1, &quot;max_depth&quot;: 3, &quot;gamma&quot;: 3, &quot;objective&quot;: &quot;reg:squarederror&quot;, &quot;eval_metric&quot;: &quot;mae&quot; } model = xgb.train( params=params, dtrain=X_y_train, num_boost_round=3 ) . Visualization of model . xgb.to_graphviz(booster = model, num_trees=0) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 0 fare&lt;15.1729002 1 age&lt;16.5 0&#45;&gt;1 yes, missing 2 pclass&lt;2.5 0&#45;&gt;2 no 3 leaf=0.0223644935 1&#45;&gt;3 yes 4 leaf=&#45;0.0156309232 1&#45;&gt;4 no, missing 5 parch&lt;0.5 2&#45;&gt;5 yes, missing 6 fare&lt;23.3500004 2&#45;&gt;6 no 11 leaf=0.0153174726 5&#45;&gt;11 yes, missing 12 leaf=0.03650124 5&#45;&gt;12 no 13 leaf=0.00960917864 6&#45;&gt;13 yes 14 leaf=&#45;0.0235449504 6&#45;&gt;14 no, missing xgb.to_graphviz(booster = model, num_trees=1) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 0 fare&lt;15.1729002 1 age&lt;16.5 0&#45;&gt;1 yes, missing 2 pclass&lt;2.5 0&#45;&gt;2 no 3 leaf=0.0201979335 1&#45;&gt;3 yes 4 leaf=&#45;0.0140708359 1&#45;&gt;4 no, missing 5 leaf=0.0204214789 2&#45;&gt;5 yes, missing 6 fare&lt;23.3500004 2&#45;&gt;6 no 13 leaf=0.00866124686 6&#45;&gt;13 yes 14 leaf=&#45;0.0212272462 6&#45;&gt;14 no, missing xgb.to_graphviz(booster = model, num_trees=2) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 0 fare&lt;15.1729002 1 leaf=&#45;0.010894442 0&#45;&gt;1 yes, missing 2 pclass&lt;2.5 0&#45;&gt;2 no 5 leaf=0.0183849707 2&#45;&gt;5 yes, missing 6 leaf=&#45;0.00472340686 2&#45;&gt;6 no print(&quot; n&quot;.join(model.get_dump())) . 0:[fare&lt;15.1729002] yes=1,no=2,missing=1 1:[age&lt;16.5] yes=3,no=4,missing=4 3:leaf=0.0223644935 4:leaf=-0.0156309232 2:[pclass&lt;2.5] yes=5,no=6,missing=5 5:[parch&lt;0.5] yes=11,no=12,missing=11 11:leaf=0.0153174726 12:leaf=0.03650124 6:[fare&lt;23.3500004] yes=13,no=14,missing=14 13:leaf=0.00960917864 14:leaf=-0.0235449504 0:[fare&lt;15.1729002] yes=1,no=2,missing=1 1:[age&lt;16.5] yes=3,no=4,missing=4 3:leaf=0.0201979335 4:leaf=-0.0140708359 2:[pclass&lt;2.5] yes=5,no=6,missing=5 5:leaf=0.0204214789 6:[fare&lt;23.3500004] yes=13,no=14,missing=14 13:leaf=0.00866124686 14:leaf=-0.0212272462 0:[fare&lt;15.1729002] yes=1,no=2,missing=1 1:leaf=-0.010894442 2:[pclass&lt;2.5] yes=5,no=6,missing=5 5:leaf=0.0183849707 6:leaf=-0.00472340686 . Convert dump string to a .py file . def string_parser(s): if len(re.findall(r&quot;:leaf=&quot;, s)) == 0: out = re.findall(r&quot;[ w.-]+&quot;, s) tabs = re.findall(r&quot;[ t]+&quot;, s) if (out[4] == out[8]): missing_value_handling = (&quot; or np.isnan(x[&#39;&quot; + out[1] + &quot;&#39;]) &quot;) else: missing_value_handling = &quot;&quot; if len(tabs) &gt; 0: return (re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; if state == &#39; + out[0] + &#39;: n&#39; + re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; state = (&#39; + out[4] + &#39; if &#39; + &quot;x[&#39;&quot; + out[1] +&quot;&#39;]&lt;&quot; + out[2] + missing_value_handling + &#39; else &#39; + out[6] + &#39;) n&#39; ) else: return (&#39; if state == &#39; + out[0] + &#39;: n&#39; + &#39; state = (&#39; + out[4] + &#39; if &#39; + &quot;x[&#39;&quot; + out[1] +&quot;&#39;]&lt;&quot; + out[2] + missing_value_handling + &#39; else &#39; + out[6] + &#39;) n&#39; ) else: out = re.findall(r&quot;[ d.-]+&quot;, s) return (re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; if state == &#39; + out[0] + &#39;: n &#39; + re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; return &#39; + out[1] + &#39; n&#39;) def tree_parser(tree, i): if i == 0: return (&#39; if num_booster == 0: n state = 0 n&#39; + &quot;&quot;.join([string_parser(tree.split(&#39; n&#39;)[i]) for i in range(len(tree.split(&#39; n&#39;))-1)])) else: return (&#39; elif num_booster == &#39;+str(i)+&#39;: n state = 0 n&#39; + &quot;&quot;.join([string_parser(tree.split(&#39; n&#39;)[i]) for i in range(len(tree.split(&#39; n&#39;))-1)])) def model_to_py(base_score, model, out_file): trees = model.get_dump() result = [&quot;import numpy as np n n&quot; +&quot;def xgb_tree(x, num_booster): n&quot;] for i in range(len(trees)): result.append(tree_parser(trees[i], i)) with open(out_file, &#39;w&#39;) as the_file: the_file.write(&quot;&quot;.join(result) + &quot; ndef xgb_predict(x): n predict = &quot; + str(base_score) + &quot; n&quot; + &quot;# initialize prediction with base score n&quot; + &quot; for i in range(&quot; + str(len(trees)) + &quot;): n predict = predict + xgb_tree(x, i)&quot; + &quot; n return predict&quot;) model_to_py(params[&#39;base_score&#39;], model, &#39;xgb_model.py&#39;) . Prediction using dump file . import xgb_model passenger_data_1 = {&#39;pclass&#39;:3, &#39;age&#39;:np.nan, &#39;sibsp&#39;:0, &#39;parch&#39;:0, &#39;fare&#39;:7.8958} passenger_data_2 = {&#39;pclass&#39;:1, &#39;age&#39;:46, &#39;sibsp&#39;:0, &#39;parch&#39;:0, &#39;fare&#39;:26} print(xgb_model.xgb_predict(passenger_data_1)) print(xgb_model.xgb_predict(passenger_data_2)) . 0.34144773395253103 0.43616785725253104 .",
            "url": "https://joaorafaelm.github.io/notebook/xgboost/2021/06/05/_deploying_xgboost_model.html",
            "relUrl": "/xgboost/2021/06/05/_deploying_xgboost_model.html",
            "date": " 2021 Jun 05"
        }
        
    
  
    
        ,"post1": {
            "title": "Incremental training with XGBoost",
            "content": "Install dependencies . pip install scikit-learn xgboost . Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1) Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1) Requirement already satisfied: numpy&gt;=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5) Requirement already satisfied: scipy&gt;=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1) . . Save your model after you train on the first batch. Then, on successive runs, provide the xgb.train method with the filepath of the saved model. . First, split the boston dataset into training and testing sets. Then split the training set into halves. Fit a model with the first half and get a score that will serve as a benchmark. Then fit two models with the second half; one model will have the additional parameter xgb_model. If passing in the extra parameter didn&#39;t make a difference, then we would expect their scores to be similar.. But, fortunately, the new model seems to perform much better than the first. . import xgboost as xgb from sklearn.model_selection import train_test_split from sklearn.datasets import load_boston from sklearn.metrics import mean_squared_error X = load_boston()[&#39;data&#39;] y = load_boston()[&#39;target&#39;] # split data into training and testing sets # then split training set in half X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0) X_train_1, X_train_2, y_train_1, y_train_2 = train_test_split( X_train, y_train, test_size=0.5, random_state=0 ) xg_train_1 = xgb.DMatrix(X_train_1, label=y_train_1) xg_train_2 = xgb.DMatrix(X_train_2, label=y_train_2) xg_test = xgb.DMatrix(X_test, label=y_test) params = {&#39;objective&#39;: &#39;reg:squarederror&#39;, &#39;verbose&#39;: False} model_1 = xgb.train(params, xg_train_1, 30) model_1.save_model(&#39;model_1.model&#39;) # ================= train two versions of the model =====================# model_2_v1 = xgb.train(params, xg_train_2, 30) model_2_v2 = xgb.train(params, xg_train_2, 30, xgb_model=&#39;model_1.model&#39;) print(mean_squared_error(model_1.predict(xg_test), y_test)) # benchmark print(mean_squared_error(model_2_v1.predict(xg_test), y_test)) # &quot;before&quot; print(mean_squared_error(model_2_v2.predict(xg_test), y_test)) # &quot;after&quot; . 21.988532050893138 39.677688213388755 23.092057209292484 .",
            "url": "https://joaorafaelm.github.io/notebook/xgboost/2021/05/31/incremental-training-xgboost.html",
            "relUrl": "/xgboost/2021/05/31/incremental-training-xgboost.html",
            "date": " 2021 May 31"
        }
        
    
  
    
        ,"post2": {
            "title": "Text Classification with Python",
            "content": "If you are already familiar with what text classification is, you might want to jump to this part, or get the code here. . What is Text Classification? . Document or text classification is used to classify information, that is, assign a category to a text; it can be a document, a tweet, a simple message, an email, and so on. In this article, I will show how you can classify retail products into categories. Although in this example the categories are structured in a hierarchy, to keep it simple I will consider all subcategories as top-level. . If you are looking for complex implementations of large scale hierarchical text classification, I will leave links to some really good papers and projects at the end of this post. . Getting started . Now, before you go any further, make sure you have installed Python3+ and virtualenv (optional, but I highly recommend you to use it). . Let’s break down the problem into steps: . Setting up the environment | Gathering the data | Extracting features from the dataset | Testing the algorithms | . Setting up the environment . The main packages used in this projects are: sklearn, nltk and dataset. Due to the size of the data-set, it might take some time to clone/download the repository; NLTK data is also considerably big. Run the following commands to setup the project structure and download the required packages: . # Clone the repo git clone https://github.com/joaorafaelm/text-classification-python; cd text-classification-python; # Create virtualenv; skip this one if you dont have virtualenv. virtualenv venv &amp;&amp; source venv/bin/activate; # Install all requirements pip install -r requirements.txt; # Download all data that NLTK uses python -m nltk.downloader all; . Gathering the data . The dataset that will be used was created by scraping some products from Amazon. Scraping might be fine for projects where only a small amount of data is required, but it can be a really slow process since it is very simple for a server to detect a robot, unless you are rotating over a list of proxies, which can slow the process even more. . Using this script, I downloaded information of over 22,000 products, organized into 42 top-level categories, and a total of 6233 subcategories. See the whole category tree structure here. . Again, to keep it simple I will be using only 3 top-level categories: Automotive, Home &amp; Kitchen and Industrial &amp; Scientific. Including the subcategories, there are 36 categories in total. . To extract the data from database, run the command: . # dump from db to dumps/all_products.json datafreeze .datafreeze.yaml; . Inside the project you will also find a file called data_prep.py, in this file you can set the categories you want to use, the minimum amount of samples per category and the depth of a category. As I said before, only 3 categories are going to be used: Home &amp; Kitchen, Industrial &amp; Scientific and Automotive. I did not specify the depth of the subcategories, but I did specify 50 as the minimum amount of samples (is this case, products) per category. To transform the data dumped from the database into this “filtered” data, just execute the file: . python data_prep.py . The script will create a new file called products.json at the root of the project, and print out the category tree structure. Change the value of the variables default_depth, min_samples and domain if you need more data. . Extracting features from the dataset . In order to run machine learning algorithms, we need to transform the text into numerical vectors. Bag-of-words is one of the most used models, it assigns a numerical value to a word, creating a list of numbers. It can also assign a value to a set of words, known as N-gram. . Scikit provides a vectorizer called TfidfVectorizer which transforms the text based on the bag-of-words/n-gram model, additionally, it computes term frequencies and evaluate each word using the tf-idf weighting scheme. . Counting terms frequencies might not be enough sometimes. Take the words ‘cars’ and ‘car’ for example, by only using tf-idf, they are considered different words. This problem can be solved using Stemming and/or Lemmatisation. And there is where NLTK comes into play. . NLTK offers some pretty useful tools for NLP. For this project I used it to perform Lemmatisation and Part-of-speech tagging. . With Lemmatisation we can group together the inflected forms of a word. For example, the words ‘walked’, ‘walks’ and ‘walking’, can be grouped into their base form, the verb ‘walk’. That is why we need to POS tag each word as a noun, verb, adverb, and so on. . It is also worth noting that some words despite the fact that they appear frequently, they do not really make any difference for classification, in fact they could even help misclassify a text. Words like ‘a’, ‘an’, ‘the’, ‘to’, ‘or’ etc, are known as stop-words. These words can be ignored during the tokenization process. . Testing the algorithms . Now that we have all the features and labels, it is time to train the classifiers. There are a number of algorithms you can use for this type of problem, for example: Multinomial Naive Bayes, Linear SVC, SGD Classifier, K-Neighbors Classifier, Random Forest Classifier. Inside the file classify.py you can find an example using the SGDClassifier. Run it yourself using the command: . python classify.py . It will print out the accuracy of each category, along with the confusion matrix. . Here is how it is implemented: load the dataset, initiate WordNetLemmatizer and PerceptronTagger from NLTK. As I was only interested in nouns, verbs, adverbs and adjectives, I created a lookup dict to quicken up the process. Although NLTK is great, its aim is not performance, so I also implemented python’s LRU Cache for both lemmatize and tagger functions. . # Load data dataset = json.load(open(&#39;products.json&#39;, encoding=&#39;utf-8&#39;)) # Initiate lemmatizer wnl = WordNetLemmatizer() # Load tagger pickle tagger = PerceptronTagger() # Lookup if tag is noun, verb, adverb or an adjective tags = {&#39;N&#39;: wn.NOUN, &#39;V&#39;: wn.VERB, &#39;R&#39;: wn.ADV, &#39;J&#39;: wn.ADJ} # Memoization of POS tagging and Lemmatizer lemmatize_mem = lru_cache(maxsize=10000)(wnl.lemmatize) tagger_mem = lru_cache(maxsize=10000)(tagger.tag) . Next, the tokenizer function was created. It breaks the text into words and iterate over them, ignoring the stop-words and POS-tagging/Lemmatising the rest. This function will receive all documents from the dataset. . # POS tag sentences and lemmatize each word def tokenizer(text): for token in wordpunct_tokenize(text): if token not in ENGLISH_STOP_WORDS: tag = tagger_mem(frozenset({token})) yield lemmatize_mem(token, tags.get(tag[0][1], wn.NOUN)) . At last the pipeline is defined; the first step is to call TfidfVectorizer, with the tokenizer function preprocessing each document, and then pass through the SGDClassifier. The classifier is trained and tested using 10-fold Cross-Validation provided by the cross_val_predict method from scikit-learn. . # Pipeline definition pipeline = Pipeline([ (&#39;vectorizer&#39;, TfidfVectorizer( tokenizer=tokenizer, ngram_range=(1, 2), stop_words=ENGLISH_STOP_WORDS, sublinear_tf=True, min_df=0.00009 )), (&#39;classifier&#39;, SGDClassifier( alpha=1e-4, n_jobs=-1 )), ]) # Cross validate using k-fold y_pred = cross_val_predict( pipeline, dataset.get(&#39;data&#39;), y=dataset.get(&#39;target&#39;), cv=10, n_jobs=-1, verbose=20 ) # Print out precison, recall and f1 scode. print(classification_report( dataset.get(&#39;target&#39;), y_pred, target_names=dataset.get(&#39;target_names&#39;), digits=3 )) . And here are the accuracy results for each algorithm I tested (all algorithms were tested with their default parameters): . Algorithms Precision Recall . SGDClassifier | 0.975 | 0.975 | . LinearSVC | 0.972 | 0.971 | . RandomForest | 0.938 | 0.936 | . MultinomialNB | 0.882 | 0.851 | . The precision is the percentage of the test samples that were classified to the category and actually belonged to the category. . The recall is the percentage of all the test samples that originally belonged to the category and in the evaluation process were correctly classified to the category. . Conclusion . As the category tree gets bigger, and you have more and more data to classify, you cannot use a model as simple as the one above (well, you can but its precision will be very low, not to mention the computational cost). Another important thing to notice, is how you structure the categories, in amazon category structure, a lot of subcategories are so confused that I doubt even humans could correctly classify products to them. The full code of this post can be found here. . If you noticed something wrong, or you know something that can make the algorithms better, please do comment bellow. Thanks for reading! . Further reading . Classifier Statistics . | A Meta-Top-Down Method for Large-Scale Hierarchical Classification . | A survey of hierarchical classification across different application domains . | Hierarchical Text Categorization and Its Application to Bioinformatics . | Comparing Several Approaches for Hierarchical Classification of Proteins with Decision Trees . | Tokenizing Words and Sentences with NLTK . | Natural Language Processing with Deep Learning . | Document Classification using Multinomial Naive Bayes Classifier . | .",
            "url": "https://joaorafaelm.github.io/notebook/nlp/2017/08/24/text-classification-with-python.html",
            "relUrl": "/nlp/2017/08/24/text-classification-with-python.html",
            "date": " 2017 Aug 24"
        }
        
    
  
    
        ,"post3": {
            "title": "GraphQL and Django in 5 minutes",
            "content": "TL;DR Jump to the coding part or get the code here. . What is GraphQL? . GraphQL query is a string that is sent to a server to be interpreted and fulfilled, which then returns JSON back to the client. It was created by Facebook in 2012 and the first specification draft was made public in 2015. . In this tutorial I will cover the basics of working with GraphQL and Django. . Getting started . Before creating the project and all, make sure you have virtualenv installed, so that the packages used in this tutorial won’t be installed system-wide. . # Clone the repo git clone https://github.com/joaorafaelm/graphql-django-example; cd graphql-django-example; # Create virtualenv virtualenv venv &amp;&amp; source venv/bin/activate; # Install django and graphene pip install -r requirements.txt; # Setup db python manage.py migrate; . Run python manage.py loaddata books.json to populate the db, or run python manage.py createsuperuser and then add some data using the admin interface. . Models and GraphQL Schema . This example is going to use the models Author, Book and Publisher. . # bookstore/store/models.py from django.db import models class Publisher(models.Model): name = models.CharField(max_length=30) website = models.URLField() def __str__(self): return self.name class Author(models.Model): first_name = models.CharField(max_length=30) last_name = models.CharField(max_length=40) email = models.EmailField() def __str__(self): return &#39;%s %s&#39; % (self.first_name, self.last_name) class Book(models.Model): title = models.CharField(max_length=100) authors = models.ManyToManyField(Author) publisher = models.ForeignKey(Publisher) publication_date = models.DateField() def __str__(self): return self.title . After creating the models, the Schema should be created, which will be used to serve the API. . At the time of writing this post, graphene-django version is 1.3 and it does not handle ManyToMany fields properly, that is why the resolve_authors method was added. This issue has been resolved for the next release. . # bookstore/schema.py import graphene from graphene_django.types import DjangoObjectType from graphene_django.debug import DjangoDebug from bookstore.store.models import Author, Book, Publisher class AuthorType(DjangoObjectType): class Meta: model = Author class BookType(DjangoObjectType): authors = graphene.List(AuthorType) # Many To Many fix until next release. # https://github.com/graphql-python/graphene-django/issues/155 @graphene.resolve_only_args def resolve_authors(self): return self.authors.all() class Meta: model = Book class PublisherType(DjangoObjectType): class Meta: model = Publisher class Query(graphene.ObjectType): all_authors = graphene.List(AuthorType) all_books = graphene.List(BookType) all_publishers = graphene.List(PublisherType) # Debug field (rawSql, parameters etc). debug = graphene.Field(DjangoDebug, name=&#39;__debug&#39;) def resolve_all_authors(self, args, context, info): return Author.objects.all() def resolve_all_books(self, args, context, info): return Book.objects.select_related(&#39;publisher&#39;).all() def resolve_all_publishers(self, args, context, info): return Publisher.objects.all() schema = graphene.Schema(query=Query) . Last but not least, the GraphQL URL must be added into the urls.py file. . # bookstore/urls.py from django.conf.urls import url from django.contrib import admin from graphene_django.views import GraphQLView from bookstore.schema import schema urlpatterns = [ url(r&#39;^admin/&#39;, admin.site.urls), url(r&#39;^graphql&#39;, GraphQLView.as_view(graphiql=True, schema=schema)), ] . You can now run python manage.py runserver and start using the API at http://localhost:8000/graphql. . Querying and debugging . GraphiQL provides a graphical interactive in-browser GraphQL IDE, including some features such as syntax highlighting, real-time error reporting, automatic query completion etc. . I will show some query examples, but you can learn more about querying at graphql.org/learn/queries/. . Given the following query, we can retrieve all books registered along with their authors. . { allBooks { title, authors { firstName, lastName } } } . And the response… . { &quot;data&quot;: { &quot;allBooks&quot;: [ { &quot;title&quot;: &quot;Resurrection&quot;, &quot;authors&quot;: [ { &quot;firstName&quot;: &quot;Leo&quot;, &quot;lastName&quot;: &quot;Tolstoy&quot; } ] }, { &quot;title&quot;: &quot;Childhood&quot;, &quot;authors&quot;: [ { &quot;firstName&quot;: &quot;Leo&quot;, &quot;lastName&quot;: &quot;Tolstoy&quot; } ] } ] } } . Using the __debug field you can get information about the actual SQL query. . { allAuthors {lastName} __debug { sql {rawSql, duration} } } . Response: . { &quot;data&quot;: { &quot;allAuthors&quot;: [ { &quot;lastName&quot;: &quot;King&quot; }, { &quot;lastName&quot;: &quot;Tolstoy&quot; }, { &quot;lastName&quot;: &quot;Gaiman&quot; }, { &quot;lastName&quot;: &quot;Pratchett&quot; } ], &quot;__debug&quot;: { &quot;sql&quot;: [ { &quot;rawSql&quot;: &quot;SELECT &quot;store_author &quot;. &quot;id &quot;, &quot;store_author &quot;. &quot;first_name &quot;, &quot;store_author &quot;. &quot;last_name &quot;, &quot;store_author &quot;. &quot;email &quot; FROM &quot;store_author &quot;&quot;, &quot;duration&quot;: 0.0009260177612304688 } ] } } } . All this code is on my Github. Please do fork it and make pull requests regarding any issues or improvements you may have with my code. .",
            "url": "https://joaorafaelm.github.io/notebook/graphql/django/2017/08/05/graphql-and-django-in-5-minutes.html",
            "relUrl": "/graphql/django/2017/08/05/graphql-and-django-in-5-minutes.html",
            "date": " 2017 Aug 05"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Hi 👋 . I’m a remote software developer, currently living in Brazil. You can find some open source projects I’ve been working on in my github. Reach me at [     joaoraf@me.com,     linkedin,     @jrmol, ]. .",
          "url": "https://joaorafaelm.github.io/notebook/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://joaorafaelm.github.io/notebook/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
