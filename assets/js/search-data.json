{
  
    
        "post0": {
            "title": "Automated data exploration with pandas profiling",
            "content": "pip install --upgrade pandas-profiling . Requirement already up-to-date: pandas-profiling in /usr/local/lib/python3.7/dist-packages (3.0.0) Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.0.1) Requirement already satisfied, skipping upgrade: tqdm&gt;=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (4.61.0) Requirement already satisfied, skipping upgrade: requests&gt;=2.24.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.25.1) Requirement already satisfied, skipping upgrade: matplotlib&gt;=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (3.2.2) Requirement already satisfied, skipping upgrade: jinja2&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.11.3) Requirement already satisfied, skipping upgrade: htmlmin&gt;=0.1.12 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.1.12) Requirement already satisfied, skipping upgrade: visions[type_image_path]==0.7.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.7.1) Requirement already satisfied, skipping upgrade: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.1.5) Requirement already satisfied, skipping upgrade: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.19.5) Requirement already satisfied, skipping upgrade: missingno&gt;=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.4.2) Requirement already satisfied, skipping upgrade: phik&gt;=0.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.11.2) Requirement already satisfied, skipping upgrade: tangled-up-in-unicode==0.1.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.1.0) Requirement already satisfied, skipping upgrade: seaborn&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.11.1) Requirement already satisfied, skipping upgrade: pydantic&gt;=1.8.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.8.2) Requirement already satisfied, skipping upgrade: scipy&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.4.1) Requirement already satisfied, skipping upgrade: PyYAML&gt;=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (5.4.1) Requirement already satisfied, skipping upgrade: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling) (1.24.3) Requirement already satisfied, skipping upgrade: chardet&lt;5,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling) (3.0.4) Requirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling) (2.10) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling) (2020.12.5) Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling) (2.4.7) Requirement already satisfied, skipping upgrade: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling) (0.10.0) Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling) (2.8.1) Requirement already satisfied, skipping upgrade: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling) (1.3.1) Requirement already satisfied, skipping upgrade: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2&gt;=2.11.1-&gt;pandas-profiling) (2.0.1) Requirement already satisfied, skipping upgrade: bottleneck in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (1.3.2) Requirement already satisfied, skipping upgrade: networkx&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (2.5.1) Requirement already satisfied, skipping upgrade: multimethod==1.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (1.4) Requirement already satisfied, skipping upgrade: attrs&gt;=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (21.2.0) Requirement already satisfied, skipping upgrade: imagehash; extra == &#34;type_image_path&#34; in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (4.2.0) Requirement already satisfied, skipping upgrade: Pillow; extra == &#34;type_image_path&#34; in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (7.1.2) Requirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3-&gt;pandas-profiling) (2018.9) Requirement already satisfied, skipping upgrade: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic&gt;=1.8.1-&gt;pandas-profiling) (3.7.4.3) Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from cycler&gt;=0.10-&gt;matplotlib&gt;=3.2.0-&gt;pandas-profiling) (1.15.0) Requirement already satisfied, skipping upgrade: decorator&lt;5,&gt;=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx&gt;=2.4-&gt;visions[type_image_path]==0.7.1-&gt;pandas-profiling) (4.4.2) Requirement already satisfied, skipping upgrade: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash; extra == &#34;type_image_path&#34;-&gt;visions[type_image_path]==0.7.1-&gt;pandas-profiling) (1.1.1) . . Importing libraries and Titanic dataset . import pandas as pd from pathlib import Path from ipywidgets import widgets from pandas_profiling import ProfileReport import warnings warnings.filterwarnings(&quot;ignore&quot;) file_name = &quot;https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv&quot; df = pd.read_csv(file_name) profile = ProfileReport(df, title=&quot;Titanic Dataset&quot;, html={&#39;style&#39;: {&#39;full_width&#39;: True}}, sort=None) . profile.to_widgets() . profile . .",
            "url": "https://joaorafaelm.github.io/notebook/pandas/2021/06/06/automated-data-exploration-with-pandas-profiling-and-titanic-dataset.html",
            "relUrl": "/pandas/2021/06/06/automated-data-exploration-with-pandas-profiling-and-titanic-dataset.html",
            "date": " 2021 Jun 06"
        }
        
    
  
    
        ,"post1": {
            "title": "Deploying a XGBoost model",
            "content": "import pandas as pd import numpy as np import xgboost as xgb import re from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error np.random.seed(42) . Load data and build a model . X, y = fetch_openml(&quot;titanic&quot;, version=1, as_frame=True, return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2) . X_y_train = xgb.DMatrix(data=X_train[[&quot;pclass&quot;, &quot;age&quot;, &quot;fare&quot;, &quot;sibsp&quot;, &quot;parch&quot;]], label=y_train.astype(int)) X_test = xgb.DMatrix(data=X_test[[&quot;pclass&quot;, &quot;age&quot;, &quot;fare&quot;, &quot;sibsp&quot;, &quot;parch&quot;]]) . X_train[[&quot;pclass&quot;, &quot;age&quot;, &quot;fare&quot;, &quot;sibsp&quot;, &quot;parch&quot;]].head() . pclass age fare sibsp parch . 662 3.0 | 40.0 | 7.2250 | 0.0 | 0.0 | . 164 1.0 | 35.0 | 26.5500 | 0.0 | 0.0 | . 871 3.0 | NaN | 7.7500 | 0.0 | 0.0 | . 1298 3.0 | 36.0 | 9.5000 | 0.0 | 0.0 | . 1004 3.0 | NaN | 7.7875 | 0.0 | 0.0 | . params = { &quot;base_score&quot;: np.mean(y_train.astype(int)), &quot;eta&quot;: 0.1, &quot;max_depth&quot;: 3, &quot;gamma&quot;: 3, &quot;objective&quot;: &quot;reg:squarederror&quot;, &quot;eval_metric&quot;: &quot;mae&quot; } model = xgb.train( params=params, dtrain=X_y_train, num_boost_round=3 ) . Visualization of model . xgb.to_graphviz(booster = model, num_trees=0) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 0 fare&lt;15.1729002 1 age&lt;16.5 0&#45;&gt;1 yes, missing 2 pclass&lt;2.5 0&#45;&gt;2 no 3 leaf=0.0223644935 1&#45;&gt;3 yes 4 leaf=&#45;0.0156309232 1&#45;&gt;4 no, missing 5 parch&lt;0.5 2&#45;&gt;5 yes, missing 6 fare&lt;23.3500004 2&#45;&gt;6 no 11 leaf=0.0153174726 5&#45;&gt;11 yes, missing 12 leaf=0.03650124 5&#45;&gt;12 no 13 leaf=0.00960917864 6&#45;&gt;13 yes 14 leaf=&#45;0.0235449504 6&#45;&gt;14 no, missing xgb.to_graphviz(booster = model, num_trees=1) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 0 fare&lt;15.1729002 1 age&lt;16.5 0&#45;&gt;1 yes, missing 2 pclass&lt;2.5 0&#45;&gt;2 no 3 leaf=0.0201979335 1&#45;&gt;3 yes 4 leaf=&#45;0.0140708359 1&#45;&gt;4 no, missing 5 leaf=0.0204214789 2&#45;&gt;5 yes, missing 6 fare&lt;23.3500004 2&#45;&gt;6 no 13 leaf=0.00866124686 6&#45;&gt;13 yes 14 leaf=&#45;0.0212272462 6&#45;&gt;14 no, missing xgb.to_graphviz(booster = model, num_trees=2) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 0 fare&lt;15.1729002 1 leaf=&#45;0.010894442 0&#45;&gt;1 yes, missing 2 pclass&lt;2.5 0&#45;&gt;2 no 5 leaf=0.0183849707 2&#45;&gt;5 yes, missing 6 leaf=&#45;0.00472340686 2&#45;&gt;6 no print(&quot; n&quot;.join(model.get_dump())) . 0:[fare&lt;15.1729002] yes=1,no=2,missing=1 1:[age&lt;16.5] yes=3,no=4,missing=4 3:leaf=0.0223644935 4:leaf=-0.0156309232 2:[pclass&lt;2.5] yes=5,no=6,missing=5 5:[parch&lt;0.5] yes=11,no=12,missing=11 11:leaf=0.0153174726 12:leaf=0.03650124 6:[fare&lt;23.3500004] yes=13,no=14,missing=14 13:leaf=0.00960917864 14:leaf=-0.0235449504 0:[fare&lt;15.1729002] yes=1,no=2,missing=1 1:[age&lt;16.5] yes=3,no=4,missing=4 3:leaf=0.0201979335 4:leaf=-0.0140708359 2:[pclass&lt;2.5] yes=5,no=6,missing=5 5:leaf=0.0204214789 6:[fare&lt;23.3500004] yes=13,no=14,missing=14 13:leaf=0.00866124686 14:leaf=-0.0212272462 0:[fare&lt;15.1729002] yes=1,no=2,missing=1 1:leaf=-0.010894442 2:[pclass&lt;2.5] yes=5,no=6,missing=5 5:leaf=0.0183849707 6:leaf=-0.00472340686 . Convert dump string to a .py file . def string_parser(s): if len(re.findall(r&quot;:leaf=&quot;, s)) == 0: out = re.findall(r&quot;[ w.-]+&quot;, s) tabs = re.findall(r&quot;[ t]+&quot;, s) if (out[4] == out[8]): missing_value_handling = (&quot; or np.isnan(x[&#39;&quot; + out[1] + &quot;&#39;]) &quot;) else: missing_value_handling = &quot;&quot; if len(tabs) &gt; 0: return (re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; if state == &#39; + out[0] + &#39;: n&#39; + re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; state = (&#39; + out[4] + &#39; if &#39; + &quot;x[&#39;&quot; + out[1] +&quot;&#39;]&lt;&quot; + out[2] + missing_value_handling + &#39; else &#39; + out[6] + &#39;) n&#39; ) else: return (&#39; if state == &#39; + out[0] + &#39;: n&#39; + &#39; state = (&#39; + out[4] + &#39; if &#39; + &quot;x[&#39;&quot; + out[1] +&quot;&#39;]&lt;&quot; + out[2] + missing_value_handling + &#39; else &#39; + out[6] + &#39;) n&#39; ) else: out = re.findall(r&quot;[ d.-]+&quot;, s) return (re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; if state == &#39; + out[0] + &#39;: n &#39; + re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; return &#39; + out[1] + &#39; n&#39;) def tree_parser(tree, i): if i == 0: return (&#39; if num_booster == 0: n state = 0 n&#39; + &quot;&quot;.join([string_parser(tree.split(&#39; n&#39;)[i]) for i in range(len(tree.split(&#39; n&#39;))-1)])) else: return (&#39; elif num_booster == &#39;+str(i)+&#39;: n state = 0 n&#39; + &quot;&quot;.join([string_parser(tree.split(&#39; n&#39;)[i]) for i in range(len(tree.split(&#39; n&#39;))-1)])) def model_to_py(base_score, model, out_file): trees = model.get_dump() result = [&quot;import numpy as np n n&quot; +&quot;def xgb_tree(x, num_booster): n&quot;] for i in range(len(trees)): result.append(tree_parser(trees[i], i)) with open(out_file, &#39;w&#39;) as the_file: the_file.write(&quot;&quot;.join(result) + &quot; ndef xgb_predict(x): n predict = &quot; + str(base_score) + &quot; n&quot; + &quot;# initialize prediction with base score n&quot; + &quot; for i in range(&quot; + str(len(trees)) + &quot;): n predict = predict + xgb_tree(x, i)&quot; + &quot; n return predict&quot;) model_to_py(params[&#39;base_score&#39;], model, &#39;xgb_model.py&#39;) . Prediction using dump file . import xgb_model passenger_data_1 = {&#39;pclass&#39;:3, &#39;age&#39;:np.nan, &#39;sibsp&#39;:0, &#39;parch&#39;:0, &#39;fare&#39;:7.8958} passenger_data_2 = {&#39;pclass&#39;:1, &#39;age&#39;:46, &#39;sibsp&#39;:0, &#39;parch&#39;:0, &#39;fare&#39;:26} print(xgb_model.xgb_predict(passenger_data_1)) print(xgb_model.xgb_predict(passenger_data_2)) . 0.34144773395253103 0.43616785725253104 .",
            "url": "https://joaorafaelm.github.io/notebook/xgboost/2021/06/06/_deploying_xgboost_model.html",
            "relUrl": "/xgboost/2021/06/06/_deploying_xgboost_model.html",
            "date": " 2021 Jun 06"
        }
        
    
  
    
        ,"post2": {
            "title": "Incremental training with XGBoost",
            "content": "Install dependencies . pip install scikit-learn xgboost . Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1) Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1) Requirement already satisfied: numpy&gt;=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5) Requirement already satisfied: scipy&gt;=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1) . . Save your model after you train on the first batch. Then, on successive runs, provide the xgb.train method with the filepath of the saved model. . First, split the boston dataset into training and testing sets. Then split the training set into halves. Fit a model with the first half and get a score that will serve as a benchmark. Then fit two models with the second half; one model will have the additional parameter xgb_model. If passing in the extra parameter didn&#39;t make a difference, then we would expect their scores to be similar.. But, fortunately, the new model seems to perform much better than the first. . import xgboost as xgb from sklearn.model_selection import train_test_split from sklearn.datasets import load_boston from sklearn.metrics import mean_squared_error X = load_boston()[&#39;data&#39;] y = load_boston()[&#39;target&#39;] # split data into training and testing sets # then split training set in half X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0) X_train_1, X_train_2, y_train_1, y_train_2 = train_test_split( X_train, y_train, test_size=0.5, random_state=0 ) xg_train_1 = xgb.DMatrix(X_train_1, label=y_train_1) xg_train_2 = xgb.DMatrix(X_train_2, label=y_train_2) xg_test = xgb.DMatrix(X_test, label=y_test) params = {&#39;objective&#39;: &#39;reg:squarederror&#39;, &#39;verbose&#39;: False} model_1 = xgb.train(params, xg_train_1, 30) model_1.save_model(&#39;model_1.model&#39;) # ================= train two versions of the model =====================# model_2_v1 = xgb.train(params, xg_train_2, 30) model_2_v2 = xgb.train(params, xg_train_2, 30, xgb_model=&#39;model_1.model&#39;) print(mean_squared_error(model_1.predict(xg_test), y_test)) # benchmark print(mean_squared_error(model_2_v1.predict(xg_test), y_test)) # &quot;before&quot; print(mean_squared_error(model_2_v2.predict(xg_test), y_test)) # &quot;after&quot; . 21.988532050893138 39.677688213388755 23.092057209292484 .",
            "url": "https://joaorafaelm.github.io/notebook/xgboost/2021/05/31/incremental-training-xgboost.html",
            "relUrl": "/xgboost/2021/05/31/incremental-training-xgboost.html",
            "date": " 2021 May 31"
        }
        
    
  
    
        ,"post3": {
            "title": "Multilanguage topic modeling with BERT",
            "content": "!pip install contextualized_topic_models !pip uninstall transformers -y !pip install transformers==3.0.2 . Requirement already satisfied: contextualized_topic_models in /usr/local/lib/python3.6/dist-packages (1.4.2) Requirement already satisfied: torchvision==0.7.0 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (0.7.0+cu101) Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (3.8.3) Requirement already satisfied: wheel==0.33.6 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (0.33.6) Requirement already satisfied: pytest-runner==5.1 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (5.1) Requirement already satisfied: pytest==4.6.5 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (4.6.5) Requirement already satisfied: numpy==1.19.1 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (1.19.1) Requirement already satisfied: sentence-transformers==0.3.2 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (0.3.2) Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (1.6.0) Requirement already satisfied: pillow&gt;=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7.0-&gt;contextualized_topic_models) (7.0.0) Requirement already satisfied: six&gt;=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3-&gt;contextualized_topic_models) (1.15.0) Requirement already satisfied: scipy&gt;=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3-&gt;contextualized_topic_models) (1.4.1) Requirement already satisfied: smart-open&gt;=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3-&gt;contextualized_topic_models) (2.1.0) Requirement already satisfied: importlib-metadata&gt;=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (1.7.0) Requirement already satisfied: more-itertools&gt;=4.0.0; python_version &gt; &#34;2.7&#34; in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (8.4.0) Requirement already satisfied: atomicwrites&gt;=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (1.4.0) Requirement already satisfied: py&gt;=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (1.9.0) Requirement already satisfied: pluggy&lt;1.0,&gt;=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (0.13.1) Requirement already satisfied: attrs&gt;=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (20.1.0) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (20.4) Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (0.2.5) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.22.2.post1) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.2-&gt;contextualized_topic_models) (4.41.1) Requirement already satisfied: transformers&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.2-&gt;contextualized_topic_models) (3.1.0) Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.2-&gt;contextualized_topic_models) (3.2.5) Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0-&gt;contextualized_topic_models) (0.16.0) Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (2.49.0) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (2.23.0) Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (1.14.48) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata&gt;=0.12-&gt;pytest==4.6.5-&gt;contextualized_topic_models) (3.1.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;pytest==4.6.5-&gt;contextualized_topic_models) (2.4.7) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.16.0) Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.0.43) Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.8.1rc2) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (3.0.12) Requirement already satisfied: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.7) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (2019.12.20) Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.1.91) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (2020.6.20) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (3.0.4) Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (0.10.0) Requirement already satisfied: s3transfer&lt;0.4.0,&gt;=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (0.3.3) Requirement already satisfied: botocore&lt;1.18.0,&gt;=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (1.17.48) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (7.1.2) Requirement already satisfied: docutils&lt;0.16,&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore&lt;1.18.0,&gt;=1.17.48-&gt;boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (0.15.2) Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore&lt;1.18.0,&gt;=1.17.48-&gt;boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (2.8.1) Uninstalling transformers-3.1.0: Successfully uninstalled transformers-3.1.0 Collecting transformers==3.0.2 Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB) |████████████████████████████████| 778kB 3.4MB/s Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.0.43) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12) Requirement already satisfied: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.7) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.4) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.19.1) Collecting tokenizers==0.8.1.rc1 Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB) |████████████████████████████████| 3.0MB 17.9MB/s Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.1.91) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==3.0.2) (0.16.0) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==3.0.2) (7.1.2) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==3.0.2) (1.15.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;transformers==3.0.2) (2.4.7) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.0.2) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.0.2) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.0.2) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.0.2) (2020.6.20) Installing collected packages: tokenizers, transformers Found existing installation: tokenizers 0.8.1rc2 Uninstalling tokenizers-0.8.1rc2: Successfully uninstalled tokenizers-0.8.1rc2 Successfully installed tokenizers-0.8.1rc1 transformers-3.0.2 . . import os import numpy as np import pickle from contextualized_topic_models.models.ctm import CTM from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file, bert_embeddings_from_list from contextualized_topic_models.datasets.dataset import CTMDataset from contextualized_topic_models.utils.data_preparation import TextHandler . !curl -s https://raw.githubusercontent.com/MilaNLProc/contextualized-topic-models/master/contextualized_topic_models/data/gnews/GoogleNews.txt | head -n1000 &gt; googlenews.txt !head googlenews.txt !cat googlenews.txt | wc -l . centrepoint winter white gala london mourinho seek killer instinct roundup golden globe won seduced johansson voice travel disruption mount storm cold air sweep south florida wes welker blame costly turnover psalm book fetch record ny auction ktvn channel reno surface review comparison window powered tablet pitted scientist unreported fish trap space nokia lumia launch edward snowden latest leak nsa monitored online porn habit radicalizers 1000 . Load The Data . file_name = &quot;googlenews.txt&quot; handler = TextHandler(file_name) handler.prepare() # create vocabulary and training data . train_bert = bert_embeddings_from_file(file_name, &quot;distiluse-base-multilingual-cased&quot;) training_dataset = CTMDataset(handler.bow, train_bert, handler.idx2token) . Train the Fully Contextualized Topic Model . num_topics = 50 ctm = CTM(input_size=len(handler.vocab), bert_input_size=512, num_epochs=100, hidden_sizes = (100, ), inference_type=&quot;contextual&quot;, n_components=num_topics, num_data_loader_workers=0) ctm.fit(training_dataset) # run the model . ctm.get_topic_lists(5) # get the top-5 words lists . [[&#39;kim&#39;, &#39;west&#39;, &#39;kanye&#39;, &#39;kardashian&#39;, &#39;bound&#39;], [&#39;day&#39;, &#39;thanksgiving&#39;, &#39;parade&#39;, &#39;macy&#39;, &#39;packer&#39;], [&#39;patriot&#39;, &#39;bronco&#39;, &#39;pat&#39;, &#39;packer&#39;, &#39;loss&#39;], [&#39;xbox&#39;, &#39;microsoft&#39;, &#39;p&#39;, &#39;game&#39;, &#39;console&#39;], [&#39;government&#39;, &#39;political&#39;, &#39;thai&#39;, &#39;party&#39;, &#39;protest&#39;], [&#39;oldboy&#39;, &#39;brolin&#39;, &#39;josh&#39;, &#39;lee&#39;, &#39;spike&#39;], [&#39;google&#39;, &#39;chrome&#39;, &#39;search&#39;, &#39;extension&#39;, &#39;voice&#39;], [&#39;johansson&#39;, &#39;globe&#39;, &#39;golden&#39;, &#39;scarlett&#39;, &#39;ineligible&#39;], [&#39;star&#39;, &#39;dancing&#39;, &#39;amber&#39;, &#39;riley&#39;, &#39;win&#39;], [&#39;police&#39;, &#39;guilty&#39;, &#39;watkins&#39;, &#39;case&#39;, &#39;lostprophets&#39;], [&#39;san&#39;, &#39;andreas&#39;, &#39;gta&#39;, &#39;mobile&#39;, &#39;android&#39;], [&#39;flat&#39;, &#39;future&#39;, &#39;record&#39;, &#39;level&#39;, &#39;p&#39;], [&#39;thanksgiving&#39;, &#39;day&#39;, &#39;parade&#39;, &#39;thanksgivukkah&#39;, &#39;holiday&#39;], [&#39;jos&#39;, &#39;wearhouse&#39;, &#39;men&#39;, &#39;bank&#39;, &#39;baldwin&#39;], [&#39;prince&#39;, &#39;william&#39;, &#39;swift&#39;, &#39;jovi&#39;, &#39;bon&#39;], [&#39;porn&#39;, &#39;nsa&#39;, &#39;habit&#39;, &#39;radicalizers&#39;, &#39;spying&#39;], [&#39;pope&#39;, &#39;church&#39;, &#39;putin&#39;, &#39;issue&#39;, &#39;coalition&#39;], [&#39;report&#39;, &#39;benghazi&#39;, &#39;security&#39;, &#39;baldwin&#39;, &#39;alec&#39;], [&#39;china&#39;, &#39;zone&#39;, &#39;flight&#39;, &#39;airspace&#39;, &#39;disputed&#39;], [&#39;storm&#39;, &#39;parade&#39;, &#39;macy&#39;, &#39;balloon&#39;, &#39;travel&#39;], [&#39;bank&#39;, &#39;men&#39;, &#39;palestinian&#39;, &#39;jos&#39;, &#39;wearhouse&#39;], [&#39;review&#39;, &#39;homefront&#39;, &#39;frozen&#39;, &#39;inch&#39;, &#39;oldboy&#39;], [&#39;bronco&#39;, &#39;packer&#39;, &#39;seahawks&#39;, &#39;rodgers&#39;, &#39;patriot&#39;], [&#39;frozen&#39;, &#39;heart&#39;, &#39;review&#39;, &#39;homefront&#39;, &#39;detroit&#39;], [&#39;hiv&#39;, &#39;meningitis&#39;, &#39;flu&#39;, &#39;greece&#39;, &#39;health&#39;], [&#39;black&#39;, &#39;friday&#39;, &#39;nativity&#39;, &#39;deal&#39;, &#39;monday&#39;], [&#39;aarushi&#39;, &#39;hiv&#39;, &#39;killing&#39;, &#39;teen&#39;, &#39;murder&#39;], [&#39;west&#39;, &#39;kanye&#39;, &#39;kim&#39;, &#39;seth&#39;, &#39;bound&#39;], [&#39;cb&#39;, &#39;seahawks&#39;, &#39;dallas&#39;, &#39;chelsea&#39;, &#39;browner&#39;], [&#39;hp&#39;, &#39;revenue&#39;, &#39;raise&#39;, &#39;week&#39;, &#39;shopping&#39;], [&#39;lumia&#39;, &#39;nokia&#39;, &#39;price&#39;, &#39;power&#39;, &#39;uk&#39;], [&#39;typhoon&#39;, &#39;philippine&#39;, &#39;haiyan&#39;, &#39;climate&#39;, &#39;gain&#39;], [&#39;african&#39;, &#39;france&#39;, &#39;central&#39;, &#39;republic&#39;, &#39;troop&#39;], [&#39;parade&#39;, &#39;macy&#39;, &#39;carlos&#39;, &#39;beltran&#39;, &#39;york&#39;], [&#39;kim&#39;, &#39;kardashian&#39;, &#39;video&#39;, &#39;west&#39;, &#39;bound&#39;], [&#39;hewitt&#39;, &#39;love&#39;, &#39;star&#39;, &#39;jennifer&#39;, &#39;dancing&#39;], [&#39;swift&#39;, &#39;william&#39;, &#39;taylor&#39;, &#39;prince&#39;, &#39;jovi&#39;], [&#39;launch&#39;, &#39;microsoft&#39;, &#39;chrome&#39;, &#39;google&#39;, &#39;search&#39;], [&#39;pakistan&#39;, &#39;army&#39;, &#39;chief&#39;, &#39;sharif&#39;, &#39;pm&#39;], [&#39;air&#39;, &#39;china&#39;, &#39;zone&#39;, &#39;sea&#39;, &#39;disputed&#39;], [&#39;west&#39;, &#39;kanye&#39;, &#39;bound&#39;, &#39;kim&#39;, &#39;video&#39;], [&#39;ison&#39;, &#39;comet&#39;, &#39;raptor&#39;, &#39;sun&#39;, &#39;bonobo&#39;], [&#39;irs&#39;, &#39;google&#39;, &#39;tax&#39;, &#39;group&#39;, &#39;glass&#39;], [&#39;net&#39;, &#39;review&#39;, &#39;preview&#39;, &#39;disney&#39;, &#39;movie&#39;], [&#39;nokia&#39;, &#39;lumia&#39;, &#39;tablet&#39;, &#39;window&#39;, &#39;moto&#39;], [&#39;three&#39;, &#39;seahawks&#39;, &#39;year&#39;, &#39;burning&#39;, &#39;officer&#39;], [&#39;report&#39;, &#39;burning&#39;, &#39;officer&#39;, &#39;storm&#39;, &#39;truck&#39;], [&#39;girl&#39;, &#39;baby&#39;, &#39;guilty&#39;, &#39;lostprophets&#39;, &#39;hewitt&#39;], [&#39;black&#39;, &#39;friday&#39;, &#39;sale&#39;, &#39;deal&#39;, &#39;monday&#39;], [&#39;heart&#39;, &#39;woman&#39;, &#39;pill&#39;, &#39;frozen&#39;, &#39;crisis&#39;]] . !tail -n 5 googlenews.txt &gt; test.txt !cat test.txt . ray whitney return will dallas star huge boost offensively s relied intermediary probe spacex sept upper stage nokia lumia tablet kill surface lakers net preview neighbor helped save girl imprisoned year speaks . test_handler = TextHandler(&quot;test.txt&quot;) test_handler.prepare() # create vocabulary and training data # generate BERT data testing_bert = bert_embeddings_from_file(&quot;test.txt&quot;, &quot;distiluse-base-multilingual-cased&quot;) testing_dataset = CTMDataset(test_handler.bow, testing_bert, test_handler.idx2token) . # we sample n times and average to get a more accurate estimate of the document-topic distribution predicted_topics = [] thetas = np.zeros((len(testing_dataset), num_topics)) for a in range(0, 100): thetas = thetas + np.array(ctm.get_thetas(testing_dataset)) for idd in range(0, len(testing_dataset)): thetas[idd] = thetas[idd]/np.sum(thetas[idd]) predicted_topic = np.argmax(thetas[idd]) predicted_topics.append(predicted_topic) # document-topic distribution , list of the topic predicted for each testing document # thetas, predicted_topics . [22, 41, 44, 23, 47] . test_handler.load_text_file()[1] . &#39;s relied intermediary probe spacex sept upper stage n&#39; . ctm.get_topic_lists(20)[41] . [&#39;ison&#39;, &#39;comet&#39;, &#39;raptor&#39;, &#39;sun&#39;, &#39;bonobo&#39;, &#39;dna&#39;, &#39;flying&#39;, &#39;trouble&#39;, &#39;stereo&#39;, &#39;seahorse&#39;, &#39;researcher&#39;, &#39;preview&#39;, &#39;spacecraft&#39;, &#39;century&#39;, &#39;jellyfish&#39;, &#39;testing&#39;, &#39;minute&#39;, &#39;net&#39;, &#39;spectacular&#39;, &#39;congo&#39;] .",
            "url": "https://joaorafaelm.github.io/notebook/bert/topics/nlp/2021/04/17/multilang-topic-model.html",
            "relUrl": "/bert/topics/nlp/2021/04/17/multilang-topic-model.html",
            "date": " 2021 Apr 17"
        }
        
    
  
    
        ,"post4": {
            "title": "Cognitive complexity and python",
            "content": "Install dependencies . pip install cognitive_complexity astunparse tabulate . Requirement already satisfied: cognitive_complexity in /usr/local/lib/python3.7/dist-packages (1.2.0) Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (1.6.3) Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (0.8.9) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from cognitive_complexity) (57.0.0) Requirement already satisfied: six&lt;2.0,&gt;=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse) (1.15.0) Requirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse) (0.36.2) . . Import and define utils . import ast import astunparse from inspect import getsource from tabulate import tabulate from cognitive_complexity.api import get_cognitive_complexity_for_node from cognitive_complexity.utils.ast import has_recursive_calls, is_decorator, process_child_nodes, process_node_itself def get_cognitive_complexity(func): func = func if isinstance(func, str) else getsource(func) funcdef = ast.parse(func).body[0] if is_decorator(funcdef): return get_cognitive_complexity(funcdef.body[0]) details = [] complexity = 0 for node in funcdef.body: node_complexity = get_cognitive_complexity_for_node(node) complexity += node_complexity node_code = astunparse.unparse(node) if f&quot;{funcdef.name}(&quot; in node_code: # +1 for recursion node_complexity += 1 complexity += 1 details.append([node_complexity, node_code]) details.append([complexity, &quot;Total&quot;]) return complexity, details . Introduction . Formulated in a Fortran environment in 1976, Cyclomatic Complexity has long been the standard for measuring the complexity of a method’s control flow. It was originally intended “to identify software modules that will be difficult to test or maintain”, but while it accurately calculates the minimum number of test cases required to fully cover a method, it is not a satisfactory measure of understandability and it also doesn’t include modern language structures like try/catch, and lambdas. . -- Cognitive Complexity:A new way of measuring understandability, white paper by G. Ann Campbell . Basic criteria and methodology . As a remedy for these problems, Cognitive Complexity has been formulated to address modern language structures, and to produce values that are meaningful at the class and application levels. A Cognitive Complexity score is assessed according to three basic rules: . Ignore structures that allow multiple statements to be readably shorthanded into one | Increment (add one) for each break in the linear flow of the code | Increment when flow-breaking structures are nested Additionally, a complexity score is made up of four different types of increments: . A. Nesting - assessed for nesting control flow structures inside each other . B. Structural - assessed on control flow structures that are subject to a nesting increment, and that increase the nesting count . C. Fundamental - assessed on statements not subject to a nesting increment . D. Hybrid - assessed on control flow structures that are not subject to a nesting increment, but which do increase the nesting count . | -- Cognitive Complexity: A new way of measuring understandability, white paper by G. Ann Campbell . def f(n): if n &gt; 10: return True if n &lt; 5: return 20 else: return 2 return f(n) total, details = get_cognitive_complexity(f) print(tabulate(details, headers=[&quot;Complexity&quot;, &quot;Node&quot;], tablefmt=&quot;fancy_grid&quot;)) . ╒══════════════╤═════════════════╕ │ Complexity │ Node │ ╞══════════════╪═════════════════╡ │ 1 │ if (n &gt; 10): │ │ │ return True │ ├──────────────┼─────────────────┤ │ 2 │ if (n &lt; 5): │ │ │ return 20 │ │ │ else: │ │ │ return 2 │ ├──────────────┼─────────────────┤ │ 1 │ return f(n) │ ├──────────────┼─────────────────┤ │ 4 │ Total │ ╘══════════════╧═════════════════╛ . References . Cognitive Complexity, Because Testability != Understandability . | Cognitive Complexity: A new way of measuring understandability, white paper by G. Ann Campbell . | Cognitive Complexity: the New Guide to Refactoring for Maintainable Code . | Cognitive Complexity from CodeClimate docs . | Is Your Code Readable By Humans? Cognitive Complexity Tells You . | .",
            "url": "https://joaorafaelm.github.io/notebook/python/2021/04/10/cognitive-complexity.html",
            "relUrl": "/python/2021/04/10/cognitive-complexity.html",
            "date": " 2021 Apr 10"
        }
        
    
  
    
        ,"post5": {
            "title": "Training GPT2 with Colab and Google Drive",
            "content": "We&#39;ll be using aitextgen to finetune the model. . pip install aitextgen . Requirement already satisfied: aitextgen in /usr/local/lib/python3.7/dist-packages (0.5.2) Requirement already satisfied: pytorch-lightning&gt;=1.3.1 in /usr/local/lib/python3.7/dist-packages (from aitextgen) (1.3.4) Requirement already satisfied: fire&gt;=0.3.0 in /usr/local/lib/python3.7/dist-packages (from aitextgen) (0.4.0) Requirement already satisfied: transformers&gt;=4.5.1 in /usr/local/lib/python3.7/dist-packages (from aitextgen) (4.6.1) Requirement already satisfied: torch&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from aitextgen) (1.8.1+cu101) Requirement already satisfied: PyYAML&lt;=5.4.1,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (5.4.1) Requirement already satisfied: future&gt;=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.18.2) Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.3.0) Requirement already satisfied: fsspec[http]&gt;=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (2021.5.0) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (20.9) Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (2.4.1) Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (4.41.1) Requirement already satisfied: torchmetrics&gt;=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.3.2) Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.19.5) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire&gt;=0.3.0-&gt;aitextgen) (1.15.0) Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire&gt;=0.3.0-&gt;aitextgen) (1.1.0) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (3.0.12) Requirement already satisfied: tokenizers&lt;0.11,&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (0.10.3) Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (0.0.8) Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (0.0.45) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (2019.12.20) Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (4.0.1) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (2.23.0) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch&gt;=1.6.0-&gt;aitextgen) (3.7.4.3) Requirement already satisfied: aiohttp; extra == &#34;http&#34; in /usr/local/lib/python3.7/dist-packages (from fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (3.7.4.post0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (2.4.7) Requirement already satisfied: google-auth&lt;2,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.30.0) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.34.1) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (3.12.4) Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.0.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (57.0.0) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.8.0) Requirement already satisfied: wheel&gt;=0.26; python_version &gt;= &#34;3&#34; in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.36.2) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.4.4) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (3.3.4) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.12.0) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (1.0.1) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (3.4.1) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (2020.12.5) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (1.24.3) Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == &#34;http&#34;-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (5.1.0) Requirement already satisfied: async-timeout&lt;4.0,&gt;=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == &#34;http&#34;-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (3.0.1) Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == &#34;http&#34;-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.6.3) Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == &#34;http&#34;-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (21.2.0) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.2.8) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4; python_version &gt;= &#34;3.6&#34; in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (4.7.2) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (4.2.2) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.3.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (3.1.0) . . Import modules and mount google drive . from aitextgen import aitextgen from aitextgen.colab import mount_gdrive, copy_file_from_gdrive from aitextgen.TokenDataset import TokenDataset, merge_datasets from aitextgen.utils import build_gpt2_config from aitextgen.tokenizers import train_tokenizer mount_gdrive() . !curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt &gt; input.txt !head input.txt . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1089k 100 1089k 0 0 9002k 0 --:--:-- --:--:-- --:--:-- 9002k First Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: You are all resolved rather to die than to famish? All: . Train tokenizer . file_name = &quot;input.txt&quot; project_name = &quot;project_name&quot; # copy_file_from_gdrive(file_name) train_tokenizer(file_name); . INFO:aitextgen.tokenizers:Saving aitextgen-vocab.json and aitextgen-merges.txt to the current directory. You will need both files to build the GPT2Tokenizer. . Training the model should take about 30 minutes . model = None config = None for _ in [&quot;pytorch_model.bin&quot;, &quot;config.json&quot;, &quot;aitextgen_vocab.json&quot;, &quot;aitextgen_merges.json&quot;]: try: copy_file_from_gdrive(_, project_name) model = &quot;pytorch_model.bin&quot; config = &quot;config.json&quot; except FileNotFoundError: pass config = config or build_gpt2_config( vocab_size=5000, max_length=200, dropout=0.0, n_embd=256, n_layer=8, n_head=8 ) ai = aitextgen( vocab_file=&quot;aitextgen-vocab.json&quot;, merges_file=&quot;aitextgen-merges.txt&quot;, config=config, model=model, to_gpu=True ) . INFO:aitextgen:Constructing GPT-2 model from provided config. INFO:aitextgen:Using a custom tokenizer. . ai.train( file_name, line_by_line=False, num_steps=10000, generate_every=1000, save_every=500, learning_rate=1e-4, batch_size=128, save_gdrive=True, run_id=project_name ) . INFO:aitextgen.TokenDataset:Encoding 40,000 sets of tokens from input.txt. GPU available: True, used: True INFO:lightning:GPU available: True, used: True TPU available: False, using: 0 TPU cores INFO:lightning:TPU available: False, using: 0 TPU cores CUDA_VISIBLE_DEVICES: [0] INFO:lightning:CUDA_VISIBLE_DEVICES: [0] . . Generating examples . ai.generate( n=5, batch_size=5, prompt=&quot;Speak:&quot;, temperature=1.0, top_p=0.9, ) .",
            "url": "https://joaorafaelm.github.io/notebook/gpt2/colab/drive/2020/06/02/gpt2-google-drive-sync.html",
            "relUrl": "/gpt2/colab/drive/2020/06/02/gpt2-google-drive-sync.html",
            "date": " 2020 Jun 02"
        }
        
    
  
    
        ,"post6": {
            "title": "Audio synthesis with Forward Transformer TTS and WaveRNN Vocoder",
            "content": "Installing dependencies and pre-trained models . # Clone the Transformer TTS and WaveRNN repos !git clone https://github.com/as-ideas/TransformerTTS.git !cd TransformerTTS &amp;&amp; git checkout 1c1cb03 &amp;&amp; cd .. !git clone https://github.com/fatchord/WaveRNN # Install requirements !apt-get install -y espeak !pip install -r TransformerTTS/requirements.txt # Download the transformer pre-trained weights ! wget https://public-asai-dl-models.s3.eu-central-1.amazonaws.com/TransformerTTS/ljspeech_wavernn_forward_transformer.zip ! unzip -o ljspeech_wavernn_forward_transformer.zip # Unzip the wave pretrained model !unzip -o WaveRNN/pretrained/ljspeech.wavernn.mol.800k.zip -d WaveRNN/pretrained/ . Cloning into &#39;TransformerTTS&#39;... remote: Enumerating objects: 4107, done. remote: Counting objects: 100% (646/646), done. remote: Compressing objects: 100% (214/214), done. remote: Total 4107 (delta 456), reused 611 (delta 431), pack-reused 3461 Receiving objects: 100% (4107/4107), 26.00 MiB | 25.43 MiB/s, done. Resolving deltas: 100% (2826/2826), done. Note: checking out &#39;1c1cb03&#39;. You are in &#39;detached HEAD&#39; state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b &lt;new-branch-name&gt; HEAD is now at 1c1cb03 Add Audio class. WaveRNN and MelGAN compatible normalizations. Cloning into &#39;WaveRNN&#39;... remote: Enumerating objects: 928, done. remote: Total 928 (delta 0), reused 0 (delta 0), pack-reused 928 Receiving objects: 100% (928/928), 242.13 MiB | 34.46 MiB/s, done. Resolving deltas: 100% (525/525), done. Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: espeak-data libespeak1 libportaudio2 libsonic0 The following NEW packages will be installed: espeak espeak-data libespeak1 libportaudio2 libsonic0 0 upgraded, 5 newly installed, 0 to remove and 39 not upgraded. Need to get 1,219 kB of archives. After this operation, 3,031 kB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB] Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsonic0 amd64 0.2.0-6 [13.4 kB] Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak-data amd64 1.48.04+dfsg-5 [934 kB] Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libespeak1 amd64 1.48.04+dfsg-5 [145 kB] Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak amd64 1.48.04+dfsg-5 [61.6 kB] Fetched 1,219 kB in 1s (927 kB/s) Selecting previously unselected package libportaudio2:amd64. (Reading database ... 160772 files and directories currently installed.) Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ... Unpacking libportaudio2:amd64 (19.6.0-1) ... Selecting previously unselected package libsonic0:amd64. Preparing to unpack .../libsonic0_0.2.0-6_amd64.deb ... Unpacking libsonic0:amd64 (0.2.0-6) ... Selecting previously unselected package espeak-data:amd64. Preparing to unpack .../espeak-data_1.48.04+dfsg-5_amd64.deb ... Unpacking espeak-data:amd64 (1.48.04+dfsg-5) ... Selecting previously unselected package libespeak1:amd64. Preparing to unpack .../libespeak1_1.48.04+dfsg-5_amd64.deb ... Unpacking libespeak1:amd64 (1.48.04+dfsg-5) ... Selecting previously unselected package espeak. Preparing to unpack .../espeak_1.48.04+dfsg-5_amd64.deb ... Unpacking espeak (1.48.04+dfsg-5) ... Setting up libportaudio2:amd64 (19.6.0-1) ... Setting up espeak-data:amd64 (1.48.04+dfsg-5) ... Setting up libsonic0:amd64 (0.2.0-6) ... Setting up libespeak1:amd64 (1.48.04+dfsg-5) ... Setting up espeak (1.48.04+dfsg-5) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... Processing triggers for libc-bin (2.27-3ubuntu1.2) ... /sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r TransformerTTS/requirements.txt (line 1)) (3.2.2) Requirement already satisfied: librosa&gt;=0.7.1 in /usr/local/lib/python3.7/dist-packages (from -r TransformerTTS/requirements.txt (line 2)) (0.8.0) Requirement already satisfied: numpy&gt;=1.17.4 in /usr/local/lib/python3.7/dist-packages (from -r TransformerTTS/requirements.txt (line 3)) (1.19.5) Collecting phonemizer==2.1 Downloading https://files.pythonhosted.org/packages/d3/82/666045375029df9c2f274923539f43346a7b7abc349b02e33dff585da56f/phonemizer-2.1-py3-none-any.whl (47kB) |████████████████████████████████| 51kB 4.6MB/s Collecting ruamel.yaml&gt;=0.16.6 Downloading https://files.pythonhosted.org/packages/9e/00/1ba32614cc9572fd6e98dbfdf642f55f9c5ed8a89ab9328d2ce6f39e6fb3/ruamel.yaml-0.17.7-py3-none-any.whl (108kB) |████████████████████████████████| 112kB 14.5MB/s Requirement already satisfied: tensorflow&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from -r TransformerTTS/requirements.txt (line 6)) (2.5.0) Requirement already satisfied: tqdm&gt;=4.38.0 in /usr/local/lib/python3.7/dist-packages (from -r TransformerTTS/requirements.txt (line 7)) (4.41.1) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;-r TransformerTTS/requirements.txt (line 1)) (2.8.1) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;-r TransformerTTS/requirements.txt (line 1)) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;-r TransformerTTS/requirements.txt (line 1)) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;-r TransformerTTS/requirements.txt (line 1)) (1.3.1) Requirement already satisfied: scikit-learn!=0.19.0,&gt;=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (0.22.2.post1) Requirement already satisfied: audioread&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (2.1.9) Requirement already satisfied: resampy&gt;=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (0.2.2) Requirement already satisfied: soundfile&gt;=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (0.10.3.post1) Requirement already satisfied: pooch&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (1.3.0) Requirement already satisfied: scipy&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (1.4.1) Requirement already satisfied: numba&gt;=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (0.51.2) Requirement already satisfied: decorator&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (4.4.2) Requirement already satisfied: joblib&gt;=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (1.0.1) Collecting segments Downloading https://files.pythonhosted.org/packages/1e/ae/02d31d73cfc3fa1dc74b7b7f14820fadc287e74406583d7af7b80fcaac41/segments-2.2.0-py2.py3-none-any.whl Requirement already satisfied: attrs&gt;=18.1 in /usr/local/lib/python3.7/dist-packages (from phonemizer==2.1-&gt;-r TransformerTTS/requirements.txt (line 4)) (21.2.0) Collecting ruamel.yaml.clib&gt;=0.1.2; platform_python_implementation == &#34;CPython&#34; and python_version &lt; &#34;3.10&#34; Downloading https://files.pythonhosted.org/packages/5e/6e/f652c56bbb2c3d3fca252ffc7c0358597f57a1bbdf484dac683054950c63/ruamel.yaml.clib-0.2.2-cp37-cp37m-manylinux1_x86_64.whl (547kB) |████████████████████████████████| 552kB 15.7MB/s Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.4.0) Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.12.0) Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.12) Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.2.0) Requirement already satisfied: protobuf&gt;=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (3.12.4) Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.36.2) Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.34.1) Requirement already satisfied: tensorflow-estimator&lt;2.6.0,&gt;=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (2.5.0) Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.1.2) Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.15.0) Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.12.1) Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.6.3) Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.1.0) Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (2.5.0) Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (2.5.0.dev2021032900) Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (3.7.4.3) Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (3.1.0) Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (3.3.0) Requirement already satisfied: cffi&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile&gt;=0.9.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (1.14.5) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch&gt;=1.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (2.23.0) Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch&gt;=1.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (1.4.4) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch&gt;=1.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (20.9) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba&gt;=0.43.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (57.0.0) Requirement already satisfied: llvmlite&lt;0.35,&gt;=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba&gt;=0.43.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (0.34.0) Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segments-&gt;phonemizer==2.1-&gt;-r TransformerTTS/requirements.txt (line 4)) (2019.12.20) Collecting clldutils&gt;=1.7.3 Downloading https://files.pythonhosted.org/packages/f7/99/3ea7e3595e730332c2799938e2dad456916772e571fa0cd8dcdfb9d5780a/clldutils-3.9.0-py2.py3-none-any.whl (195kB) |████████████████████████████████| 204kB 18.3MB/s Collecting csvw&gt;=1.5.6 Downloading https://files.pythonhosted.org/packages/55/ae/afb43a6b88c4202d29e4ec7aca76633d8c530140f4f5a32ee762d07c4607/csvw-1.11.0-py2.py3-none-any.whl Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.0.1) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.6.1) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.4.4) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.8.0) Requirement already satisfied: google-auth&lt;2,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.30.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (3.3.4) Requirement already satisfied: cached-property; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.5.2) Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi&gt;=1.0-&gt;soundfile&gt;=0.9.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (2.20) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;pooch&gt;=1.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;pooch&gt;=1.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;pooch&gt;=1.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (2020.12.5) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;pooch&gt;=1.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (2.10) Collecting colorlog Downloading https://files.pythonhosted.org/packages/32/e6/e9ddc6fa1104fda718338b341e4b3dc31cd8039ab29e52fc73b508515361/colorlog-5.0.1-py2.py3-none-any.whl Requirement already satisfied: tabulate&gt;=0.7.7 in /usr/local/lib/python3.7/dist-packages (from clldutils&gt;=1.7.3-&gt;segments-&gt;phonemizer==2.1-&gt;-r TransformerTTS/requirements.txt (line 4)) (0.8.9) Requirement already satisfied: uritemplate&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from csvw&gt;=1.5.6-&gt;segments-&gt;phonemizer==2.1-&gt;-r TransformerTTS/requirements.txt (line 4)) (3.0.1) Collecting rfc3986 Downloading https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl Collecting isodate Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB) |████████████████████████████████| 51kB 7.9MB/s Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.3.0) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (4.2.2) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.2.8) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4; python_version &gt;= &#34;3.6&#34; in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (4.7.2) Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from markdown&gt;=2.6.8-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (4.0.1) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (3.1.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.4.8) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;markdown&gt;=2.6.8-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (3.4.1) Installing collected packages: colorlog, rfc3986, isodate, csvw, clldutils, segments, phonemizer, ruamel.yaml.clib, ruamel.yaml Successfully installed clldutils-3.9.0 colorlog-5.0.1 csvw-1.11.0 isodate-0.6.0 phonemizer-2.1 rfc3986-1.5.0 ruamel.yaml-0.17.7 ruamel.yaml.clib-0.2.2 segments-2.2.0 --2021-06-07 01:11:12-- https://public-asai-dl-models.s3.eu-central-1.amazonaws.com/TransformerTTS/ljspeech_wavernn_forward_transformer.zip Resolving public-asai-dl-models.s3.eu-central-1.amazonaws.com (public-asai-dl-models.s3.eu-central-1.amazonaws.com)... 52.219.168.9 Connecting to public-asai-dl-models.s3.eu-central-1.amazonaws.com (public-asai-dl-models.s3.eu-central-1.amazonaws.com)|52.219.168.9|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 210039489 (200M) [application/zip] Saving to: ‘ljspeech_wavernn_forward_transformer.zip’ ljspeech_wavernn_fo 100%[===================&gt;] 200.31M 21.1MB/s in 11s 2021-06-07 01:11:23 (19.0 MB/s) - ‘ljspeech_wavernn_forward_transformer.zip’ saved [210039489/210039489] Archive: ljspeech_wavernn_forward_transformer.zip creating: ljspeech_wavernn_forward_transformer/ inflating: __MACOSX/._ljspeech_wavernn_forward_transformer inflating: ljspeech_wavernn_forward_transformer/.DS_Store inflating: __MACOSX/ljspeech_wavernn_forward_transformer/._.DS_Store creating: ljspeech_wavernn_forward_transformer/wavernn/ inflating: __MACOSX/ljspeech_wavernn_forward_transformer/._wavernn inflating: ljspeech_wavernn_forward_transformer/wavernn/.DS_Store inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/._.DS_Store inflating: ljspeech_wavernn_forward_transformer/wavernn/forward_config.yaml inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/._forward_config.yaml creating: ljspeech_wavernn_forward_transformer/wavernn/forward_weights/ inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/._forward_weights inflating: ljspeech_wavernn_forward_transformer/wavernn/data_config.yaml inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/._data_config.yaml inflating: ljspeech_wavernn_forward_transformer/wavernn/forward_weights/checkpoint inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/forward_weights/._checkpoint inflating: ljspeech_wavernn_forward_transformer/wavernn/forward_weights/ckpt-133.index inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/forward_weights/._ckpt-133.index inflating: ljspeech_wavernn_forward_transformer/wavernn/forward_weights/ckpt-133.data-00001-of-00002 inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/forward_weights/._ckpt-133.data-00001-of-00002 inflating: ljspeech_wavernn_forward_transformer/wavernn/forward_weights/ckpt-133.data-00000-of-00002 inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/forward_weights/._ckpt-133.data-00000-of-00002 Archive: WaveRNN/pretrained/ljspeech.wavernn.mol.800k.zip inflating: WaveRNN/pretrained/latest_weights.pyt . . Load pre trained models . from pathlib import Path WaveRNN_path = &#39;WaveRNN/&#39; TTS_path = &#39;TransformerTTS/&#39; config_path = Path(&#39;ljspeech_wavernn_forward_transformer/wavernn&#39;) # wavernn model import sys sys.path.append(WaveRNN_path) from utils.dsp import hp from models.fatchord_version import WaveRNN import torch import numpy as np WaveRNN_path = Path(WaveRNN_path) # Load pretrained model try: hp.configure(WaveRNN_path / &#39;hparams.py&#39;) # Load hparams from file except: # cant reconfigure, bypass to avoid restart runtime pass if torch.cuda.is_available(): device = torch.device(&#39;cuda&#39;) else: device = torch.device(&#39;cpu&#39;) wave_model = WaveRNN(rnn_dims=hp.voc_rnn_dims, fc_dims=hp.voc_fc_dims, bits=hp.bits, pad=hp.voc_pad, upsample_factors=hp.voc_upsample_factors, feat_dims=hp.num_mels, compute_dims=hp.voc_compute_dims, res_out_dims=hp.voc_res_out_dims, res_blocks=hp.voc_res_blocks, hop_length=hp.hop_length, sample_rate=hp.sample_rate, mode=hp.voc_mode).to(device) wave_model.load(str(WaveRNN_path / &#39;pretrained/latest_weights.pyt&#39;)) # Ignore some TF warnings import tensorflow as tf tf.get_logger().setLevel(&#39;ERROR&#39;) # fix deprecated module on librosa import soundfile as sf import librosa class output: write_wav = lambda path, data, sr: sf.write(path, data, samplerate=sr, subtype=&#39;PCM_24&#39;) librosa.output = output # Generate sample with pre-trained WaveRNN vocoder hp_data = hp def generate(mel, file_name=&quot;sample.wav&quot;, batch_pred=False, batch_size=5000, hp=hp_data): _ = wave_model.generate(mel.clip(0,1)[np.newaxis,:,:], file_name, batch_pred, batch_size, hp.voc_overlap, hp.mu_law) # Load wav file ipd.display(ipd.Audio(file_name)) # ljspeech_wavernn_forward_model sys.path.remove(&#39;WaveRNN/&#39;) sys.modules.pop(&#39;utils&#39;) sys.path.append(TTS_path) # Load pretrained models from utils.config_manager import ConfigManager from utils.audio import Audio import IPython.display as ipd config_loader = ConfigManager(str(config_path), model_kind=&#39;forward&#39;) audio = Audio(config_loader.config) model = config_loader.load_model(str(config_path / &#39;forward_weights/ckpt-133&#39;)) . Trainable Parameters: 4.234M WARNING: could not retrieve git hash. Command &#39;[&#39;git&#39;, &#39;describe&#39;, &#39;--always&#39;]&#39; returned non-zero exit status 128. WARNING: could not check git hash. Command &#39;[&#39;git&#39;, &#39;describe&#39;, &#39;--always&#39;]&#39; returned non-zero exit status 128. restored weights from ljspeech_wavernn_forward_transformer/wavernn/forward_weights/ckpt-133 at step 665000 . sentence = &#39;Transformer TTS: A Text-to-Speech Transformer in TensorFlow 2, Audio synthesis with Forward Transformer TTS and WaveRNN Vocoder&#39; #@param {type:&quot;string&quot;} speed_regulator = 1 #@param {type:&quot;slider&quot;, min:0, max:2, step:0.1} batch_pred = True #@param {type:&quot;boolean&quot;} batch_size = 1 #@param out_normal = model.predict(sentence, speed_regulator=speed_regulator) # Normalize for WaveRNN mel = (out_normal[&#39;mel&#39;].numpy().T+4.)/8. generate(mel, batch_pred=batch_pred, batch_size=batch_size) . | ████████████████ 445500/445905 | Batch Size: 405 | Gen Rate: 233.7kHz | . Your browser does not support the audio element.",
            "url": "https://joaorafaelm.github.io/notebook/tensorflow/transformer/tts/2019/06/04/forward-tts-transformer-and-wavernn-vocoder.html",
            "relUrl": "/tensorflow/transformer/tts/2019/06/04/forward-tts-transformer-and-wavernn-vocoder.html",
            "date": " 2019 Jun 04"
        }
        
    
  
    
        ,"post7": {
            "title": "Text Classification with Python",
            "content": "If you are already familiar with what text classification is, you might want to jump to this part, or get the code here. . What is Text Classification? . Document or text classification is used to classify information, that is, assign a category to a text; it can be a document, a tweet, a simple message, an email, and so on. In this article, I will show how you can classify retail products into categories. Although in this example the categories are structured in a hierarchy, to keep it simple I will consider all subcategories as top-level. . If you are looking for complex implementations of large scale hierarchical text classification, I will leave links to some really good papers and projects at the end of this post. . Getting started . Now, before you go any further, make sure you have installed Python3+ and virtualenv (optional, but I highly recommend you to use it). . Let’s break down the problem into steps: . Setting up the environment | Gathering the data | Extracting features from the dataset | Testing the algorithms | . Setting up the environment . The main packages used in this projects are: sklearn, nltk and dataset. Due to the size of the data-set, it might take some time to clone/download the repository; NLTK data is also considerably big. Run the following commands to setup the project structure and download the required packages: . # Clone the repo git clone https://github.com/joaorafaelm/text-classification-python; cd text-classification-python; # Create virtualenv; skip this one if you dont have virtualenv. virtualenv venv &amp;&amp; source venv/bin/activate; # Install all requirements pip install -r requirements.txt; # Download all data that NLTK uses python -m nltk.downloader all; . Gathering the data . The dataset that will be used was created by scraping some products from Amazon. Scraping might be fine for projects where only a small amount of data is required, but it can be a really slow process since it is very simple for a server to detect a robot, unless you are rotating over a list of proxies, which can slow the process even more. . Using this script, I downloaded information of over 22,000 products, organized into 42 top-level categories, and a total of 6233 subcategories. See the whole category tree structure here. . Again, to keep it simple I will be using only 3 top-level categories: Automotive, Home &amp; Kitchen and Industrial &amp; Scientific. Including the subcategories, there are 36 categories in total. . To extract the data from database, run the command: . # dump from db to dumps/all_products.json datafreeze .datafreeze.yaml; . Inside the project you will also find a file called data_prep.py, in this file you can set the categories you want to use, the minimum amount of samples per category and the depth of a category. As I said before, only 3 categories are going to be used: Home &amp; Kitchen, Industrial &amp; Scientific and Automotive. I did not specify the depth of the subcategories, but I did specify 50 as the minimum amount of samples (is this case, products) per category. To transform the data dumped from the database into this “filtered” data, just execute the file: . python data_prep.py . The script will create a new file called products.json at the root of the project, and print out the category tree structure. Change the value of the variables default_depth, min_samples and domain if you need more data. . Extracting features from the dataset . In order to run machine learning algorithms, we need to transform the text into numerical vectors. Bag-of-words is one of the most used models, it assigns a numerical value to a word, creating a list of numbers. It can also assign a value to a set of words, known as N-gram. . Scikit provides a vectorizer called TfidfVectorizer which transforms the text based on the bag-of-words/n-gram model, additionally, it computes term frequencies and evaluate each word using the tf-idf weighting scheme. . Counting terms frequencies might not be enough sometimes. Take the words ‘cars’ and ‘car’ for example, by only using tf-idf, they are considered different words. This problem can be solved using Stemming and/or Lemmatisation. And there is where NLTK comes into play. . NLTK offers some pretty useful tools for NLP. For this project I used it to perform Lemmatisation and Part-of-speech tagging. . With Lemmatisation we can group together the inflected forms of a word. For example, the words ‘walked’, ‘walks’ and ‘walking’, can be grouped into their base form, the verb ‘walk’. That is why we need to POS tag each word as a noun, verb, adverb, and so on. . It is also worth noting that some words despite the fact that they appear frequently, they do not really make any difference for classification, in fact they could even help misclassify a text. Words like ‘a’, ‘an’, ‘the’, ‘to’, ‘or’ etc, are known as stop-words. These words can be ignored during the tokenization process. . Testing the algorithms . Now that we have all the features and labels, it is time to train the classifiers. There are a number of algorithms you can use for this type of problem, for example: Multinomial Naive Bayes, Linear SVC, SGD Classifier, K-Neighbors Classifier, Random Forest Classifier. Inside the file classify.py you can find an example using the SGDClassifier. Run it yourself using the command: . python classify.py . It will print out the accuracy of each category, along with the confusion matrix. . Here is how it is implemented: load the dataset, initiate WordNetLemmatizer and PerceptronTagger from NLTK. As I was only interested in nouns, verbs, adverbs and adjectives, I created a lookup dict to quicken up the process. Although NLTK is great, its aim is not performance, so I also implemented python’s LRU Cache for both lemmatize and tagger functions. . # Load data dataset = json.load(open(&#39;products.json&#39;, encoding=&#39;utf-8&#39;)) # Initiate lemmatizer wnl = WordNetLemmatizer() # Load tagger pickle tagger = PerceptronTagger() # Lookup if tag is noun, verb, adverb or an adjective tags = {&#39;N&#39;: wn.NOUN, &#39;V&#39;: wn.VERB, &#39;R&#39;: wn.ADV, &#39;J&#39;: wn.ADJ} # Memoization of POS tagging and Lemmatizer lemmatize_mem = lru_cache(maxsize=10000)(wnl.lemmatize) tagger_mem = lru_cache(maxsize=10000)(tagger.tag) . Next, the tokenizer function was created. It breaks the text into words and iterate over them, ignoring the stop-words and POS-tagging/Lemmatising the rest. This function will receive all documents from the dataset. . # POS tag sentences and lemmatize each word def tokenizer(text): for token in wordpunct_tokenize(text): if token not in ENGLISH_STOP_WORDS: tag = tagger_mem(frozenset({token})) yield lemmatize_mem(token, tags.get(tag[0][1], wn.NOUN)) . At last the pipeline is defined; the first step is to call TfidfVectorizer, with the tokenizer function preprocessing each document, and then pass through the SGDClassifier. The classifier is trained and tested using 10-fold Cross-Validation provided by the cross_val_predict method from scikit-learn. . # Pipeline definition pipeline = Pipeline([ (&#39;vectorizer&#39;, TfidfVectorizer( tokenizer=tokenizer, ngram_range=(1, 2), stop_words=ENGLISH_STOP_WORDS, sublinear_tf=True, min_df=0.00009 )), (&#39;classifier&#39;, SGDClassifier( alpha=1e-4, n_jobs=-1 )), ]) # Cross validate using k-fold y_pred = cross_val_predict( pipeline, dataset.get(&#39;data&#39;), y=dataset.get(&#39;target&#39;), cv=10, n_jobs=-1, verbose=20 ) # Print out precison, recall and f1 scode. print(classification_report( dataset.get(&#39;target&#39;), y_pred, target_names=dataset.get(&#39;target_names&#39;), digits=3 )) . And here are the accuracy results for each algorithm I tested (all algorithms were tested with their default parameters): . Algorithms Precision Recall . SGDClassifier | 0.975 | 0.975 | . LinearSVC | 0.972 | 0.971 | . RandomForest | 0.938 | 0.936 | . MultinomialNB | 0.882 | 0.851 | . The precision is the percentage of the test samples that were classified to the category and actually belonged to the category. . The recall is the percentage of all the test samples that originally belonged to the category and in the evaluation process were correctly classified to the category. . Conclusion . As the category tree gets bigger, and you have more and more data to classify, you cannot use a model as simple as the one above (well, you can but its precision will be very low, not to mention the computational cost). Another important thing to notice, is how you structure the categories, in amazon category structure, a lot of subcategories are so confused that I doubt even humans could correctly classify products to them. The full code of this post can be found here. . If you noticed something wrong, or you know something that can make the algorithms better, please do comment bellow. Thanks for reading! . Further reading . Classifier Statistics . | A Meta-Top-Down Method for Large-Scale Hierarchical Classification . | A survey of hierarchical classification across different application domains . | Hierarchical Text Categorization and Its Application to Bioinformatics . | Comparing Several Approaches for Hierarchical Classification of Proteins with Decision Trees . | Tokenizing Words and Sentences with NLTK . | Natural Language Processing with Deep Learning . | Document Classification using Multinomial Naive Bayes Classifier . | .",
            "url": "https://joaorafaelm.github.io/notebook/nlp/2017/08/24/text-classification-with-python.html",
            "relUrl": "/nlp/2017/08/24/text-classification-with-python.html",
            "date": " 2017 Aug 24"
        }
        
    
  
    
        ,"post8": {
            "title": "GraphQL and Django in 5 minutes",
            "content": "TL;DR Jump to the coding part or get the code here. . What is GraphQL? . GraphQL query is a string that is sent to a server to be interpreted and fulfilled, which then returns JSON back to the client. It was created by Facebook in 2012 and the first specification draft was made public in 2015. . In this tutorial I will cover the basics of working with GraphQL and Django. . Getting started . Before creating the project and all, make sure you have virtualenv installed, so that the packages used in this tutorial won’t be installed system-wide. . # Clone the repo git clone https://github.com/joaorafaelm/graphql-django-example; cd graphql-django-example; # Create virtualenv virtualenv venv &amp;&amp; source venv/bin/activate; # Install django and graphene pip install -r requirements.txt; # Setup db python manage.py migrate; . Run python manage.py loaddata books.json to populate the db, or run python manage.py createsuperuser and then add some data using the admin interface. . Models and GraphQL Schema . This example is going to use the models Author, Book and Publisher. . # bookstore/store/models.py from django.db import models class Publisher(models.Model): name = models.CharField(max_length=30) website = models.URLField() def __str__(self): return self.name class Author(models.Model): first_name = models.CharField(max_length=30) last_name = models.CharField(max_length=40) email = models.EmailField() def __str__(self): return &#39;%s %s&#39; % (self.first_name, self.last_name) class Book(models.Model): title = models.CharField(max_length=100) authors = models.ManyToManyField(Author) publisher = models.ForeignKey(Publisher) publication_date = models.DateField() def __str__(self): return self.title . After creating the models, the Schema should be created, which will be used to serve the API. . At the time of writing this post, graphene-django version is 1.3 and it does not handle ManyToMany fields properly, that is why the resolve_authors method was added. This issue has been resolved for the next release. . # bookstore/schema.py import graphene from graphene_django.types import DjangoObjectType from graphene_django.debug import DjangoDebug from bookstore.store.models import Author, Book, Publisher class AuthorType(DjangoObjectType): class Meta: model = Author class BookType(DjangoObjectType): authors = graphene.List(AuthorType) # Many To Many fix until next release. # https://github.com/graphql-python/graphene-django/issues/155 @graphene.resolve_only_args def resolve_authors(self): return self.authors.all() class Meta: model = Book class PublisherType(DjangoObjectType): class Meta: model = Publisher class Query(graphene.ObjectType): all_authors = graphene.List(AuthorType) all_books = graphene.List(BookType) all_publishers = graphene.List(PublisherType) # Debug field (rawSql, parameters etc). debug = graphene.Field(DjangoDebug, name=&#39;__debug&#39;) def resolve_all_authors(self, args, context, info): return Author.objects.all() def resolve_all_books(self, args, context, info): return Book.objects.select_related(&#39;publisher&#39;).all() def resolve_all_publishers(self, args, context, info): return Publisher.objects.all() schema = graphene.Schema(query=Query) . Last but not least, the GraphQL URL must be added into the urls.py file. . # bookstore/urls.py from django.conf.urls import url from django.contrib import admin from graphene_django.views import GraphQLView from bookstore.schema import schema urlpatterns = [ url(r&#39;^admin/&#39;, admin.site.urls), url(r&#39;^graphql&#39;, GraphQLView.as_view(graphiql=True, schema=schema)), ] . You can now run python manage.py runserver and start using the API at http://localhost:8000/graphql. . Querying and debugging . GraphiQL provides a graphical interactive in-browser GraphQL IDE, including some features such as syntax highlighting, real-time error reporting, automatic query completion etc. . I will show some query examples, but you can learn more about querying at graphql.org/learn/queries/. . Given the following query, we can retrieve all books registered along with their authors. . { allBooks { title, authors { firstName, lastName } } } . And the response… . { &quot;data&quot;: { &quot;allBooks&quot;: [ { &quot;title&quot;: &quot;Resurrection&quot;, &quot;authors&quot;: [ { &quot;firstName&quot;: &quot;Leo&quot;, &quot;lastName&quot;: &quot;Tolstoy&quot; } ] }, { &quot;title&quot;: &quot;Childhood&quot;, &quot;authors&quot;: [ { &quot;firstName&quot;: &quot;Leo&quot;, &quot;lastName&quot;: &quot;Tolstoy&quot; } ] } ] } } . Using the __debug field you can get information about the actual SQL query. . { allAuthors {lastName} __debug { sql {rawSql, duration} } } . Response: . { &quot;data&quot;: { &quot;allAuthors&quot;: [ { &quot;lastName&quot;: &quot;King&quot; }, { &quot;lastName&quot;: &quot;Tolstoy&quot; }, { &quot;lastName&quot;: &quot;Gaiman&quot; }, { &quot;lastName&quot;: &quot;Pratchett&quot; } ], &quot;__debug&quot;: { &quot;sql&quot;: [ { &quot;rawSql&quot;: &quot;SELECT &quot;store_author &quot;. &quot;id &quot;, &quot;store_author &quot;. &quot;first_name &quot;, &quot;store_author &quot;. &quot;last_name &quot;, &quot;store_author &quot;. &quot;email &quot; FROM &quot;store_author &quot;&quot;, &quot;duration&quot;: 0.0009260177612304688 } ] } } } . All this code is on my Github. Please do fork it and make pull requests regarding any issues or improvements you may have with my code. .",
            "url": "https://joaorafaelm.github.io/notebook/graphql/django/2017/08/05/graphql-and-django-in-5-minutes.html",
            "relUrl": "/graphql/django/2017/08/05/graphql-and-django-in-5-minutes.html",
            "date": " 2017 Aug 05"
        }
        
    
  

  
  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://joaorafaelm.github.io/notebook/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
