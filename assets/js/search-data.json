{
  
    
        "post0": {
            "title": "Deploying a XGBoost model",
            "content": "import pandas as pd import numpy as np import xgboost as xgb import re from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error np.random.seed(42) . Load data and build a model . X, y = fetch_openml(&quot;titanic&quot;, version=1, as_frame=True, return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2) . X_y_train = xgb.DMatrix(data=X_train[[&quot;pclass&quot;, &quot;age&quot;, &quot;fare&quot;, &quot;sibsp&quot;, &quot;parch&quot;]], label=y_train.astype(int)) X_test = xgb.DMatrix(data=X_test[[&quot;pclass&quot;, &quot;age&quot;, &quot;fare&quot;, &quot;sibsp&quot;, &quot;parch&quot;]]) . X_train[[&quot;pclass&quot;, &quot;age&quot;, &quot;fare&quot;, &quot;sibsp&quot;, &quot;parch&quot;]].head() . pclass age fare sibsp parch . 662 3.0 | 40.0 | 7.2250 | 0.0 | 0.0 | . 164 1.0 | 35.0 | 26.5500 | 0.0 | 0.0 | . 871 3.0 | NaN | 7.7500 | 0.0 | 0.0 | . 1298 3.0 | 36.0 | 9.5000 | 0.0 | 0.0 | . 1004 3.0 | NaN | 7.7875 | 0.0 | 0.0 | . params = { &quot;base_score&quot;: np.mean(y_train.astype(int)), &quot;eta&quot;: 0.1, &quot;max_depth&quot;: 3, &quot;gamma&quot;: 3, &quot;objective&quot;: &quot;reg:squarederror&quot;, &quot;eval_metric&quot;: &quot;mae&quot; } model = xgb.train( params=params, dtrain=X_y_train, num_boost_round=3 ) . Visualization of model . xgb.to_graphviz(booster = model, num_trees=0) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 0 fare&lt;15.1729002 1 age&lt;16.5 0&#45;&gt;1 yes, missing 2 pclass&lt;2.5 0&#45;&gt;2 no 3 leaf=0.0223644935 1&#45;&gt;3 yes 4 leaf=&#45;0.0156309232 1&#45;&gt;4 no, missing 5 parch&lt;0.5 2&#45;&gt;5 yes, missing 6 fare&lt;23.3500004 2&#45;&gt;6 no 11 leaf=0.0153174726 5&#45;&gt;11 yes, missing 12 leaf=0.03650124 5&#45;&gt;12 no 13 leaf=0.00960917864 6&#45;&gt;13 yes 14 leaf=&#45;0.0235449504 6&#45;&gt;14 no, missing xgb.to_graphviz(booster = model, num_trees=1) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 0 fare&lt;15.1729002 1 age&lt;16.5 0&#45;&gt;1 yes, missing 2 pclass&lt;2.5 0&#45;&gt;2 no 3 leaf=0.0201979335 1&#45;&gt;3 yes 4 leaf=&#45;0.0140708359 1&#45;&gt;4 no, missing 5 leaf=0.0204214789 2&#45;&gt;5 yes, missing 6 fare&lt;23.3500004 2&#45;&gt;6 no 13 leaf=0.00866124686 6&#45;&gt;13 yes 14 leaf=&#45;0.0212272462 6&#45;&gt;14 no, missing xgb.to_graphviz(booster = model, num_trees=2) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 0 fare&lt;15.1729002 1 leaf=&#45;0.010894442 0&#45;&gt;1 yes, missing 2 pclass&lt;2.5 0&#45;&gt;2 no 5 leaf=0.0183849707 2&#45;&gt;5 yes, missing 6 leaf=&#45;0.00472340686 2&#45;&gt;6 no print(&quot; n&quot;.join(model.get_dump())) . 0:[fare&lt;15.1729002] yes=1,no=2,missing=1 1:[age&lt;16.5] yes=3,no=4,missing=4 3:leaf=0.0223644935 4:leaf=-0.0156309232 2:[pclass&lt;2.5] yes=5,no=6,missing=5 5:[parch&lt;0.5] yes=11,no=12,missing=11 11:leaf=0.0153174726 12:leaf=0.03650124 6:[fare&lt;23.3500004] yes=13,no=14,missing=14 13:leaf=0.00960917864 14:leaf=-0.0235449504 0:[fare&lt;15.1729002] yes=1,no=2,missing=1 1:[age&lt;16.5] yes=3,no=4,missing=4 3:leaf=0.0201979335 4:leaf=-0.0140708359 2:[pclass&lt;2.5] yes=5,no=6,missing=5 5:leaf=0.0204214789 6:[fare&lt;23.3500004] yes=13,no=14,missing=14 13:leaf=0.00866124686 14:leaf=-0.0212272462 0:[fare&lt;15.1729002] yes=1,no=2,missing=1 1:leaf=-0.010894442 2:[pclass&lt;2.5] yes=5,no=6,missing=5 5:leaf=0.0183849707 6:leaf=-0.00472340686 . Convert dump string to a .py file . def string_parser(s): if len(re.findall(r&quot;:leaf=&quot;, s)) == 0: out = re.findall(r&quot;[ w.-]+&quot;, s) tabs = re.findall(r&quot;[ t]+&quot;, s) if (out[4] == out[8]): missing_value_handling = (&quot; or np.isnan(x[&#39;&quot; + out[1] + &quot;&#39;]) &quot;) else: missing_value_handling = &quot;&quot; if len(tabs) &gt; 0: return (re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; if state == &#39; + out[0] + &#39;: n&#39; + re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; state = (&#39; + out[4] + &#39; if &#39; + &quot;x[&#39;&quot; + out[1] +&quot;&#39;]&lt;&quot; + out[2] + missing_value_handling + &#39; else &#39; + out[6] + &#39;) n&#39; ) else: return (&#39; if state == &#39; + out[0] + &#39;: n&#39; + &#39; state = (&#39; + out[4] + &#39; if &#39; + &quot;x[&#39;&quot; + out[1] +&quot;&#39;]&lt;&quot; + out[2] + missing_value_handling + &#39; else &#39; + out[6] + &#39;) n&#39; ) else: out = re.findall(r&quot;[ d.-]+&quot;, s) return (re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; if state == &#39; + out[0] + &#39;: n &#39; + re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; return &#39; + out[1] + &#39; n&#39;) def tree_parser(tree, i): if i == 0: return (&#39; if num_booster == 0: n state = 0 n&#39; + &quot;&quot;.join([string_parser(tree.split(&#39; n&#39;)[i]) for i in range(len(tree.split(&#39; n&#39;))-1)])) else: return (&#39; elif num_booster == &#39;+str(i)+&#39;: n state = 0 n&#39; + &quot;&quot;.join([string_parser(tree.split(&#39; n&#39;)[i]) for i in range(len(tree.split(&#39; n&#39;))-1)])) def model_to_py(base_score, model, out_file): trees = model.get_dump() result = [&quot;import numpy as np n n&quot; +&quot;def xgb_tree(x, num_booster): n&quot;] for i in range(len(trees)): result.append(tree_parser(trees[i], i)) with open(out_file, &#39;w&#39;) as the_file: the_file.write(&quot;&quot;.join(result) + &quot; ndef xgb_predict(x): n predict = &quot; + str(base_score) + &quot; n&quot; + &quot;# initialize prediction with base score n&quot; + &quot; for i in range(&quot; + str(len(trees)) + &quot;): n predict = predict + xgb_tree(x, i)&quot; + &quot; n return predict&quot;) model_to_py(params[&#39;base_score&#39;], model, &#39;xgb_model.py&#39;) . Prediction using dump file . import xgb_model passenger_data_1 = {&#39;pclass&#39;:3, &#39;age&#39;:np.nan, &#39;sibsp&#39;:0, &#39;parch&#39;:0, &#39;fare&#39;:7.8958} passenger_data_2 = {&#39;pclass&#39;:1, &#39;age&#39;:46, &#39;sibsp&#39;:0, &#39;parch&#39;:0, &#39;fare&#39;:26} print(xgb_model.xgb_predict(passenger_data_1)) print(xgb_model.xgb_predict(passenger_data_2)) . 0.34144773395253103 0.43616785725253104 .",
            "url": "https://joaorafaelm.github.io/notebook/xgboost/2021/06/24/_deploying_xgboost_model.html",
            "relUrl": "/xgboost/2021/06/24/_deploying_xgboost_model.html",
            "date": " 2021 Jun 24"
        }
        
    
  
    
        ,"post1": {
            "title": "GPT-J-6B Inference Demo",
            "content": "Install Dependencies . First we download the model and install some dependencies. This step takes at least 5 minutes (possibly longer depending on server load). . Make sure you are using a TPU runtime! . !apt install zstd # the &quot;slim&quot; version contain only bf16 weights and no optimizer parameters, which minimizes bandwidth and memory !time wget -c https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd !time tar -I zstd -xf step_383500_slim.tar.zstd !git clone https://github.com/kingoflolz/mesh-transformer-jax.git !pip install -r mesh-transformer-jax/requirements.txt # jax 0.2.12 is required due to a regression with xmap in 0.2.13 !pip install mesh-transformer-jax/ jax==0.2.12 . Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: zstd 0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded. Need to get 278 kB of archives. After this operation, 1,141 kB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 zstd amd64 1.3.3+dfsg-2ubuntu1.2 [278 kB] Fetched 278 kB in 1s (375 kB/s) Selecting previously unselected package zstd. (Reading database ... 160772 files and directories currently installed.) Preparing to unpack .../zstd_1.3.3+dfsg-2ubuntu1.2_amd64.deb ... Unpacking zstd (1.3.3+dfsg-2ubuntu1.2) ... Setting up zstd (1.3.3+dfsg-2ubuntu1.2) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... --2021-06-25 01:33:38-- https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd Resolving the-eye.eu (the-eye.eu)... 162.213.130.242 Connecting to the-eye.eu (the-eye.eu)|162.213.130.242|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 9414712325 (8.8G) [application/octet-stream] Saving to: ‘step_383500_slim.tar.zstd’ step_383500_slim.ta 100%[===================&gt;] 8.77G 34.6MB/s in 3m 19s 2021-06-25 01:36:56 (45.2 MB/s) - ‘step_383500_slim.tar.zstd’ saved [9414712325/9414712325] real 3m21.064s user 0m5.690s sys 0m23.579s real 4m0.314s user 0m35.084s sys 0m40.486s Cloning into &#39;mesh-transformer-jax&#39;... remote: Enumerating objects: 531, done. remote: Counting objects: 100% (130/130), done. remote: Compressing objects: 100% (61/61), done. remote: Total 531 (delta 98), reused 81 (delta 69), pack-reused 401 Receiving objects: 100% (531/531), 176.53 KiB | 3.46 MiB/s, done. Resolving deltas: 100% (346/346), done. Collecting git+https://github.com/deepmind/dm-haiku (from -r mesh-transformer-jax/requirements.txt (line 10)) Cloning https://github.com/deepmind/dm-haiku to /tmp/pip-req-build-1chvq9i7 Running command git clone -q https://github.com/deepmind/dm-haiku /tmp/pip-req-build-1chvq9i7 Collecting git+https://github.com/EleutherAI/lm-evaluation-harness/ (from -r mesh-transformer-jax/requirements.txt (line 11)) Cloning https://github.com/EleutherAI/lm-evaluation-harness/ to /tmp/pip-req-build-m_53n_vt Running command git clone -q https://github.com/EleutherAI/lm-evaluation-harness/ /tmp/pip-req-build-m_53n_vt Requirement already satisfied: numpy~=1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r mesh-transformer-jax/requirements.txt (line 1)) (1.19.5) Collecting transformers~=4.4.2 Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB) |████████████████████████████████| 2.0MB 6.8MB/s Collecting tqdm~=4.45.0 Downloading https://files.pythonhosted.org/packages/4a/1c/6359be64e8301b84160f6f6f7936bbfaaa5e9a4eab6cbc681db07600b949/tqdm-4.45.0-py2.py3-none-any.whl (60kB) |████████████████████████████████| 61kB 6.8MB/s Collecting setuptools~=51.3.3 Downloading https://files.pythonhosted.org/packages/b2/81/509db0082c0d2ca2af307c6652ea422865de1f83c14b1e1f3549e415cfac/setuptools-51.3.3-py3-none-any.whl (786kB) |████████████████████████████████| 788kB 21.8MB/s Collecting wandb~=0.10.22 Downloading https://files.pythonhosted.org/packages/e0/b4/9d92953d8cddc8450c859be12e3dbdd4c7754fb8def94c28b3b351c6ee4e/wandb-0.10.32-py2.py3-none-any.whl (1.8MB) |████████████████████████████████| 1.8MB 29.3MB/s Collecting einops~=0.3.0 Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl Collecting requests~=2.25.1 Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB) |████████████████████████████████| 61kB 7.6MB/s Collecting fabric~=2.6.0 Downloading https://files.pythonhosted.org/packages/c1/9d/59df62b620985871a4ba7d8b509b84340bbd1573257e55a427ae2df2d56e/fabric-2.6.0-py2.py3-none-any.whl (53kB) |████████████████████████████████| 61kB 8.0MB/s Collecting optax==0.0.6 Downloading https://files.pythonhosted.org/packages/ec/7a/6259edd319ee7fa94dd23c54f15eff667f599d179e889af90fe0c204612c/optax-0.0.6-py3-none-any.whl (96kB) |████████████████████████████████| 102kB 11.2MB/s Collecting ray~=1.2.0 Downloading https://files.pythonhosted.org/packages/11/14/15d0f0aec20a4674a996429160565a071688f27f49f789327ebed8188ffb/ray-1.2.0-cp37-cp37m-manylinux2014_x86_64.whl (47.5MB) |████████████████████████████████| 47.5MB 88kB/s Requirement already satisfied: jax~=0.2.12 in /usr/local/lib/python3.7/dist-packages (from -r mesh-transformer-jax/requirements.txt (line 13)) (0.2.13) Requirement already satisfied: Flask~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r mesh-transformer-jax/requirements.txt (line 14)) (1.1.4) Requirement already satisfied: cloudpickle~=1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r mesh-transformer-jax/requirements.txt (line 15)) (1.3.0) Collecting tensorflow-cpu~=2.4.1 Downloading https://files.pythonhosted.org/packages/69/99/c4ffb7dec86049dcbe871262622caef477ff248963d0398d07c8d726a6de/tensorflow_cpu-2.4.2-cp37-cp37m-manylinux2010_x86_64.whl (144.3MB) |████████████████████████████████| 144.3MB 30kB/s Collecting google-cloud-storage~=1.36.2 Downloading https://files.pythonhosted.org/packages/f2/0e/da07ffa511daa559bcc209f9344d71a90ba4d7b391fb795e6282f86d2935/google_cloud_storage-1.36.2-py2.py3-none-any.whl (97kB) |████████████████████████████████| 102kB 11.6MB/s Requirement already satisfied: smart_open[gcs] in /usr/local/lib/python3.7/dist-packages (from -r mesh-transformer-jax/requirements.txt (line 18)) (5.1.0) Collecting func_timeout Downloading https://files.pythonhosted.org/packages/b3/0d/bf0567477f7281d9a3926c582bfef21bff7498fc0ffd3e9de21811896a0b/func_timeout-4.3.5.tar.gz (44kB) |████████████████████████████████| 51kB 6.8MB/s Collecting ftfy Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB) |████████████████████████████████| 71kB 8.2MB/s Requirement already satisfied: absl-py&gt;=0.7.1 in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.5.dev0-&gt;-r mesh-transformer-jax/requirements.txt (line 10)) (0.12.0) Collecting jmp&gt;=0.0.2 Downloading https://files.pythonhosted.org/packages/ff/5c/1482f4a4a502e080af2ca54d7f80a60b5d4735f464c151666d583b78c226/jmp-0.0.2-py3-none-any.whl Requirement already satisfied: tabulate&gt;=0.8.9 in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.5.dev0-&gt;-r mesh-transformer-jax/requirements.txt (line 10)) (0.8.9) Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.5.dev0-&gt;-r mesh-transformer-jax/requirements.txt (line 10)) (3.7.4.3) Collecting black==20.8b1 Downloading https://files.pythonhosted.org/packages/dc/7b/5a6bbe89de849f28d7c109f5ea87b65afa5124ad615f3419e71beb29dc96/black-20.8b1.tar.gz (1.1MB) |████████████████████████████████| 1.1MB 51.7MB/s Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done Collecting best_download&gt;=0.0.6 Downloading https://files.pythonhosted.org/packages/cd/8f/d8f2145cb0fd42ca55a6c6420d512156b6ff5d9857b3dd9cd0d679377cc5/best_download-0.0.7-py3-none-any.whl Collecting datasets&gt;=1.2.1 Downloading https://files.pythonhosted.org/packages/08/a2/d4e1024c891506e1cee8f9d719d20831bac31cb5b7416983c4d2f65a6287/datasets-1.8.0-py3-none-any.whl (237kB) |████████████████████████████████| 245kB 53.0MB/s Requirement already satisfied: click&gt;=7.1 in /usr/local/lib/python3.7/dist-packages (from lm-eval-harness==0.0.1-&gt;-r mesh-transformer-jax/requirements.txt (line 11)) (7.1.2) Collecting scikit-learn&gt;=0.24.1 Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB) |████████████████████████████████| 22.3MB 1.4MB/s Requirement already satisfied: torch&gt;=1.7 in /usr/local/lib/python3.7/dist-packages (from lm-eval-harness==0.0.1-&gt;-r mesh-transformer-jax/requirements.txt (line 11)) (1.9.0+cu102) Collecting sqlitedict==1.6.0 Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz Collecting pytablewriter==0.58.0 Downloading https://files.pythonhosted.org/packages/fd/e2/62b208cdb8771dee1849bd2b4ed129284e1efff7669985697e4c124c1000/pytablewriter-0.58.0-py3-none-any.whl (96kB) |████████████████████████████████| 102kB 11.6MB/s Collecting sacrebleu==1.5.0 Downloading https://files.pythonhosted.org/packages/3b/7f/4fd83db8570288c3899d8e57666c2841403c15659f3d792a3cb8dc1c6689/sacrebleu-1.5.0-py3-none-any.whl (65kB) |████████████████████████████████| 71kB 8.5MB/s Collecting pycountry==20.7.3 Downloading https://files.pythonhosted.org/packages/76/73/6f1a412f14f68c273feea29a6ea9b9f1e268177d32e0e69ad6790d306312/pycountry-20.7.3.tar.gz (10.1MB) |████████████████████████████████| 10.1MB 51.0MB/s Collecting numexpr==2.7.2 Downloading https://files.pythonhosted.org/packages/9c/f4/fa8755c1aa44b431267aa019922f6cc9ec099cef0c6fc0ead0f9a2aa59e5/numexpr-2.7.2-cp37-cp37m-manylinux2010_x86_64.whl (471kB) |████████████████████████████████| 481kB 50.5MB/s Collecting lm_dataformat==0.0.19 Downloading https://files.pythonhosted.org/packages/83/b5/8d10bf5a8082921792bb09c9d591dfd622cf4a16fbb7e283cc921c5ffc50/lm_dataformat-0.0.19-py3-none-any.whl Collecting pytest==6.2.3 Downloading https://files.pythonhosted.org/packages/76/4d/9c00146923da9f1cabd1878209d71b1380d537ec331a1a613e8f4b9d7985/pytest-6.2.3-py3-none-any.whl (280kB) |████████████████████████████████| 286kB 53.4MB/s Collecting pybind11==2.6.2 Downloading https://files.pythonhosted.org/packages/8d/43/7339dbabbc2793718d59703aace4166f53c29ee1c202f6ff5bf8a26c4d91/pybind11-2.6.2-py2.py3-none-any.whl (191kB) |████████████████████████████████| 194kB 59.3MB/s Collecting tqdm-multiprocess==0.0.11 Downloading https://files.pythonhosted.org/packages/25/7e/0d889fc6c84e3df6b69aaafe893fc77f69b3d968ac9ce574d1c62c688050/tqdm_multiprocess-0.0.11-py3-none-any.whl Collecting zstandard==0.15.2 Downloading https://files.pythonhosted.org/packages/5b/56/dc2a85d06e973f2ad96584b0e5b876d063135d449cb040aeaadd22a910f9/zstandard-0.15.2-cp37-cp37m-manylinux2014_x86_64.whl (2.2MB) |████████████████████████████████| 2.2MB 52.1MB/s Collecting jsonlines==2.0.0 Downloading https://files.pythonhosted.org/packages/d4/58/06f430ff7607a2929f80f07bfd820acbc508a4e977542fefcc522cde9dff/jsonlines-2.0.0-py3-none-any.whl Collecting mock==4.0.3 Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl Collecting openai==0.6.4 Downloading https://files.pythonhosted.org/packages/00/6b/ec780cfdd44ce0d7e56aae12477fe163ad152078cfdb744dfccab4ddae4b/openai-0.6.4.tar.gz (159kB) |████████████████████████████████| 163kB 56.9MB/s Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers~=4.4.2-&gt;-r mesh-transformer-jax/requirements.txt (line 2)) (3.0.12) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers~=4.4.2-&gt;-r mesh-transformer-jax/requirements.txt (line 2)) (2019.12.20) Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB) |████████████████████████████████| 3.3MB 50.9MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers~=4.4.2-&gt;-r mesh-transformer-jax/requirements.txt (line 2)) (20.9) Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from transformers~=4.4.2-&gt;-r mesh-transformer-jax/requirements.txt (line 2)) (4.5.0) Collecting sacremoses Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB) |████████████████████████████████| 901kB 48.1MB/s Collecting pathtools Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz Requirement already satisfied: python-dateutil&gt;=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb~=0.10.22-&gt;-r mesh-transformer-jax/requirements.txt (line 5)) (2.8.1) Requirement already satisfied: promise&lt;3,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb~=0.10.22-&gt;-r mesh-transformer-jax/requirements.txt (line 5)) (2.3) Collecting shortuuid&gt;=0.5.0 Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl Requirement already satisfied: psutil&gt;=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb~=0.10.22-&gt;-r mesh-transformer-jax/requirements.txt (line 5)) (5.4.8) Collecting sentry-sdk&gt;=0.4.0 Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB) |████████████████████████████████| 133kB 52.4MB/s Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb~=0.10.22-&gt;-r mesh-transformer-jax/requirements.txt (line 5)) (3.13) Requirement already satisfied: protobuf&gt;=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb~=0.10.22-&gt;-r mesh-transformer-jax/requirements.txt (line 5)) (3.12.4) Requirement already satisfied: six&gt;=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb~=0.10.22-&gt;-r mesh-transformer-jax/requirements.txt (line 5)) (1.15.0) Collecting docker-pycreds&gt;=0.4.0 Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl Collecting configparser&gt;=3.8.1 Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl Collecting GitPython&gt;=1.0.0 Downloading https://files.pythonhosted.org/packages/bc/91/b38c4fabb6e5092ab23492ded4f318ab7299b19263272b703478038c0fbc/GitPython-3.1.18-py3-none-any.whl (170kB) |████████████████████████████████| 174kB 52.2MB/s Collecting subprocess32&gt;=3.5.3 Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB) |████████████████████████████████| 102kB 11.6MB/s Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.25.1-&gt;-r mesh-transformer-jax/requirements.txt (line 7)) (1.24.3) Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.25.1-&gt;-r mesh-transformer-jax/requirements.txt (line 7)) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests~=2.25.1-&gt;-r mesh-transformer-jax/requirements.txt (line 7)) (2021.5.30) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests~=2.25.1-&gt;-r mesh-transformer-jax/requirements.txt (line 7)) (2.10) Collecting paramiko&gt;=2.4 Downloading https://files.pythonhosted.org/packages/95/19/124e9287b43e6ff3ebb9cdea3e5e8e88475a873c05ccdf8b7e20d2c4201e/paramiko-2.7.2-py2.py3-none-any.whl (206kB) |████████████████████████████████| 215kB 51.2MB/s Collecting pathlib2 Downloading https://files.pythonhosted.org/packages/e9/45/9c82d3666af4ef9f221cbb954e1d77ddbb513faf552aea6df5f37f1a4859/pathlib2-2.3.5-py2.py3-none-any.whl Collecting invoke&lt;2.0,&gt;=1.3 Downloading https://files.pythonhosted.org/packages/87/8f/c153d7db091f342da6bc97f7bedd1b2ce2867c4a8b0aab40fbba85a05e33/invoke-1.5.0-py3-none-any.whl (211kB) |████████████████████████████████| 215kB 55.6MB/s Collecting chex&gt;=0.0.4 Downloading https://files.pythonhosted.org/packages/f5/b9/445eb59ec23249acffc5322c79b07e20b12dbff45b9c1da6cdae9e947685/chex-0.0.7-py3-none-any.whl (52kB) |████████████████████████████████| 61kB 8.1MB/s Requirement already satisfied: jaxlib&gt;=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax==0.0.6-&gt;-r mesh-transformer-jax/requirements.txt (line 9)) (0.1.66+cuda110) Collecting redis&gt;=3.5.0 Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB) |████████████████████████████████| 81kB 9.3MB/s Collecting opencensus Downloading https://files.pythonhosted.org/packages/18/59/12044123133d000f705383ad98579aeb0dd82d66b33a254a21b54bf0d6bb/opencensus-0.7.13-py2.py3-none-any.whl (127kB) |████████████████████████████████| 133kB 56.9MB/s Requirement already satisfied: grpcio&gt;=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray~=1.2.0-&gt;-r mesh-transformer-jax/requirements.txt (line 12)) (1.34.1) Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray~=1.2.0-&gt;-r mesh-transformer-jax/requirements.txt (line 12)) (2.6.0) Collecting colorama Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl Collecting gpustat Downloading https://files.pythonhosted.org/packages/b4/69/d8c849715171aeabd61af7da080fdc60948b5a396d2422f1f4672e43d008/gpustat-0.6.0.tar.gz (78kB) |████████████████████████████████| 81kB 9.0MB/s Collecting py-spy&gt;=0.2.0 Downloading https://files.pythonhosted.org/packages/9d/4d/1a9cbe9a0b543e6733cb38afe26451522a9ef8e4897b59e74cc76838f245/py_spy-0.3.7-py2.py3-none-manylinux1_x86_64.whl (3.1MB) |████████████████████████████████| 3.1MB 46.7MB/s Collecting aioredis Downloading https://files.pythonhosted.org/packages/b0/64/1b1612d0a104f21f80eb4c6e1b6075f2e6aba8e228f46f229cfd3fdac859/aioredis-1.3.1-py3-none-any.whl (65kB) |████████████████████████████████| 71kB 9.1MB/s Requirement already satisfied: msgpack&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray~=1.2.0-&gt;-r mesh-transformer-jax/requirements.txt (line 12)) (1.0.2) Collecting aiohttp Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB) |████████████████████████████████| 1.3MB 47.5MB/s Collecting aiohttp-cors Downloading https://files.pythonhosted.org/packages/13/e7/e436a0c0eb5127d8b491a9b83ecd2391c6ff7dcd5548dfaec2080a2340fd/aiohttp_cors-0.7.0-py3-none-any.whl Collecting colorful Downloading https://files.pythonhosted.org/packages/b0/8e/e386e248266952d24d73ed734c2f5513f34d9557032618c8910e605dfaf6/colorful-0.5.4-py2.py3-none-any.whl (201kB) |████████████████████████████████| 204kB 55.3MB/s Requirement already satisfied: prometheus-client&gt;=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray~=1.2.0-&gt;-r mesh-transformer-jax/requirements.txt (line 12)) (0.11.0) Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax~=0.2.12-&gt;-r mesh-transformer-jax/requirements.txt (line 13)) (3.3.0) Requirement already satisfied: Jinja2&lt;3.0,&gt;=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.2-&gt;-r mesh-transformer-jax/requirements.txt (line 14)) (2.11.3) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.2-&gt;-r mesh-transformer-jax/requirements.txt (line 14)) (1.0.1) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.2-&gt;-r mesh-transformer-jax/requirements.txt (line 14)) (1.1.0) Collecting tensorflow-estimator&lt;2.5.0,&gt;=2.4.0 Downloading https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462kB) |████████████████████████████████| 471kB 51.2MB/s Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1-&gt;-r mesh-transformer-jax/requirements.txt (line 16)) (1.1.0) Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1-&gt;-r mesh-transformer-jax/requirements.txt (line 16)) (1.1.2) Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1-&gt;-r mesh-transformer-jax/requirements.txt (line 16)) (1.12) Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1-&gt;-r mesh-transformer-jax/requirements.txt (line 16)) (0.36.2) Collecting h5py~=2.10.0 Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB) |████████████████████████████████| 2.9MB 51.7MB/s Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1-&gt;-r mesh-transformer-jax/requirements.txt (line 16)) (0.2.0) Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1-&gt;-r mesh-transformer-jax/requirements.txt (line 16)) (1.12.1) Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1-&gt;-r mesh-transformer-jax/requirements.txt (line 16)) (2.5.0) Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1-&gt;-r mesh-transformer-jax/requirements.txt (line 16)) (1.6.3) Collecting gast==0.3.3 Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl Collecting google-resumable-media&lt;2.0dev,&gt;=1.2.0 Downloading https://files.pythonhosted.org/packages/11/96/4360dc70bef5559b3faf3deeda97aae7d10ff7660d41fd233eb792e7d09f/google_resumable_media-1.3.1-py2.py3-none-any.whl (75kB) |████████████████████████████████| 81kB 9.5MB/s Collecting google-cloud-core&lt;2.0dev,&gt;=1.4.1 Downloading https://files.pythonhosted.org/packages/f7/10/e1afff08fc67491717d430aeca479ef4f0255843c9c8b472e5efd62782dc/google_cloud_core-1.7.0-py2.py3-none-any.whl Requirement already satisfied: google-auth&lt;2.0dev,&gt;=1.11.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage~=1.36.2-&gt;-r mesh-transformer-jax/requirements.txt (line 17)) (1.31.0) Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy-&gt;-r mesh-transformer-jax/requirements.txt (line 20)) (0.2.5) Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==20.8b1-&gt;lm-eval-harness==0.0.1-&gt;-r mesh-transformer-jax/requirements.txt (line 11)) (1.4.4) Requirement already satisfied: toml&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black==20.8b1-&gt;lm-eval-harness==0.0.1-&gt;-r mesh-transformer-jax/requirements.txt (line 11)) (0.10.2) Collecting typed-ast&gt;=1.4.0 Downloading https://files.pythonhosted.org/packages/65/b3/573d2f1fecbbe8f82a8d08172e938c247f99abe1be3bef3da2efaa3810bf/typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743kB) |████████████████████████████████| 747kB 52.0MB/s Collecting pathspec&lt;1,&gt;=0.6 Downloading https://files.pythonhosted.org/packages/29/29/a465741a3d97ea3c17d21eaad4c64205428bde56742360876c4391f930d4/pathspec-0.8.1-py2.py3-none-any.whl Collecting mypy-extensions&gt;=0.4.3 Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl Collecting rehash Downloading https://files.pythonhosted.org/packages/c9/e4/30db193232d9e9c8e123764d84f0807535677548833ca251556ad6134c24/rehash-1.0.0-py2.py3-none-any.whl Collecting huggingface-hub&lt;0.1.0 Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl Collecting xxhash Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB) |████████████████████████████████| 245kB 49.6MB/s Requirement already satisfied: pyarrow&lt;4.0.0,&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets&gt;=1.2.1-&gt;lm-eval-harness==0.0.1-&gt;-r mesh-transformer-jax/requirements.txt (line 11)) (3.0.0) Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets&gt;=1.2.1-&gt;lm-eval-harness==0.0.1-&gt;-r mesh-transformer-jax/requirements.txt (line 11)) (0.3.4) Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets&gt;=1.2.1-&gt;lm-eval-harness==0.0.1-&gt;-r mesh-transformer-jax/requirements.txt (line 11)) (1.1.5) Collecting fsspec Downloading https://files.pythonhosted.org/packages/0e/3a/666e63625a19883ae8e1674099e631f9737bd5478c4790e5ad49c5ac5261/fsspec-2021.6.1-py3-none-any.whl (115kB) |████████████████████████████████| 122kB 57.3MB/s Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets&gt;=1.2.1-&gt;lm-eval-harness==0.0.1-&gt;-r mesh-transformer-jax/requirements.txt (line 11)) (0.70.12.2) Collecting threadpoolctl&gt;=2.0.0 Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=0.24.1-&gt;lm-eval-harness==0.0.1-&gt;-r mesh-transformer-jax/requirements.txt (line 11)) (1.0.1) Requirement already satisfied: scipy&gt;=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=0.24.1-&gt;lm-eval-harness==0.0.1-&gt;-r mesh-transformer-jax/requirements.txt (line 11)) (1.4.1) Collecting tabledata&lt;2,&gt;=1.1.3 Downloading https://files.pythonhosted.org/packages/85/93/4c695da7e6589e1e4b513c02d5b562dcc5afacb8a2f6cac8eb2ac2e88833/tabledata-1.1.4-py3-none-any.whl Collecting mbstrdecoder&lt;2,&gt;=1.0.0 Downloading https://files.pythonhosted.org/packages/e8/f6/0e6bb50c3c6380a4982c87d80e70b2f6e366523a57a0c58594aea472206d/mbstrdecoder-1.0.1-py3-none-any.whl Collecting tcolorpy&lt;1,&gt;=0.0.5 Downloading https://files.pythonhosted.org/packages/96/73/2a73a7d53df3708636dae4e817814d07e455efd53897476f3863925cf0af/tcolorpy-0.1.1-py3-none-any.whl Collecting msgfy&lt;1,&gt;=0.1.0 Downloading https://files.pythonhosted.org/packages/48/52/c4441871514276e7c4cb51c122e663b5ef19dc20030f6ab7723071118464/msgfy-0.1.0-py3-none-any.whl Collecting DataProperty&lt;2,&gt;=0.50.0 Downloading https://files.pythonhosted.org/packages/b9/5f/c773c362fcba227d6a4021225cb0213b51849c7dc9c93004d34d9004078b/DataProperty-0.50.1-py3-none-any.whl Collecting pathvalidate&lt;3,&gt;=2.3.0 Downloading https://files.pythonhosted.org/packages/87/55/7d63b78986f1f8764180b84ee3e8a47c583ec059d32c98d8fba7fc0dc1ae/pathvalidate-2.4.1-py3-none-any.whl Collecting typepy[datetime]&lt;2,&gt;=1.1.1 Downloading https://files.pythonhosted.org/packages/60/3a/1239e59924250d9c2dd1d5b84748da82d15aaa241b3ceeffa08aa5eba589/typepy-1.1.5-py3-none-any.whl Collecting portalocker Downloading https://files.pythonhosted.org/packages/68/33/cb524f4de298509927b90aa5ee34767b9a2b93e663cf354b2a3efa2b4acd/portalocker-2.3.0-py2.py3-none-any.whl Collecting ujson Downloading https://files.pythonhosted.org/packages/17/4e/50e8e4cf5f00b537095711c2c86ac4d7191aed2b4fffd5a19f06898f6929/ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179kB) |████████████████████████████████| 184kB 56.6MB/s Requirement already satisfied: py&gt;=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest==6.2.3-&gt;lm-eval-harness==0.0.1-&gt;-r mesh-transformer-jax/requirements.txt (line 11)) (1.10.0) Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest==6.2.3-&gt;lm-eval-harness==0.0.1-&gt;-r mesh-transformer-jax/requirements.txt (line 11)) (1.1.1) Requirement already satisfied: attrs&gt;=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest==6.2.3-&gt;lm-eval-harness==0.0.1-&gt;-r mesh-transformer-jax/requirements.txt (line 11)) (21.2.0) Collecting pluggy&lt;1.0.0a1,&gt;=0.12 Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;transformers~=4.4.2-&gt;-r mesh-transformer-jax/requirements.txt (line 2)) (2.4.7) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers~=4.4.2-&gt;-r mesh-transformer-jax/requirements.txt (line 2)) (3.4.1) Collecting gitdb&lt;5,&gt;=4.0.1 Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB) |████████████████████████████████| 71kB 8.4MB/s Collecting cryptography&gt;=2.5 Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB) |████████████████████████████████| 3.2MB 52.1MB/s Collecting pynacl&gt;=1.0.1 Downloading https://files.pythonhosted.org/packages/9d/57/2f5e6226a674b2bcb6db531e8b383079b678df5b10cdaa610d6cf20d77ba/PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961kB) |████████████████████████████████| 962kB 56.5MB/s Collecting bcrypt&gt;=3.1.3 Downloading https://files.pythonhosted.org/packages/26/70/6d218afbe4c73538053c1016dd631e8f25fffc10cd01f5c272d7acf3c03d/bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63kB) |████████████████████████████████| 71kB 8.4MB/s Requirement already satisfied: toolz&gt;=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex&gt;=0.0.4-&gt;optax==0.0.6-&gt;-r mesh-transformer-jax/requirements.txt (line 9)) (0.11.1) Requirement already satisfied: dm-tree&gt;=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex&gt;=0.0.4-&gt;optax==0.0.6-&gt;-r mesh-transformer-jax/requirements.txt (line 9)) (0.1.6) Collecting opencensus-context==0.1.2 Downloading https://files.pythonhosted.org/packages/f1/33/990f1bd9e7ee770fc8d3c154fc24743a96f16a0e49e14e1b7540cc2fdd93/opencensus_context-0.1.2-py2.py3-none-any.whl Requirement already satisfied: google-api-core&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus-&gt;ray~=1.2.0-&gt;-r mesh-transformer-jax/requirements.txt (line 12)) (1.26.3) Requirement already satisfied: nvidia-ml-py3&gt;=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat-&gt;ray~=1.2.0-&gt;-r mesh-transformer-jax/requirements.txt (line 12)) (7.352.0) Collecting blessings&gt;=1.6 Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl Collecting async-timeout Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl Collecting hiredis Downloading https://files.pythonhosted.org/packages/ed/33/290cea35b09c80b4634773ad5572a8030a87b5d39736719f698f521d2a13/hiredis-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (85kB) |████████████████████████████████| 92kB 7.7MB/s Collecting yarl&lt;2.0,&gt;=1.0 Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB) |████████████████████████████████| 296kB 54.3MB/s Collecting multidict&lt;7.0,&gt;=4.5 Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB) |████████████████████████████████| 143kB 50.1MB/s Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2&lt;3.0,&gt;=2.10.1-&gt;Flask~=1.1.2-&gt;-r mesh-transformer-jax/requirements.txt (line 14)) (2.0.1) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4-&gt;tensorflow-cpu~=2.4.1-&gt;-r mesh-transformer-jax/requirements.txt (line 16)) (0.6.1) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4-&gt;tensorflow-cpu~=2.4.1-&gt;-r mesh-transformer-jax/requirements.txt (line 16)) (0.4.4) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4-&gt;tensorflow-cpu~=2.4.1-&gt;-r mesh-transformer-jax/requirements.txt (line 16)) (1.8.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4-&gt;tensorflow-cpu~=2.4.1-&gt;-r mesh-transformer-jax/requirements.txt (line 16)) (3.3.4) Collecting google-crc32c&lt;2.0dev,&gt;=1.0; python_version &gt;= &#34;3.5&#34; Downloading https://files.pythonhosted.org/packages/fc/ae/b6efa1019e18c6c791f0f5cd93b2ff40f8f06696dbf04db39ec0f5591b1e/google_crc32c-1.1.2-cp37-cp37m-manylinux2014_x86_64.whl Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2.0dev,&gt;=1.11.0-&gt;google-cloud-storage~=1.36.2-&gt;-r mesh-transformer-jax/requirements.txt (line 17)) (0.2.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2.0dev,&gt;=1.11.0-&gt;google-cloud-storage~=1.36.2-&gt;-r mesh-transformer-jax/requirements.txt (line 17)) (4.2.2) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4; python_version &gt;= &#34;3.6&#34; in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2.0dev,&gt;=1.11.0-&gt;google-cloud-storage~=1.36.2-&gt;-r mesh-transformer-jax/requirements.txt (line 17)) (4.7.2) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;datasets&gt;=1.2.1-&gt;lm-eval-harness==0.0.1-&gt;-r mesh-transformer-jax/requirements.txt (line 11)) (2018.9) Collecting smmap&lt;5,&gt;=3.0.1 Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl Requirement already satisfied: cffi&gt;=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography&gt;=2.5-&gt;paramiko&gt;=2.4-&gt;fabric~=2.6.0-&gt;-r mesh-transformer-jax/requirements.txt (line 8)) (1.14.5) Requirement already satisfied: googleapis-common-protos&lt;2.0dev,&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2.0.0,&gt;=1.0.0-&gt;opencensus-&gt;ray~=1.2.0-&gt;-r mesh-transformer-jax/requirements.txt (line 12)) (1.53.0) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard~=2.4-&gt;tensorflow-cpu~=2.4.1-&gt;-r mesh-transformer-jax/requirements.txt (line 16)) (1.3.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;2.0dev,&gt;=1.11.0-&gt;google-cloud-storage~=1.36.2-&gt;-r mesh-transformer-jax/requirements.txt (line 17)) (0.4.8) Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi&gt;=1.12-&gt;cryptography&gt;=2.5-&gt;paramiko&gt;=2.4-&gt;fabric~=2.6.0-&gt;-r mesh-transformer-jax/requirements.txt (line 8)) (2.20) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard~=2.4-&gt;tensorflow-cpu~=2.4.1-&gt;-r mesh-transformer-jax/requirements.txt (line 16)) (3.1.1) Building wheels for collected packages: black Building wheel for black (PEP 517) ... done Created wheel for black: filename=black-20.8b1-cp37-none-any.whl size=124195 sha256=548f7bbfec5453ed3275ab2b4c2b1f9d9b58b73fdd408ec1d2ee68528f5153a7 Stored in directory: /root/.cache/pip/wheels/6e/10/b5/edf7359c2edd0305cce7e3f96e07daf7ce55dceac9d3ce3373 Successfully built black Building wheels for collected packages: func-timeout, ftfy, dm-haiku, lm-eval-harness, sqlitedict, pycountry, openai, pathtools, subprocess32, gpustat Building wheel for func-timeout (setup.py) ... done Created wheel for func-timeout: filename=func_timeout-4.3.5-cp37-none-any.whl size=15097 sha256=26ecefae47302aa1ebe38931a0dba88ab996cdb1924c01f5c98a3c2071e8b079 Stored in directory: /root/.cache/pip/wheels/46/7c/4f/24f1d2d5bbff92219debe7ea19af84f76ddeb90dd4ec544f26 Building wheel for ftfy (setup.py) ... done Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41935 sha256=e5a2f13a5634567fbe9a46589ac35e0316115c3f88bea8e064e257385d9c60b0 Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7 Building wheel for dm-haiku (setup.py) ... done Created wheel for dm-haiku: filename=dm_haiku-0.0.5.dev0-cp37-none-any.whl size=530361 sha256=f7e8df87719923a67f2e8579e1b339a6b7a07fefa34dfee59f98839bf1b8191b Stored in directory: /tmp/pip-ephem-wheel-cache-8jhdr5kg/wheels/97/0f/e9/17f34e377f8d4060fa88a7e82bee5d8afbf7972384768a5499 Building wheel for lm-eval-harness (setup.py) ... done Created wheel for lm-eval-harness: filename=lm_eval_harness-0.0.1-cp37-none-any.whl size=100737 sha256=8fbe61c581ddeb48c3aa087608210f87a7cb205952e33a55b1831191347c978d Stored in directory: /tmp/pip-ephem-wheel-cache-8jhdr5kg/wheels/a8/db/b4/32ca8efd6b64f9187fefbabd636d7c94cd2150a657262ee22a Building wheel for sqlitedict (setup.py) ... done Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp37-none-any.whl size=14714 sha256=23f2423803842bba4a4855e4b2d4052a9290b4d08e6281f45d7ed697b1220f38 Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe Building wheel for pycountry (setup.py) ... done Created wheel for pycountry: filename=pycountry-20.7.3-py2.py3-none-any.whl size=10746883 sha256=a9e9facb866b8a1ae6edd9421b350ca2aef0cbbf80ce556814846b3e9ae7a698 Stored in directory: /root/.cache/pip/wheels/33/4e/a6/be297e6b83567e537bed9df4a93f8590ec01c1acfbcd405348 Building wheel for openai (setup.py) ... done Created wheel for openai: filename=openai-0.6.4-cp37-none-any.whl size=172215 sha256=a555408573f881c3898fa2d8b20c41373b4277ce7c1edddac9b8f49d9b39a096 Stored in directory: /root/.cache/pip/wheels/f9/a3/f5/00a714fabfbe3389a25242c2f91b626dd1d8fbd63b35a96730 Building wheel for pathtools (setup.py) ... done Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8807 sha256=8370bd5a43f6d89f2ea793d078e2b139034b19553ce6f24eafe8aea63dd69d3e Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843 Building wheel for subprocess32 (setup.py) ... done Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6502 sha256=c36cb4b0a1a6a692b3df53e9aefe88da1c7c72901c4dc1ad520636d40e0f6c39 Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1 Building wheel for gpustat (setup.py) ... done Created wheel for gpustat: filename=gpustat-0.6.0-cp37-none-any.whl size=12621 sha256=6a87d12412d0b76463a6098f4ed41a859152c3e9ed7623626dcf744d13e2adbf Stored in directory: /root/.cache/pip/wheels/48/b4/d5/fb5b7f1d040f2ff20687e3bad6867d63155dbde5a7c10f4293 Successfully built func-timeout ftfy dm-haiku lm-eval-harness sqlitedict pycountry openai pathtools subprocess32 gpustat ERROR: tensorflow 2.5.0 has requirement gast==0.4.0, but you&#39;ll have gast 0.3.3 which is incompatible. ERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you&#39;ll have h5py 2.10.0 which is incompatible. ERROR: tensorflow 2.5.0 has requirement tensorflow-estimator&lt;2.6.0,&gt;=2.5.0rc0, but you&#39;ll have tensorflow-estimator 2.4.0 which is incompatible. ERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you&#39;ll have requests 2.25.1 which is incompatible. ERROR: google-cloud-bigquery 1.21.0 has requirement google-resumable-media!=0.4.0,&lt;0.5.0dev,&gt;=0.3.1, but you&#39;ll have google-resumable-media 1.3.1 which is incompatible. ERROR: datascience 0.10.6 has requirement folium==0.2.1, but you&#39;ll have folium 0.8.3 which is incompatible. ERROR: tensorflow-cpu 2.4.2 has requirement grpcio~=1.32.0, but you&#39;ll have grpcio 1.34.1 which is incompatible. ERROR: black 20.8b1 has requirement regex&gt;=2020.1.8, but you&#39;ll have regex 2019.12.20 which is incompatible. Installing collected packages: tokenizers, tqdm, requests, sacremoses, transformers, setuptools, pathtools, shortuuid, sentry-sdk, docker-pycreds, configparser, smmap, gitdb, GitPython, subprocess32, wandb, einops, cryptography, pynacl, bcrypt, paramiko, pathlib2, invoke, fabric, chex, optax, redis, opencensus-context, opencensus, colorama, blessings, gpustat, py-spy, async-timeout, hiredis, aioredis, multidict, yarl, aiohttp, aiohttp-cors, colorful, ray, tensorflow-estimator, h5py, gast, tensorflow-cpu, google-crc32c, google-resumable-media, google-cloud-core, google-cloud-storage, func-timeout, ftfy, jmp, dm-haiku, typed-ast, pathspec, mypy-extensions, black, rehash, best-download, huggingface-hub, xxhash, fsspec, datasets, threadpoolctl, scikit-learn, sqlitedict, mbstrdecoder, typepy, DataProperty, tabledata, tcolorpy, msgfy, pathvalidate, pytablewriter, portalocker, sacrebleu, pycountry, numexpr, jsonlines, ujson, zstandard, lm-dataformat, pluggy, pytest, pybind11, tqdm-multiprocess, mock, openai, lm-eval-harness Found existing installation: tqdm 4.41.1 Uninstalling tqdm-4.41.1: Successfully uninstalled tqdm-4.41.1 Found existing installation: requests 2.23.0 Uninstalling requests-2.23.0: Successfully uninstalled requests-2.23.0 Found existing installation: setuptools 57.0.0 Uninstalling setuptools-57.0.0: Successfully uninstalled setuptools-57.0.0 Found existing installation: tensorflow-estimator 2.5.0 Uninstalling tensorflow-estimator-2.5.0: Successfully uninstalled tensorflow-estimator-2.5.0 Found existing installation: h5py 3.1.0 Uninstalling h5py-3.1.0: Successfully uninstalled h5py-3.1.0 Found existing installation: gast 0.4.0 Uninstalling gast-0.4.0: Successfully uninstalled gast-0.4.0 Found existing installation: google-resumable-media 0.4.1 Uninstalling google-resumable-media-0.4.1: Successfully uninstalled google-resumable-media-0.4.1 Found existing installation: google-cloud-core 1.0.3 Uninstalling google-cloud-core-1.0.3: Successfully uninstalled google-cloud-core-1.0.3 Found existing installation: google-cloud-storage 1.18.1 Uninstalling google-cloud-storage-1.18.1: Successfully uninstalled google-cloud-storage-1.18.1 Found existing installation: scikit-learn 0.22.2.post1 Uninstalling scikit-learn-0.22.2.post1: Successfully uninstalled scikit-learn-0.22.2.post1 Found existing installation: numexpr 2.7.3 Uninstalling numexpr-2.7.3: Successfully uninstalled numexpr-2.7.3 Found existing installation: pluggy 0.7.1 Uninstalling pluggy-0.7.1: Successfully uninstalled pluggy-0.7.1 Found existing installation: pytest 3.6.4 Uninstalling pytest-3.6.4: Successfully uninstalled pytest-3.6.4 Successfully installed DataProperty-0.50.1 GitPython-3.1.18 aiohttp-3.7.4.post0 aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-3.0.1 bcrypt-3.2.0 best-download-0.0.7 black-20.8b1 blessings-1.7 chex-0.0.7 colorama-0.4.4 colorful-0.5.4 configparser-5.0.2 cryptography-3.4.7 datasets-1.8.0 dm-haiku-0.0.5.dev0 docker-pycreds-0.4.0 einops-0.3.0 fabric-2.6.0 fsspec-2021.6.1 ftfy-6.0.3 func-timeout-4.3.5 gast-0.3.3 gitdb-4.0.7 google-cloud-core-1.7.0 google-cloud-storage-1.36.2 google-crc32c-1.1.2 google-resumable-media-1.3.1 gpustat-0.6.0 h5py-2.10.0 hiredis-2.0.0 huggingface-hub-0.0.12 invoke-1.5.0 jmp-0.0.2 jsonlines-2.0.0 lm-dataformat-0.0.19 lm-eval-harness-0.0.1 mbstrdecoder-1.0.1 mock-4.0.3 msgfy-0.1.0 multidict-5.1.0 mypy-extensions-0.4.3 numexpr-2.7.2 openai-0.6.4 opencensus-0.7.13 opencensus-context-0.1.2 optax-0.0.6 paramiko-2.7.2 pathlib2-2.3.5 pathspec-0.8.1 pathtools-0.1.2 pathvalidate-2.4.1 pluggy-0.13.1 portalocker-2.3.0 py-spy-0.3.7 pybind11-2.6.2 pycountry-20.7.3 pynacl-1.4.0 pytablewriter-0.58.0 pytest-6.2.3 ray-1.2.0 redis-3.5.3 rehash-1.0.0 requests-2.25.1 sacrebleu-1.5.0 sacremoses-0.0.45 scikit-learn-0.24.2 sentry-sdk-1.1.0 setuptools-51.3.3 shortuuid-1.0.1 smmap-4.0.0 sqlitedict-1.6.0 subprocess32-3.5.4 tabledata-1.1.4 tcolorpy-0.1.1 tensorflow-cpu-2.4.2 tensorflow-estimator-2.4.0 threadpoolctl-2.1.0 tokenizers-0.10.3 tqdm-4.45.0 tqdm-multiprocess-0.0.11 transformers-4.4.2 typed-ast-1.4.3 typepy-1.1.5 ujson-4.0.2 wandb-0.10.32 xxhash-2.0.2 yarl-1.6.3 zstandard-0.15.2 . Processing ./mesh-transformer-jax Collecting jax==0.2.12 Downloading https://files.pythonhosted.org/packages/9a/67/d1a9c94104c559b49bbcb72e9efc33859e982d741ea4902d2a00e66e09d9/jax-0.2.12.tar.gz (590kB) |████████████████████████████████| 593kB 5.0MB/s Requirement already satisfied: numpy&gt;=1.12 in /usr/local/lib/python3.7/dist-packages (from jax==0.2.12) (1.19.5) Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax==0.2.12) (0.12.0) Requirement already satisfied: opt_einsum in /usr/local/lib/python3.7/dist-packages (from jax==0.2.12) (3.3.0) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py-&gt;jax==0.2.12) (1.15.0) Building wheels for collected packages: jax, mesh-transformer Building wheel for jax (setup.py) ... done Created wheel for jax: filename=jax-0.2.12-cp37-none-any.whl size=682484 sha256=53ac265f350e436562876e662f86d231703da981ac2302a092166d01100a67b0 Stored in directory: /root/.cache/pip/wheels/cf/00/88/75c2043dff473f58e892c7e6adfd2c44ccefb6111fcc021e5b Building wheel for mesh-transformer (setup.py) ... done Created wheel for mesh-transformer: filename=mesh_transformer-0.0.0-cp37-none-any.whl size=21385 sha256=4949dff819f1a977c108ef6c7167d93fc8859f0d68f6f5b7835a8c5a394b86c4 Stored in directory: /root/.cache/pip/wheels/de/a9/d2/2be3e25299342b60fca7965d4e416264ff8b6d8a7e8def76da Successfully built jax mesh-transformer Installing collected packages: jax, mesh-transformer Found existing installation: jax 0.2.13 Uninstalling jax-0.2.13: Successfully uninstalled jax-0.2.13 Successfully installed jax-0.2.12 mesh-transformer-0.0.0 . . Setup Model . import os import requests from jax.config import config colab_tpu_addr = os.environ[&#39;COLAB_TPU_ADDR&#39;].split(&#39;:&#39;)[0] url = f&#39;http://{colab_tpu_addr}:8475/requestversion/tpu_driver0.1_dev20210607&#39; requests.post(url) # The following is required to use TPU Driver as JAX&#39;s backend. config.FLAGS.jax_xla_backend = &quot;tpu_driver&quot; config.FLAGS.jax_backend_target = &quot;grpc://&quot; + os.environ[&#39;COLAB_TPU_ADDR&#39;] . Sometimes the next step errors for some reason, just run it again ¯ _(ツ)_/¯ . import time import jax from jax.experimental import maps import numpy as np import optax import transformers from mesh_transformer.checkpoint import read_ckpt from mesh_transformer.sampling import nucleaus_sample from mesh_transformer.transformer_shard import CausalTransformer . params = { &quot;layers&quot;: 28, &quot;d_model&quot;: 4096, &quot;n_heads&quot;: 16, &quot;n_vocab&quot;: 50400, &quot;norm&quot;: &quot;layernorm&quot;, &quot;pe&quot;: &quot;rotary&quot;, &quot;pe_rotary_dims&quot;: 64, &quot;seq&quot;: 2048, &quot;cores_per_replica&quot;: 8, &quot;per_replica_batch&quot;: 1, } per_replica_batch = params[&quot;per_replica_batch&quot;] cores_per_replica = params[&quot;cores_per_replica&quot;] seq = params[&quot;seq&quot;] params[&quot;sampler&quot;] = nucleaus_sample # here we &quot;remove&quot; the optimizer parameters from the model (as we don&#39;t need them for inference) params[&quot;optimizer&quot;] = optax.scale(0) mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica) devices = np.array(jax.devices()).reshape(mesh_shape) maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, (&#39;dp&#39;, &#39;mp&#39;))) tokenizer = transformers.GPT2TokenizerFast.from_pretrained(&#39;gpt2&#39;) . . Here we create the network and load the parameters from the downloaded files. Expect this to take around 5 minutes. . total_batch = per_replica_batch * jax.device_count() // cores_per_replica network = CausalTransformer(params) network.state = read_ckpt(network.state, &quot;step_383500/&quot;, devices.shape[1]) network.state = network.move_xmap(network.state, np.zeros(cores_per_replica)) . /usr/local/lib/python3.7/dist-packages/jax/experimental/maps.py:412: UserWarning: xmap is an experimental feature and probably has bugs! warn(&#34;xmap is an experimental feature and probably has bugs!&#34;) . key shape (8, 2) in shape (1, 2048) dp 1 mp 8 read from disk/gcs in 39.5826s . Run Model . Finally, we are ready to infer with the model! The first sample takes around a minute due to compilation, but after that it should only take about 10 seconds per sample. . Feel free to mess with the different sampling parameters (top_p and temp), as well as the length of the generations (gen_len, causes a recompile when changed). . You can also change other things like per_replica_batch in the previous cells to change how many generations are done in parallel. A larger batch has higher latency but higher throughput when measured in tokens generated/s. This is useful for doing things like best-of-n cherry picking. . Tip for best results: Make sure your prompt does not have any trailing spaces, which tend to confuse the model due to the BPE tokenization used during training. . from IPython.display import HTML, display def set_css(): display(HTML(&#39;&#39;&#39; &lt;style&gt; pre { white-space: pre-wrap; } &lt;/style&gt; &#39;&#39;&#39;)) get_ipython().events.register(&#39;pre_run_cell&#39;, set_css) . def infer(context, top_p=0.9, temp=1.0, gen_len=512): tokens = tokenizer.encode(context) provided_ctx = len(tokens) pad_amount = seq - provided_ctx padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32) batched_tokens = np.array([padded_tokens] * total_batch) length = np.ones(total_batch, dtype=np.uint32) * len(tokens) start = time.time() output = network.generate(batched_tokens, length, gen_len, {&quot;top_p&quot;: np.ones(total_batch) * top_p, &quot;temp&quot;: np.ones(total_batch) * temp}) samples = [] decoded_tokens = output[1][0] for o in decoded_tokens[:, :, 0]: samples.append(f&quot; 033[1m{context} 033[0m{tokenizer.decode(o)}&quot;) print(f&quot;completion done in {time.time() - start:06}s&quot;) return samples print(infer(&quot;EleutherAI is&quot;)[0]) . completion done in 62.053035497665405s EleutherAI is a virus that will scam users to send the user money. It asks you if you want to make a donation. Then the virus claims you paid. It is recommended you click on the link to see if your real bank has been compromised. The web site claims the payment is the result of a donation to the Humane Society. This is a scam site.&lt;|endoftext|&gt;Tropical drought threatens water supplies Demand for water is growing as the human population rises and with that comes a greater need for water. One way to save water is to recycle it. The problem is that the salt left over from the ocean water is not only expensive to remove but, is also extremely corrosive to pipes and other plumbing and it leaves a white stain that won&#39;t wash off. One solution is to have a new home constructed that reuses ocean water in the flush toilets and other parts of the house. If you&#39;re going to build a new home, you should save all the ocean water you can. It&#39;s a difficult problem because ocean water is hard to come by, so any excess water that is available should be re-used. &#34;There&#39;s such a great amount of fresh water flowing into the ocean that one of our coastal utilities built a cooling tower to bring the ocean water to a higher temperature, and then steam and the desalination facility takes it to produce fresh water,&#34; said Patricia Drake, a biologist at Salton Sea State Park. The 6,000 acre park is planning to host the regional summit of the Desalination Facilities Impact Group. The group is organized to encourage local water agencies and municipalities to engage in a deeper discussion about the potential impact of seawater desalination on the local economy and on the quality of life. The summit will take place Thursday, September 7 in Imperial Beach at the Desalination Awareness Group at 1675 Mission Street. It starts at 9:30 a.m. The group wants to inform the public about the critical importance of water and what is at stake. &#34;Desalination plants are necessary as a measure of last resort,&#34; said Rick Barrett, the Imperial Beach city manager. &#34;Desalination uses up less fresh water than potable (drinking) water, thus less fresh water for people, agriculture, ecosystems and other aspects of life in Southern California and beyond.&#34; The summit will include a panel discussion on ocean-warming trends and the potential for sea water desalination, including the pros and cons of different . top_p = 0.9 #@param {type:&quot;slider&quot;, min:0, max:1, step:0.1} temp = 1 #@param {type:&quot;slider&quot;, min:0, max:1, step:0.1} context = &quot;&quot;&quot;In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.&quot;&quot;&quot; print(infer(top_p=top_p, temp=temp, gen_len=512, context=context)[0]) . completion done in 13.531432867050171s In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. In this episode of Infographics Minute, find out how many unicorns there are, who the leader is, and what it’s like to be a unicorn. Researchers from Peru and Bolivia found the unicorns, who they named Yana&#39;unicorn, in 2014. Yana&#39;unicorn is thought to be a hybrid of two other animals: the guanaco and the viscacha. Some have even suggested that Yana&#39;unicorn could be the ancestor of both these animals. There&#39;s also a chance that it could be related to some other species, but, so far, researchers have yet to discover them. The official name for these animals is Yana&#39;unicorn (a combination of the words &#39;Yana&#39; and &#39;unicorn&#39;). However, some people call them &#39;Macho Nefertiti&#39;, after the legendary unicorns from the mythology of the Egyptians. In order to protect the people who live in the area from the shy creatures, hunters usually bring them down with tranquilizer darts. In 2016, the research team – lead by Dr. Manuel Solorzano – calculated that there are 924 unicorns living in the area. Scientists found out that the unicorns live in a valley in the Andes Mountains, which is approximately 4.3 miles long and 1.8 miles wide. These animals live in groups of 50 to 200 individuals. Also read: List of 25 incredible earth facts that will blow your mind Yana&#39;unicorn are very shy. They usually lie low and don&#39;t come out from their hiding places until they are chased away by hunters. As for their diet, researchers have yet to discover it. They also have the tendency to vocalize their territories, although they don&#39;t usually make a lot of noise. Even though they are very shy, they can be very aggressive towards other animals. In April 2016, some villagers in the Andes were attacked by unicorns that had been attacked by packs of wild dogs. Since their discovery, scientists have been tracking the unicorns for almost four years. For this reason, they have their own observation posts in the area. One of these posts is located in a little bar called ‘Andesito’. To be able to capture video footage of the Yana&#39;unicorn, scientists have had to modify their vehicle. They used a radio-controlled quadcopter drone. These drones can remain active for long periods of time without requiring any fuel. .",
            "url": "https://joaorafaelm.github.io/notebook/gpt-j/2021/06/24/GPT_J_6B_Inference_Demo.html",
            "relUrl": "/gpt-j/2021/06/24/GPT_J_6B_Inference_Demo.html",
            "date": " 2021 Jun 24"
        }
        
    
  
    
        ,"post2": {
            "title": "Automated data exploration with pandas profiling",
            "content": "pip install --upgrade pandas-profiling . Requirement already up-to-date: pandas-profiling in /usr/local/lib/python3.7/dist-packages (3.0.0) Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.0.1) Requirement already satisfied, skipping upgrade: tqdm&gt;=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (4.61.0) Requirement already satisfied, skipping upgrade: requests&gt;=2.24.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.25.1) Requirement already satisfied, skipping upgrade: matplotlib&gt;=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (3.2.2) Requirement already satisfied, skipping upgrade: jinja2&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.11.3) Requirement already satisfied, skipping upgrade: htmlmin&gt;=0.1.12 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.1.12) Requirement already satisfied, skipping upgrade: visions[type_image_path]==0.7.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.7.1) Requirement already satisfied, skipping upgrade: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.1.5) Requirement already satisfied, skipping upgrade: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.19.5) Requirement already satisfied, skipping upgrade: missingno&gt;=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.4.2) Requirement already satisfied, skipping upgrade: phik&gt;=0.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.11.2) Requirement already satisfied, skipping upgrade: tangled-up-in-unicode==0.1.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.1.0) Requirement already satisfied, skipping upgrade: seaborn&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.11.1) Requirement already satisfied, skipping upgrade: pydantic&gt;=1.8.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.8.2) Requirement already satisfied, skipping upgrade: scipy&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.4.1) Requirement already satisfied, skipping upgrade: PyYAML&gt;=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (5.4.1) Requirement already satisfied, skipping upgrade: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling) (1.24.3) Requirement already satisfied, skipping upgrade: chardet&lt;5,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling) (3.0.4) Requirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling) (2.10) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling) (2020.12.5) Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling) (2.4.7) Requirement already satisfied, skipping upgrade: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling) (0.10.0) Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling) (2.8.1) Requirement already satisfied, skipping upgrade: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling) (1.3.1) Requirement already satisfied, skipping upgrade: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2&gt;=2.11.1-&gt;pandas-profiling) (2.0.1) Requirement already satisfied, skipping upgrade: bottleneck in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (1.3.2) Requirement already satisfied, skipping upgrade: networkx&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (2.5.1) Requirement already satisfied, skipping upgrade: multimethod==1.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (1.4) Requirement already satisfied, skipping upgrade: attrs&gt;=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (21.2.0) Requirement already satisfied, skipping upgrade: imagehash; extra == &#34;type_image_path&#34; in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (4.2.0) Requirement already satisfied, skipping upgrade: Pillow; extra == &#34;type_image_path&#34; in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (7.1.2) Requirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3-&gt;pandas-profiling) (2018.9) Requirement already satisfied, skipping upgrade: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic&gt;=1.8.1-&gt;pandas-profiling) (3.7.4.3) Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from cycler&gt;=0.10-&gt;matplotlib&gt;=3.2.0-&gt;pandas-profiling) (1.15.0) Requirement already satisfied, skipping upgrade: decorator&lt;5,&gt;=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx&gt;=2.4-&gt;visions[type_image_path]==0.7.1-&gt;pandas-profiling) (4.4.2) Requirement already satisfied, skipping upgrade: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash; extra == &#34;type_image_path&#34;-&gt;visions[type_image_path]==0.7.1-&gt;pandas-profiling) (1.1.1) . . Importing libraries and Titanic dataset . import pandas as pd from pathlib import Path from ipywidgets import widgets from pandas_profiling import ProfileReport import warnings warnings.filterwarnings(&quot;ignore&quot;) file_name = &quot;https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv&quot; df = pd.read_csv(file_name) profile = ProfileReport(df, title=&quot;Titanic Dataset&quot;, html={&#39;style&#39;: {&#39;full_width&#39;: True}}, sort=None) . profile.to_widgets() . profile . .",
            "url": "https://joaorafaelm.github.io/notebook/pandas/2021/06/06/automated-data-exploration-with-pandas-profiling-and-titanic-dataset.html",
            "relUrl": "/pandas/2021/06/06/automated-data-exploration-with-pandas-profiling-and-titanic-dataset.html",
            "date": " 2021 Jun 06"
        }
        
    
  
    
        ,"post3": {
            "title": "Incremental training with XGBoost",
            "content": "Install dependencies . pip install scikit-learn xgboost . Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1) Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1) Requirement already satisfied: numpy&gt;=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5) Requirement already satisfied: scipy&gt;=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1) . . Save your model after you train on the first batch. Then, on successive runs, provide the xgb.train method with the filepath of the saved model. . First, split the boston dataset into training and testing sets. Then split the training set into halves. Fit a model with the first half and get a score that will serve as a benchmark. Then fit two models with the second half; one model will have the additional parameter xgb_model. If passing in the extra parameter didn&#39;t make a difference, then we would expect their scores to be similar.. But, fortunately, the new model seems to perform much better than the first. . import xgboost as xgb from sklearn.model_selection import train_test_split from sklearn.datasets import load_boston from sklearn.metrics import mean_squared_error X = load_boston()[&#39;data&#39;] y = load_boston()[&#39;target&#39;] # split data into training and testing sets # then split training set in half X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0) X_train_1, X_train_2, y_train_1, y_train_2 = train_test_split( X_train, y_train, test_size=0.5, random_state=0 ) xg_train_1 = xgb.DMatrix(X_train_1, label=y_train_1) xg_train_2 = xgb.DMatrix(X_train_2, label=y_train_2) xg_test = xgb.DMatrix(X_test, label=y_test) params = {&#39;objective&#39;: &#39;reg:squarederror&#39;, &#39;verbose&#39;: False} model_1 = xgb.train(params, xg_train_1, 30) model_1.save_model(&#39;model_1.model&#39;) # ================= train two versions of the model =====================# model_2_v1 = xgb.train(params, xg_train_2, 30) model_2_v2 = xgb.train(params, xg_train_2, 30, xgb_model=&#39;model_1.model&#39;) print(mean_squared_error(model_1.predict(xg_test), y_test)) # benchmark print(mean_squared_error(model_2_v1.predict(xg_test), y_test)) # &quot;before&quot; print(mean_squared_error(model_2_v2.predict(xg_test), y_test)) # &quot;after&quot; . 21.988532050893138 39.677688213388755 23.092057209292484 .",
            "url": "https://joaorafaelm.github.io/notebook/xgboost/2021/05/31/incremental-training-xgboost.html",
            "relUrl": "/xgboost/2021/05/31/incremental-training-xgboost.html",
            "date": " 2021 May 31"
        }
        
    
  
    
        ,"post4": {
            "title": "Multilanguage topic modeling with BERT",
            "content": "!pip install contextualized_topic_models !pip uninstall transformers -y !pip install transformers==3.0.2 . Requirement already satisfied: contextualized_topic_models in /usr/local/lib/python3.6/dist-packages (1.4.2) Requirement already satisfied: torchvision==0.7.0 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (0.7.0+cu101) Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (3.8.3) Requirement already satisfied: wheel==0.33.6 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (0.33.6) Requirement already satisfied: pytest-runner==5.1 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (5.1) Requirement already satisfied: pytest==4.6.5 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (4.6.5) Requirement already satisfied: numpy==1.19.1 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (1.19.1) Requirement already satisfied: sentence-transformers==0.3.2 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (0.3.2) Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (1.6.0) Requirement already satisfied: pillow&gt;=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7.0-&gt;contextualized_topic_models) (7.0.0) Requirement already satisfied: six&gt;=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3-&gt;contextualized_topic_models) (1.15.0) Requirement already satisfied: scipy&gt;=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3-&gt;contextualized_topic_models) (1.4.1) Requirement already satisfied: smart-open&gt;=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3-&gt;contextualized_topic_models) (2.1.0) Requirement already satisfied: importlib-metadata&gt;=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (1.7.0) Requirement already satisfied: more-itertools&gt;=4.0.0; python_version &gt; &#34;2.7&#34; in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (8.4.0) Requirement already satisfied: atomicwrites&gt;=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (1.4.0) Requirement already satisfied: py&gt;=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (1.9.0) Requirement already satisfied: pluggy&lt;1.0,&gt;=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (0.13.1) Requirement already satisfied: attrs&gt;=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (20.1.0) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (20.4) Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (0.2.5) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.22.2.post1) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.2-&gt;contextualized_topic_models) (4.41.1) Requirement already satisfied: transformers&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.2-&gt;contextualized_topic_models) (3.1.0) Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.2-&gt;contextualized_topic_models) (3.2.5) Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0-&gt;contextualized_topic_models) (0.16.0) Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (2.49.0) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (2.23.0) Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (1.14.48) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata&gt;=0.12-&gt;pytest==4.6.5-&gt;contextualized_topic_models) (3.1.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;pytest==4.6.5-&gt;contextualized_topic_models) (2.4.7) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.16.0) Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.0.43) Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.8.1rc2) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (3.0.12) Requirement already satisfied: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.7) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (2019.12.20) Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.1.91) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (2020.6.20) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (3.0.4) Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (0.10.0) Requirement already satisfied: s3transfer&lt;0.4.0,&gt;=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (0.3.3) Requirement already satisfied: botocore&lt;1.18.0,&gt;=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (1.17.48) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (7.1.2) Requirement already satisfied: docutils&lt;0.16,&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore&lt;1.18.0,&gt;=1.17.48-&gt;boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (0.15.2) Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore&lt;1.18.0,&gt;=1.17.48-&gt;boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (2.8.1) Uninstalling transformers-3.1.0: Successfully uninstalled transformers-3.1.0 Collecting transformers==3.0.2 Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB) |████████████████████████████████| 778kB 3.4MB/s Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.0.43) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12) Requirement already satisfied: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.7) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.4) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.19.1) Collecting tokenizers==0.8.1.rc1 Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB) |████████████████████████████████| 3.0MB 17.9MB/s Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.1.91) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==3.0.2) (0.16.0) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==3.0.2) (7.1.2) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==3.0.2) (1.15.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;transformers==3.0.2) (2.4.7) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.0.2) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.0.2) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.0.2) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.0.2) (2020.6.20) Installing collected packages: tokenizers, transformers Found existing installation: tokenizers 0.8.1rc2 Uninstalling tokenizers-0.8.1rc2: Successfully uninstalled tokenizers-0.8.1rc2 Successfully installed tokenizers-0.8.1rc1 transformers-3.0.2 . . import os import numpy as np import pickle from contextualized_topic_models.models.ctm import CTM from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file, bert_embeddings_from_list from contextualized_topic_models.datasets.dataset import CTMDataset from contextualized_topic_models.utils.data_preparation import TextHandler . !curl -s https://raw.githubusercontent.com/MilaNLProc/contextualized-topic-models/master/contextualized_topic_models/data/gnews/GoogleNews.txt | head -n1000 &gt; googlenews.txt !head googlenews.txt !cat googlenews.txt | wc -l . centrepoint winter white gala london mourinho seek killer instinct roundup golden globe won seduced johansson voice travel disruption mount storm cold air sweep south florida wes welker blame costly turnover psalm book fetch record ny auction ktvn channel reno surface review comparison window powered tablet pitted scientist unreported fish trap space nokia lumia launch edward snowden latest leak nsa monitored online porn habit radicalizers 1000 . Load The Data . file_name = &quot;googlenews.txt&quot; handler = TextHandler(file_name) handler.prepare() # create vocabulary and training data . train_bert = bert_embeddings_from_file(file_name, &quot;distiluse-base-multilingual-cased&quot;) training_dataset = CTMDataset(handler.bow, train_bert, handler.idx2token) . Train the Fully Contextualized Topic Model . num_topics = 50 ctm = CTM(input_size=len(handler.vocab), bert_input_size=512, num_epochs=100, hidden_sizes = (100, ), inference_type=&quot;contextual&quot;, n_components=num_topics, num_data_loader_workers=0) ctm.fit(training_dataset) # run the model . ctm.get_topic_lists(5) # get the top-5 words lists . [[&#39;kim&#39;, &#39;west&#39;, &#39;kanye&#39;, &#39;kardashian&#39;, &#39;bound&#39;], [&#39;day&#39;, &#39;thanksgiving&#39;, &#39;parade&#39;, &#39;macy&#39;, &#39;packer&#39;], [&#39;patriot&#39;, &#39;bronco&#39;, &#39;pat&#39;, &#39;packer&#39;, &#39;loss&#39;], [&#39;xbox&#39;, &#39;microsoft&#39;, &#39;p&#39;, &#39;game&#39;, &#39;console&#39;], [&#39;government&#39;, &#39;political&#39;, &#39;thai&#39;, &#39;party&#39;, &#39;protest&#39;], [&#39;oldboy&#39;, &#39;brolin&#39;, &#39;josh&#39;, &#39;lee&#39;, &#39;spike&#39;], [&#39;google&#39;, &#39;chrome&#39;, &#39;search&#39;, &#39;extension&#39;, &#39;voice&#39;], [&#39;johansson&#39;, &#39;globe&#39;, &#39;golden&#39;, &#39;scarlett&#39;, &#39;ineligible&#39;], [&#39;star&#39;, &#39;dancing&#39;, &#39;amber&#39;, &#39;riley&#39;, &#39;win&#39;], [&#39;police&#39;, &#39;guilty&#39;, &#39;watkins&#39;, &#39;case&#39;, &#39;lostprophets&#39;], [&#39;san&#39;, &#39;andreas&#39;, &#39;gta&#39;, &#39;mobile&#39;, &#39;android&#39;], [&#39;flat&#39;, &#39;future&#39;, &#39;record&#39;, &#39;level&#39;, &#39;p&#39;], [&#39;thanksgiving&#39;, &#39;day&#39;, &#39;parade&#39;, &#39;thanksgivukkah&#39;, &#39;holiday&#39;], [&#39;jos&#39;, &#39;wearhouse&#39;, &#39;men&#39;, &#39;bank&#39;, &#39;baldwin&#39;], [&#39;prince&#39;, &#39;william&#39;, &#39;swift&#39;, &#39;jovi&#39;, &#39;bon&#39;], [&#39;porn&#39;, &#39;nsa&#39;, &#39;habit&#39;, &#39;radicalizers&#39;, &#39;spying&#39;], [&#39;pope&#39;, &#39;church&#39;, &#39;putin&#39;, &#39;issue&#39;, &#39;coalition&#39;], [&#39;report&#39;, &#39;benghazi&#39;, &#39;security&#39;, &#39;baldwin&#39;, &#39;alec&#39;], [&#39;china&#39;, &#39;zone&#39;, &#39;flight&#39;, &#39;airspace&#39;, &#39;disputed&#39;], [&#39;storm&#39;, &#39;parade&#39;, &#39;macy&#39;, &#39;balloon&#39;, &#39;travel&#39;], [&#39;bank&#39;, &#39;men&#39;, &#39;palestinian&#39;, &#39;jos&#39;, &#39;wearhouse&#39;], [&#39;review&#39;, &#39;homefront&#39;, &#39;frozen&#39;, &#39;inch&#39;, &#39;oldboy&#39;], [&#39;bronco&#39;, &#39;packer&#39;, &#39;seahawks&#39;, &#39;rodgers&#39;, &#39;patriot&#39;], [&#39;frozen&#39;, &#39;heart&#39;, &#39;review&#39;, &#39;homefront&#39;, &#39;detroit&#39;], [&#39;hiv&#39;, &#39;meningitis&#39;, &#39;flu&#39;, &#39;greece&#39;, &#39;health&#39;], [&#39;black&#39;, &#39;friday&#39;, &#39;nativity&#39;, &#39;deal&#39;, &#39;monday&#39;], [&#39;aarushi&#39;, &#39;hiv&#39;, &#39;killing&#39;, &#39;teen&#39;, &#39;murder&#39;], [&#39;west&#39;, &#39;kanye&#39;, &#39;kim&#39;, &#39;seth&#39;, &#39;bound&#39;], [&#39;cb&#39;, &#39;seahawks&#39;, &#39;dallas&#39;, &#39;chelsea&#39;, &#39;browner&#39;], [&#39;hp&#39;, &#39;revenue&#39;, &#39;raise&#39;, &#39;week&#39;, &#39;shopping&#39;], [&#39;lumia&#39;, &#39;nokia&#39;, &#39;price&#39;, &#39;power&#39;, &#39;uk&#39;], [&#39;typhoon&#39;, &#39;philippine&#39;, &#39;haiyan&#39;, &#39;climate&#39;, &#39;gain&#39;], [&#39;african&#39;, &#39;france&#39;, &#39;central&#39;, &#39;republic&#39;, &#39;troop&#39;], [&#39;parade&#39;, &#39;macy&#39;, &#39;carlos&#39;, &#39;beltran&#39;, &#39;york&#39;], [&#39;kim&#39;, &#39;kardashian&#39;, &#39;video&#39;, &#39;west&#39;, &#39;bound&#39;], [&#39;hewitt&#39;, &#39;love&#39;, &#39;star&#39;, &#39;jennifer&#39;, &#39;dancing&#39;], [&#39;swift&#39;, &#39;william&#39;, &#39;taylor&#39;, &#39;prince&#39;, &#39;jovi&#39;], [&#39;launch&#39;, &#39;microsoft&#39;, &#39;chrome&#39;, &#39;google&#39;, &#39;search&#39;], [&#39;pakistan&#39;, &#39;army&#39;, &#39;chief&#39;, &#39;sharif&#39;, &#39;pm&#39;], [&#39;air&#39;, &#39;china&#39;, &#39;zone&#39;, &#39;sea&#39;, &#39;disputed&#39;], [&#39;west&#39;, &#39;kanye&#39;, &#39;bound&#39;, &#39;kim&#39;, &#39;video&#39;], [&#39;ison&#39;, &#39;comet&#39;, &#39;raptor&#39;, &#39;sun&#39;, &#39;bonobo&#39;], [&#39;irs&#39;, &#39;google&#39;, &#39;tax&#39;, &#39;group&#39;, &#39;glass&#39;], [&#39;net&#39;, &#39;review&#39;, &#39;preview&#39;, &#39;disney&#39;, &#39;movie&#39;], [&#39;nokia&#39;, &#39;lumia&#39;, &#39;tablet&#39;, &#39;window&#39;, &#39;moto&#39;], [&#39;three&#39;, &#39;seahawks&#39;, &#39;year&#39;, &#39;burning&#39;, &#39;officer&#39;], [&#39;report&#39;, &#39;burning&#39;, &#39;officer&#39;, &#39;storm&#39;, &#39;truck&#39;], [&#39;girl&#39;, &#39;baby&#39;, &#39;guilty&#39;, &#39;lostprophets&#39;, &#39;hewitt&#39;], [&#39;black&#39;, &#39;friday&#39;, &#39;sale&#39;, &#39;deal&#39;, &#39;monday&#39;], [&#39;heart&#39;, &#39;woman&#39;, &#39;pill&#39;, &#39;frozen&#39;, &#39;crisis&#39;]] . !tail -n 5 googlenews.txt &gt; test.txt !cat test.txt . ray whitney return will dallas star huge boost offensively s relied intermediary probe spacex sept upper stage nokia lumia tablet kill surface lakers net preview neighbor helped save girl imprisoned year speaks . test_handler = TextHandler(&quot;test.txt&quot;) test_handler.prepare() # create vocabulary and training data # generate BERT data testing_bert = bert_embeddings_from_file(&quot;test.txt&quot;, &quot;distiluse-base-multilingual-cased&quot;) testing_dataset = CTMDataset(test_handler.bow, testing_bert, test_handler.idx2token) . # we sample n times and average to get a more accurate estimate of the document-topic distribution predicted_topics = [] thetas = np.zeros((len(testing_dataset), num_topics)) for a in range(0, 100): thetas = thetas + np.array(ctm.get_thetas(testing_dataset)) for idd in range(0, len(testing_dataset)): thetas[idd] = thetas[idd]/np.sum(thetas[idd]) predicted_topic = np.argmax(thetas[idd]) predicted_topics.append(predicted_topic) # document-topic distribution , list of the topic predicted for each testing document # thetas, predicted_topics . [22, 41, 44, 23, 47] . test_handler.load_text_file()[1] . &#39;s relied intermediary probe spacex sept upper stage n&#39; . ctm.get_topic_lists(20)[41] . [&#39;ison&#39;, &#39;comet&#39;, &#39;raptor&#39;, &#39;sun&#39;, &#39;bonobo&#39;, &#39;dna&#39;, &#39;flying&#39;, &#39;trouble&#39;, &#39;stereo&#39;, &#39;seahorse&#39;, &#39;researcher&#39;, &#39;preview&#39;, &#39;spacecraft&#39;, &#39;century&#39;, &#39;jellyfish&#39;, &#39;testing&#39;, &#39;minute&#39;, &#39;net&#39;, &#39;spectacular&#39;, &#39;congo&#39;] .",
            "url": "https://joaorafaelm.github.io/notebook/bert/topics/nlp/2021/04/17/multilang-topic-model.html",
            "relUrl": "/bert/topics/nlp/2021/04/17/multilang-topic-model.html",
            "date": " 2021 Apr 17"
        }
        
    
  
    
        ,"post5": {
            "title": "Cognitive complexity and python",
            "content": "Install dependencies . pip install cognitive_complexity astunparse tabulate . Requirement already satisfied: cognitive_complexity in /usr/local/lib/python3.7/dist-packages (1.2.0) Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (1.6.3) Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (0.8.9) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from cognitive_complexity) (57.0.0) Requirement already satisfied: six&lt;2.0,&gt;=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse) (1.15.0) Requirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse) (0.36.2) . . Import and define utils . import ast import astunparse from inspect import getsource from tabulate import tabulate from cognitive_complexity.api import get_cognitive_complexity_for_node from cognitive_complexity.utils.ast import has_recursive_calls, is_decorator, process_child_nodes, process_node_itself def get_cognitive_complexity(func): func = func if isinstance(func, str) else getsource(func) funcdef = ast.parse(func).body[0] if is_decorator(funcdef): return get_cognitive_complexity(funcdef.body[0]) details = [] complexity = 0 for node in funcdef.body: node_complexity = get_cognitive_complexity_for_node(node) complexity += node_complexity node_code = astunparse.unparse(node) if f&quot;{funcdef.name}(&quot; in node_code: # +1 for recursion node_complexity += 1 complexity += 1 details.append([node_complexity, node_code]) details.append([complexity, &quot;Total&quot;]) return complexity, details . Introduction . Formulated in a Fortran environment in 1976, Cyclomatic Complexity has long been the standard for measuring the complexity of a method’s control flow. It was originally intended “to identify software modules that will be difficult to test or maintain”, but while it accurately calculates the minimum number of test cases required to fully cover a method, it is not a satisfactory measure of understandability and it also doesn’t include modern language structures like try/catch, and lambdas. . -- Cognitive Complexity:A new way of measuring understandability, white paper by G. Ann Campbell . Basic criteria and methodology . As a remedy for these problems, Cognitive Complexity has been formulated to address modern language structures, and to produce values that are meaningful at the class and application levels. A Cognitive Complexity score is assessed according to three basic rules: . Ignore structures that allow multiple statements to be readably shorthanded into one | Increment (add one) for each break in the linear flow of the code | Increment when flow-breaking structures are nested Additionally, a complexity score is made up of four different types of increments: . A. Nesting - assessed for nesting control flow structures inside each other . B. Structural - assessed on control flow structures that are subject to a nesting increment, and that increase the nesting count . C. Fundamental - assessed on statements not subject to a nesting increment . D. Hybrid - assessed on control flow structures that are not subject to a nesting increment, but which do increase the nesting count . | -- Cognitive Complexity: A new way of measuring understandability, white paper by G. Ann Campbell . def f(n): if n &gt; 10: return True if n &lt; 5: return 20 else: return 2 return f(n) total, details = get_cognitive_complexity(f) print(tabulate(details, headers=[&quot;Complexity&quot;, &quot;Node&quot;], tablefmt=&quot;fancy_grid&quot;)) . ╒══════════════╤═════════════════╕ │ Complexity │ Node │ ╞══════════════╪═════════════════╡ │ 1 │ if (n &gt; 10): │ │ │ return True │ ├──────────────┼─────────────────┤ │ 2 │ if (n &lt; 5): │ │ │ return 20 │ │ │ else: │ │ │ return 2 │ ├──────────────┼─────────────────┤ │ 1 │ return f(n) │ ├──────────────┼─────────────────┤ │ 4 │ Total │ ╘══════════════╧═════════════════╛ . References . Cognitive Complexity, Because Testability != Understandability . | Cognitive Complexity: A new way of measuring understandability, white paper by G. Ann Campbell . | Cognitive Complexity: the New Guide to Refactoring for Maintainable Code . | Cognitive Complexity from CodeClimate docs . | Is Your Code Readable By Humans? Cognitive Complexity Tells You . | .",
            "url": "https://joaorafaelm.github.io/notebook/python/2021/04/10/cognitive-complexity.html",
            "relUrl": "/python/2021/04/10/cognitive-complexity.html",
            "date": " 2021 Apr 10"
        }
        
    
  
    
        ,"post6": {
            "title": "Training GPT2 with Colab and Google Drive",
            "content": "We&#39;ll be using aitextgen to finetune the model. . pip install aitextgen . Requirement already satisfied: aitextgen in /usr/local/lib/python3.7/dist-packages (0.5.2) Requirement already satisfied: pytorch-lightning&gt;=1.3.1 in /usr/local/lib/python3.7/dist-packages (from aitextgen) (1.3.4) Requirement already satisfied: fire&gt;=0.3.0 in /usr/local/lib/python3.7/dist-packages (from aitextgen) (0.4.0) Requirement already satisfied: transformers&gt;=4.5.1 in /usr/local/lib/python3.7/dist-packages (from aitextgen) (4.6.1) Requirement already satisfied: torch&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from aitextgen) (1.8.1+cu101) Requirement already satisfied: PyYAML&lt;=5.4.1,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (5.4.1) Requirement already satisfied: future&gt;=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.18.2) Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.3.0) Requirement already satisfied: fsspec[http]&gt;=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (2021.5.0) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (20.9) Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (2.4.1) Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (4.41.1) Requirement already satisfied: torchmetrics&gt;=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.3.2) Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.19.5) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire&gt;=0.3.0-&gt;aitextgen) (1.15.0) Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire&gt;=0.3.0-&gt;aitextgen) (1.1.0) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (3.0.12) Requirement already satisfied: tokenizers&lt;0.11,&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (0.10.3) Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (0.0.8) Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (0.0.45) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (2019.12.20) Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (4.0.1) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (2.23.0) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch&gt;=1.6.0-&gt;aitextgen) (3.7.4.3) Requirement already satisfied: aiohttp; extra == &#34;http&#34; in /usr/local/lib/python3.7/dist-packages (from fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (3.7.4.post0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (2.4.7) Requirement already satisfied: google-auth&lt;2,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.30.0) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.34.1) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (3.12.4) Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.0.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (57.0.0) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.8.0) Requirement already satisfied: wheel&gt;=0.26; python_version &gt;= &#34;3&#34; in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.36.2) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.4.4) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (3.3.4) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.12.0) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (1.0.1) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (3.4.1) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (2020.12.5) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (1.24.3) Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == &#34;http&#34;-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (5.1.0) Requirement already satisfied: async-timeout&lt;4.0,&gt;=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == &#34;http&#34;-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (3.0.1) Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == &#34;http&#34;-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.6.3) Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == &#34;http&#34;-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (21.2.0) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.2.8) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4; python_version &gt;= &#34;3.6&#34; in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (4.7.2) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (4.2.2) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.3.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (3.1.0) . . Import modules and mount google drive . from aitextgen import aitextgen from aitextgen.colab import mount_gdrive, copy_file_from_gdrive from aitextgen.TokenDataset import TokenDataset, merge_datasets from aitextgen.utils import build_gpt2_config from aitextgen.tokenizers import train_tokenizer mount_gdrive() . !curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt &gt; input.txt !head input.txt . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1089k 100 1089k 0 0 9002k 0 --:--:-- --:--:-- --:--:-- 9002k First Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: You are all resolved rather to die than to famish? All: . Train tokenizer . file_name = &quot;input.txt&quot; project_name = &quot;project_name&quot; # copy_file_from_gdrive(file_name) train_tokenizer(file_name); . INFO:aitextgen.tokenizers:Saving aitextgen-vocab.json and aitextgen-merges.txt to the current directory. You will need both files to build the GPT2Tokenizer. . Training the model should take about 30 minutes . model = None config = None for _ in [&quot;pytorch_model.bin&quot;, &quot;config.json&quot;, &quot;aitextgen_vocab.json&quot;, &quot;aitextgen_merges.json&quot;]: try: copy_file_from_gdrive(_, project_name) model = &quot;pytorch_model.bin&quot; config = &quot;config.json&quot; except FileNotFoundError: pass config = config or build_gpt2_config( vocab_size=5000, max_length=200, dropout=0.0, n_embd=256, n_layer=8, n_head=8 ) ai = aitextgen( vocab_file=&quot;aitextgen-vocab.json&quot;, merges_file=&quot;aitextgen-merges.txt&quot;, config=config, model=model, to_gpu=True ) . INFO:aitextgen:Constructing GPT-2 model from provided config. INFO:aitextgen:Using a custom tokenizer. . ai.train( file_name, line_by_line=False, num_steps=10000, generate_every=1000, save_every=500, learning_rate=1e-4, batch_size=128, save_gdrive=True, run_id=project_name ) . INFO:aitextgen.TokenDataset:Encoding 40,000 sets of tokens from input.txt. GPU available: True, used: True INFO:lightning:GPU available: True, used: True TPU available: False, using: 0 TPU cores INFO:lightning:TPU available: False, using: 0 TPU cores CUDA_VISIBLE_DEVICES: [0] INFO:lightning:CUDA_VISIBLE_DEVICES: [0] . . Generating examples . ai.generate( n=5, batch_size=5, prompt=&quot;Speak:&quot;, temperature=1.0, top_p=0.9, ) .",
            "url": "https://joaorafaelm.github.io/notebook/gpt2/colab/drive/2020/06/02/gpt2-google-drive-sync.html",
            "relUrl": "/gpt2/colab/drive/2020/06/02/gpt2-google-drive-sync.html",
            "date": " 2020 Jun 02"
        }
        
    
  
    
        ,"post7": {
            "title": "Audio synthesis with Forward Transformer TTS and WaveRNN Vocoder",
            "content": "Installing dependencies and pre-trained models . # Clone the Transformer TTS and WaveRNN repos !git clone https://github.com/as-ideas/TransformerTTS.git !cd TransformerTTS &amp;&amp; git checkout 1c1cb03 &amp;&amp; cd .. !git clone https://github.com/fatchord/WaveRNN # Install requirements !apt-get install -y espeak !pip install -r TransformerTTS/requirements.txt # Download the transformer pre-trained weights ! wget https://public-asai-dl-models.s3.eu-central-1.amazonaws.com/TransformerTTS/ljspeech_wavernn_forward_transformer.zip ! unzip -o ljspeech_wavernn_forward_transformer.zip # Unzip the wave pretrained model !unzip -o WaveRNN/pretrained/ljspeech.wavernn.mol.800k.zip -d WaveRNN/pretrained/ . Cloning into &#39;TransformerTTS&#39;... remote: Enumerating objects: 4107, done. remote: Counting objects: 100% (646/646), done. remote: Compressing objects: 100% (214/214), done. remote: Total 4107 (delta 456), reused 611 (delta 431), pack-reused 3461 Receiving objects: 100% (4107/4107), 26.00 MiB | 25.43 MiB/s, done. Resolving deltas: 100% (2826/2826), done. Note: checking out &#39;1c1cb03&#39;. You are in &#39;detached HEAD&#39; state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b &lt;new-branch-name&gt; HEAD is now at 1c1cb03 Add Audio class. WaveRNN and MelGAN compatible normalizations. Cloning into &#39;WaveRNN&#39;... remote: Enumerating objects: 928, done. remote: Total 928 (delta 0), reused 0 (delta 0), pack-reused 928 Receiving objects: 100% (928/928), 242.13 MiB | 34.46 MiB/s, done. Resolving deltas: 100% (525/525), done. Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: espeak-data libespeak1 libportaudio2 libsonic0 The following NEW packages will be installed: espeak espeak-data libespeak1 libportaudio2 libsonic0 0 upgraded, 5 newly installed, 0 to remove and 39 not upgraded. Need to get 1,219 kB of archives. After this operation, 3,031 kB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB] Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsonic0 amd64 0.2.0-6 [13.4 kB] Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak-data amd64 1.48.04+dfsg-5 [934 kB] Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libespeak1 amd64 1.48.04+dfsg-5 [145 kB] Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak amd64 1.48.04+dfsg-5 [61.6 kB] Fetched 1,219 kB in 1s (927 kB/s) Selecting previously unselected package libportaudio2:amd64. (Reading database ... 160772 files and directories currently installed.) Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ... Unpacking libportaudio2:amd64 (19.6.0-1) ... Selecting previously unselected package libsonic0:amd64. Preparing to unpack .../libsonic0_0.2.0-6_amd64.deb ... Unpacking libsonic0:amd64 (0.2.0-6) ... Selecting previously unselected package espeak-data:amd64. Preparing to unpack .../espeak-data_1.48.04+dfsg-5_amd64.deb ... Unpacking espeak-data:amd64 (1.48.04+dfsg-5) ... Selecting previously unselected package libespeak1:amd64. Preparing to unpack .../libespeak1_1.48.04+dfsg-5_amd64.deb ... Unpacking libespeak1:amd64 (1.48.04+dfsg-5) ... Selecting previously unselected package espeak. Preparing to unpack .../espeak_1.48.04+dfsg-5_amd64.deb ... Unpacking espeak (1.48.04+dfsg-5) ... Setting up libportaudio2:amd64 (19.6.0-1) ... Setting up espeak-data:amd64 (1.48.04+dfsg-5) ... Setting up libsonic0:amd64 (0.2.0-6) ... Setting up libespeak1:amd64 (1.48.04+dfsg-5) ... Setting up espeak (1.48.04+dfsg-5) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... Processing triggers for libc-bin (2.27-3ubuntu1.2) ... /sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r TransformerTTS/requirements.txt (line 1)) (3.2.2) Requirement already satisfied: librosa&gt;=0.7.1 in /usr/local/lib/python3.7/dist-packages (from -r TransformerTTS/requirements.txt (line 2)) (0.8.0) Requirement already satisfied: numpy&gt;=1.17.4 in /usr/local/lib/python3.7/dist-packages (from -r TransformerTTS/requirements.txt (line 3)) (1.19.5) Collecting phonemizer==2.1 Downloading https://files.pythonhosted.org/packages/d3/82/666045375029df9c2f274923539f43346a7b7abc349b02e33dff585da56f/phonemizer-2.1-py3-none-any.whl (47kB) |████████████████████████████████| 51kB 4.6MB/s Collecting ruamel.yaml&gt;=0.16.6 Downloading https://files.pythonhosted.org/packages/9e/00/1ba32614cc9572fd6e98dbfdf642f55f9c5ed8a89ab9328d2ce6f39e6fb3/ruamel.yaml-0.17.7-py3-none-any.whl (108kB) |████████████████████████████████| 112kB 14.5MB/s Requirement already satisfied: tensorflow&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from -r TransformerTTS/requirements.txt (line 6)) (2.5.0) Requirement already satisfied: tqdm&gt;=4.38.0 in /usr/local/lib/python3.7/dist-packages (from -r TransformerTTS/requirements.txt (line 7)) (4.41.1) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;-r TransformerTTS/requirements.txt (line 1)) (2.8.1) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;-r TransformerTTS/requirements.txt (line 1)) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;-r TransformerTTS/requirements.txt (line 1)) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;-r TransformerTTS/requirements.txt (line 1)) (1.3.1) Requirement already satisfied: scikit-learn!=0.19.0,&gt;=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (0.22.2.post1) Requirement already satisfied: audioread&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (2.1.9) Requirement already satisfied: resampy&gt;=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (0.2.2) Requirement already satisfied: soundfile&gt;=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (0.10.3.post1) Requirement already satisfied: pooch&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (1.3.0) Requirement already satisfied: scipy&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (1.4.1) Requirement already satisfied: numba&gt;=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (0.51.2) Requirement already satisfied: decorator&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (4.4.2) Requirement already satisfied: joblib&gt;=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (1.0.1) Collecting segments Downloading https://files.pythonhosted.org/packages/1e/ae/02d31d73cfc3fa1dc74b7b7f14820fadc287e74406583d7af7b80fcaac41/segments-2.2.0-py2.py3-none-any.whl Requirement already satisfied: attrs&gt;=18.1 in /usr/local/lib/python3.7/dist-packages (from phonemizer==2.1-&gt;-r TransformerTTS/requirements.txt (line 4)) (21.2.0) Collecting ruamel.yaml.clib&gt;=0.1.2; platform_python_implementation == &#34;CPython&#34; and python_version &lt; &#34;3.10&#34; Downloading https://files.pythonhosted.org/packages/5e/6e/f652c56bbb2c3d3fca252ffc7c0358597f57a1bbdf484dac683054950c63/ruamel.yaml.clib-0.2.2-cp37-cp37m-manylinux1_x86_64.whl (547kB) |████████████████████████████████| 552kB 15.7MB/s Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.4.0) Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.12.0) Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.12) Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.2.0) Requirement already satisfied: protobuf&gt;=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (3.12.4) Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.36.2) Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.34.1) Requirement already satisfied: tensorflow-estimator&lt;2.6.0,&gt;=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (2.5.0) Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.1.2) Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.15.0) Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.12.1) Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.6.3) Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.1.0) Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (2.5.0) Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (2.5.0.dev2021032900) Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (3.7.4.3) Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (3.1.0) Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (3.3.0) Requirement already satisfied: cffi&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile&gt;=0.9.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (1.14.5) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch&gt;=1.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (2.23.0) Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch&gt;=1.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (1.4.4) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch&gt;=1.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (20.9) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba&gt;=0.43.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (57.0.0) Requirement already satisfied: llvmlite&lt;0.35,&gt;=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba&gt;=0.43.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (0.34.0) Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segments-&gt;phonemizer==2.1-&gt;-r TransformerTTS/requirements.txt (line 4)) (2019.12.20) Collecting clldutils&gt;=1.7.3 Downloading https://files.pythonhosted.org/packages/f7/99/3ea7e3595e730332c2799938e2dad456916772e571fa0cd8dcdfb9d5780a/clldutils-3.9.0-py2.py3-none-any.whl (195kB) |████████████████████████████████| 204kB 18.3MB/s Collecting csvw&gt;=1.5.6 Downloading https://files.pythonhosted.org/packages/55/ae/afb43a6b88c4202d29e4ec7aca76633d8c530140f4f5a32ee762d07c4607/csvw-1.11.0-py2.py3-none-any.whl Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.0.1) Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.6.1) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.4.4) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.8.0) Requirement already satisfied: google-auth&lt;2,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.30.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (3.3.4) Requirement already satisfied: cached-property; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.5.2) Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi&gt;=1.0-&gt;soundfile&gt;=0.9.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (2.20) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;pooch&gt;=1.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;pooch&gt;=1.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;pooch&gt;=1.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (2020.12.5) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;pooch&gt;=1.0-&gt;librosa&gt;=0.7.1-&gt;-r TransformerTTS/requirements.txt (line 2)) (2.10) Collecting colorlog Downloading https://files.pythonhosted.org/packages/32/e6/e9ddc6fa1104fda718338b341e4b3dc31cd8039ab29e52fc73b508515361/colorlog-5.0.1-py2.py3-none-any.whl Requirement already satisfied: tabulate&gt;=0.7.7 in /usr/local/lib/python3.7/dist-packages (from clldutils&gt;=1.7.3-&gt;segments-&gt;phonemizer==2.1-&gt;-r TransformerTTS/requirements.txt (line 4)) (0.8.9) Requirement already satisfied: uritemplate&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from csvw&gt;=1.5.6-&gt;segments-&gt;phonemizer==2.1-&gt;-r TransformerTTS/requirements.txt (line 4)) (3.0.1) Collecting rfc3986 Downloading https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl Collecting isodate Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB) |████████████████████████████████| 51kB 7.9MB/s Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (1.3.0) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (4.2.2) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.2.8) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4; python_version &gt;= &#34;3.6&#34; in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (4.7.2) Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from markdown&gt;=2.6.8-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (4.0.1) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (3.1.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (0.4.8) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;markdown&gt;=2.6.8-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.2.0-&gt;-r TransformerTTS/requirements.txt (line 6)) (3.4.1) Installing collected packages: colorlog, rfc3986, isodate, csvw, clldutils, segments, phonemizer, ruamel.yaml.clib, ruamel.yaml Successfully installed clldutils-3.9.0 colorlog-5.0.1 csvw-1.11.0 isodate-0.6.0 phonemizer-2.1 rfc3986-1.5.0 ruamel.yaml-0.17.7 ruamel.yaml.clib-0.2.2 segments-2.2.0 --2021-06-07 01:11:12-- https://public-asai-dl-models.s3.eu-central-1.amazonaws.com/TransformerTTS/ljspeech_wavernn_forward_transformer.zip Resolving public-asai-dl-models.s3.eu-central-1.amazonaws.com (public-asai-dl-models.s3.eu-central-1.amazonaws.com)... 52.219.168.9 Connecting to public-asai-dl-models.s3.eu-central-1.amazonaws.com (public-asai-dl-models.s3.eu-central-1.amazonaws.com)|52.219.168.9|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 210039489 (200M) [application/zip] Saving to: ‘ljspeech_wavernn_forward_transformer.zip’ ljspeech_wavernn_fo 100%[===================&gt;] 200.31M 21.1MB/s in 11s 2021-06-07 01:11:23 (19.0 MB/s) - ‘ljspeech_wavernn_forward_transformer.zip’ saved [210039489/210039489] Archive: ljspeech_wavernn_forward_transformer.zip creating: ljspeech_wavernn_forward_transformer/ inflating: __MACOSX/._ljspeech_wavernn_forward_transformer inflating: ljspeech_wavernn_forward_transformer/.DS_Store inflating: __MACOSX/ljspeech_wavernn_forward_transformer/._.DS_Store creating: ljspeech_wavernn_forward_transformer/wavernn/ inflating: __MACOSX/ljspeech_wavernn_forward_transformer/._wavernn inflating: ljspeech_wavernn_forward_transformer/wavernn/.DS_Store inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/._.DS_Store inflating: ljspeech_wavernn_forward_transformer/wavernn/forward_config.yaml inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/._forward_config.yaml creating: ljspeech_wavernn_forward_transformer/wavernn/forward_weights/ inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/._forward_weights inflating: ljspeech_wavernn_forward_transformer/wavernn/data_config.yaml inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/._data_config.yaml inflating: ljspeech_wavernn_forward_transformer/wavernn/forward_weights/checkpoint inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/forward_weights/._checkpoint inflating: ljspeech_wavernn_forward_transformer/wavernn/forward_weights/ckpt-133.index inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/forward_weights/._ckpt-133.index inflating: ljspeech_wavernn_forward_transformer/wavernn/forward_weights/ckpt-133.data-00001-of-00002 inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/forward_weights/._ckpt-133.data-00001-of-00002 inflating: ljspeech_wavernn_forward_transformer/wavernn/forward_weights/ckpt-133.data-00000-of-00002 inflating: __MACOSX/ljspeech_wavernn_forward_transformer/wavernn/forward_weights/._ckpt-133.data-00000-of-00002 Archive: WaveRNN/pretrained/ljspeech.wavernn.mol.800k.zip inflating: WaveRNN/pretrained/latest_weights.pyt . . Load pre trained models . from pathlib import Path WaveRNN_path = &#39;WaveRNN/&#39; TTS_path = &#39;TransformerTTS/&#39; config_path = Path(&#39;ljspeech_wavernn_forward_transformer/wavernn&#39;) # wavernn model import sys sys.path.append(WaveRNN_path) from utils.dsp import hp from models.fatchord_version import WaveRNN import torch import numpy as np WaveRNN_path = Path(WaveRNN_path) # Load pretrained model try: hp.configure(WaveRNN_path / &#39;hparams.py&#39;) # Load hparams from file except: # cant reconfigure, bypass to avoid restart runtime pass if torch.cuda.is_available(): device = torch.device(&#39;cuda&#39;) else: device = torch.device(&#39;cpu&#39;) wave_model = WaveRNN(rnn_dims=hp.voc_rnn_dims, fc_dims=hp.voc_fc_dims, bits=hp.bits, pad=hp.voc_pad, upsample_factors=hp.voc_upsample_factors, feat_dims=hp.num_mels, compute_dims=hp.voc_compute_dims, res_out_dims=hp.voc_res_out_dims, res_blocks=hp.voc_res_blocks, hop_length=hp.hop_length, sample_rate=hp.sample_rate, mode=hp.voc_mode).to(device) wave_model.load(str(WaveRNN_path / &#39;pretrained/latest_weights.pyt&#39;)) # Ignore some TF warnings import tensorflow as tf tf.get_logger().setLevel(&#39;ERROR&#39;) # fix deprecated module on librosa import soundfile as sf import librosa class output: write_wav = lambda path, data, sr: sf.write(path, data, samplerate=sr, subtype=&#39;PCM_24&#39;) librosa.output = output # Generate sample with pre-trained WaveRNN vocoder hp_data = hp def generate(mel, file_name=&quot;sample.wav&quot;, batch_pred=False, batch_size=5000, hp=hp_data): _ = wave_model.generate(mel.clip(0,1)[np.newaxis,:,:], file_name, batch_pred, batch_size, hp.voc_overlap, hp.mu_law) # Load wav file ipd.display(ipd.Audio(file_name)) # ljspeech_wavernn_forward_model sys.path.remove(&#39;WaveRNN/&#39;) sys.modules.pop(&#39;utils&#39;) sys.path.append(TTS_path) # Load pretrained models from utils.config_manager import ConfigManager from utils.audio import Audio import IPython.display as ipd config_loader = ConfigManager(str(config_path), model_kind=&#39;forward&#39;) audio = Audio(config_loader.config) model = config_loader.load_model(str(config_path / &#39;forward_weights/ckpt-133&#39;)) . Trainable Parameters: 4.234M WARNING: could not retrieve git hash. Command &#39;[&#39;git&#39;, &#39;describe&#39;, &#39;--always&#39;]&#39; returned non-zero exit status 128. WARNING: could not check git hash. Command &#39;[&#39;git&#39;, &#39;describe&#39;, &#39;--always&#39;]&#39; returned non-zero exit status 128. restored weights from ljspeech_wavernn_forward_transformer/wavernn/forward_weights/ckpt-133 at step 665000 . sentence = &#39;Transformer TTS: A Text-to-Speech Transformer in TensorFlow 2, Audio synthesis with Forward Transformer TTS and WaveRNN Vocoder&#39; #@param {type:&quot;string&quot;} speed_regulator = 1 #@param {type:&quot;slider&quot;, min:0, max:2, step:0.1} batch_pred = True #@param {type:&quot;boolean&quot;} batch_size = 1 #@param out_normal = model.predict(sentence, speed_regulator=speed_regulator) # Normalize for WaveRNN mel = (out_normal[&#39;mel&#39;].numpy().T+4.)/8. generate(mel, batch_pred=batch_pred, batch_size=batch_size) . | ████████████████ 445500/445905 | Batch Size: 405 | Gen Rate: 233.7kHz | . Your browser does not support the audio element.",
            "url": "https://joaorafaelm.github.io/notebook/tensorflow/transformer/tts/2019/06/04/forward-tts-transformer-and-wavernn-vocoder.html",
            "relUrl": "/tensorflow/transformer/tts/2019/06/04/forward-tts-transformer-and-wavernn-vocoder.html",
            "date": " 2019 Jun 04"
        }
        
    
  
    
        ,"post8": {
            "title": "Text Classification with Python",
            "content": "If you are already familiar with what text classification is, you might want to jump to this part, or get the code here. . What is Text Classification? . Document or text classification is used to classify information, that is, assign a category to a text; it can be a document, a tweet, a simple message, an email, and so on. In this article, I will show how you can classify retail products into categories. Although in this example the categories are structured in a hierarchy, to keep it simple I will consider all subcategories as top-level. . If you are looking for complex implementations of large scale hierarchical text classification, I will leave links to some really good papers and projects at the end of this post. . Getting started . Now, before you go any further, make sure you have installed Python3+ and virtualenv (optional, but I highly recommend you to use it). . Let’s break down the problem into steps: . Setting up the environment | Gathering the data | Extracting features from the dataset | Testing the algorithms | . Setting up the environment . The main packages used in this projects are: sklearn, nltk and dataset. Due to the size of the data-set, it might take some time to clone/download the repository; NLTK data is also considerably big. Run the following commands to setup the project structure and download the required packages: . # Clone the repo git clone https://github.com/joaorafaelm/text-classification-python; cd text-classification-python; # Create virtualenv; skip this one if you dont have virtualenv. virtualenv venv &amp;&amp; source venv/bin/activate; # Install all requirements pip install -r requirements.txt; # Download all data that NLTK uses python -m nltk.downloader all; . Gathering the data . The dataset that will be used was created by scraping some products from Amazon. Scraping might be fine for projects where only a small amount of data is required, but it can be a really slow process since it is very simple for a server to detect a robot, unless you are rotating over a list of proxies, which can slow the process even more. . Using this script, I downloaded information of over 22,000 products, organized into 42 top-level categories, and a total of 6233 subcategories. See the whole category tree structure here. . Again, to keep it simple I will be using only 3 top-level categories: Automotive, Home &amp; Kitchen and Industrial &amp; Scientific. Including the subcategories, there are 36 categories in total. . To extract the data from database, run the command: . # dump from db to dumps/all_products.json datafreeze .datafreeze.yaml; . Inside the project you will also find a file called data_prep.py, in this file you can set the categories you want to use, the minimum amount of samples per category and the depth of a category. As I said before, only 3 categories are going to be used: Home &amp; Kitchen, Industrial &amp; Scientific and Automotive. I did not specify the depth of the subcategories, but I did specify 50 as the minimum amount of samples (is this case, products) per category. To transform the data dumped from the database into this “filtered” data, just execute the file: . python data_prep.py . The script will create a new file called products.json at the root of the project, and print out the category tree structure. Change the value of the variables default_depth, min_samples and domain if you need more data. . Extracting features from the dataset . In order to run machine learning algorithms, we need to transform the text into numerical vectors. Bag-of-words is one of the most used models, it assigns a numerical value to a word, creating a list of numbers. It can also assign a value to a set of words, known as N-gram. . Scikit provides a vectorizer called TfidfVectorizer which transforms the text based on the bag-of-words/n-gram model, additionally, it computes term frequencies and evaluate each word using the tf-idf weighting scheme. . Counting terms frequencies might not be enough sometimes. Take the words ‘cars’ and ‘car’ for example, by only using tf-idf, they are considered different words. This problem can be solved using Stemming and/or Lemmatisation. And there is where NLTK comes into play. . NLTK offers some pretty useful tools for NLP. For this project I used it to perform Lemmatisation and Part-of-speech tagging. . With Lemmatisation we can group together the inflected forms of a word. For example, the words ‘walked’, ‘walks’ and ‘walking’, can be grouped into their base form, the verb ‘walk’. That is why we need to POS tag each word as a noun, verb, adverb, and so on. . It is also worth noting that some words despite the fact that they appear frequently, they do not really make any difference for classification, in fact they could even help misclassify a text. Words like ‘a’, ‘an’, ‘the’, ‘to’, ‘or’ etc, are known as stop-words. These words can be ignored during the tokenization process. . Testing the algorithms . Now that we have all the features and labels, it is time to train the classifiers. There are a number of algorithms you can use for this type of problem, for example: Multinomial Naive Bayes, Linear SVC, SGD Classifier, K-Neighbors Classifier, Random Forest Classifier. Inside the file classify.py you can find an example using the SGDClassifier. Run it yourself using the command: . python classify.py . It will print out the accuracy of each category, along with the confusion matrix. . Here is how it is implemented: load the dataset, initiate WordNetLemmatizer and PerceptronTagger from NLTK. As I was only interested in nouns, verbs, adverbs and adjectives, I created a lookup dict to quicken up the process. Although NLTK is great, its aim is not performance, so I also implemented python’s LRU Cache for both lemmatize and tagger functions. . # Load data dataset = json.load(open(&#39;products.json&#39;, encoding=&#39;utf-8&#39;)) # Initiate lemmatizer wnl = WordNetLemmatizer() # Load tagger pickle tagger = PerceptronTagger() # Lookup if tag is noun, verb, adverb or an adjective tags = {&#39;N&#39;: wn.NOUN, &#39;V&#39;: wn.VERB, &#39;R&#39;: wn.ADV, &#39;J&#39;: wn.ADJ} # Memoization of POS tagging and Lemmatizer lemmatize_mem = lru_cache(maxsize=10000)(wnl.lemmatize) tagger_mem = lru_cache(maxsize=10000)(tagger.tag) . Next, the tokenizer function was created. It breaks the text into words and iterate over them, ignoring the stop-words and POS-tagging/Lemmatising the rest. This function will receive all documents from the dataset. . # POS tag sentences and lemmatize each word def tokenizer(text): for token in wordpunct_tokenize(text): if token not in ENGLISH_STOP_WORDS: tag = tagger_mem(frozenset({token})) yield lemmatize_mem(token, tags.get(tag[0][1], wn.NOUN)) . At last the pipeline is defined; the first step is to call TfidfVectorizer, with the tokenizer function preprocessing each document, and then pass through the SGDClassifier. The classifier is trained and tested using 10-fold Cross-Validation provided by the cross_val_predict method from scikit-learn. . # Pipeline definition pipeline = Pipeline([ (&#39;vectorizer&#39;, TfidfVectorizer( tokenizer=tokenizer, ngram_range=(1, 2), stop_words=ENGLISH_STOP_WORDS, sublinear_tf=True, min_df=0.00009 )), (&#39;classifier&#39;, SGDClassifier( alpha=1e-4, n_jobs=-1 )), ]) # Cross validate using k-fold y_pred = cross_val_predict( pipeline, dataset.get(&#39;data&#39;), y=dataset.get(&#39;target&#39;), cv=10, n_jobs=-1, verbose=20 ) # Print out precison, recall and f1 scode. print(classification_report( dataset.get(&#39;target&#39;), y_pred, target_names=dataset.get(&#39;target_names&#39;), digits=3 )) . And here are the accuracy results for each algorithm I tested (all algorithms were tested with their default parameters): . Algorithms Precision Recall . SGDClassifier | 0.975 | 0.975 | . LinearSVC | 0.972 | 0.971 | . RandomForest | 0.938 | 0.936 | . MultinomialNB | 0.882 | 0.851 | . The precision is the percentage of the test samples that were classified to the category and actually belonged to the category. . The recall is the percentage of all the test samples that originally belonged to the category and in the evaluation process were correctly classified to the category. . Conclusion . As the category tree gets bigger, and you have more and more data to classify, you cannot use a model as simple as the one above (well, you can but its precision will be very low, not to mention the computational cost). Another important thing to notice, is how you structure the categories, in amazon category structure, a lot of subcategories are so confused that I doubt even humans could correctly classify products to them. The full code of this post can be found here. . If you noticed something wrong, or you know something that can make the algorithms better, please do comment bellow. Thanks for reading! . Further reading . Classifier Statistics . | A Meta-Top-Down Method for Large-Scale Hierarchical Classification . | A survey of hierarchical classification across different application domains . | Hierarchical Text Categorization and Its Application to Bioinformatics . | Comparing Several Approaches for Hierarchical Classification of Proteins with Decision Trees . | Tokenizing Words and Sentences with NLTK . | Natural Language Processing with Deep Learning . | Document Classification using Multinomial Naive Bayes Classifier . | .",
            "url": "https://joaorafaelm.github.io/notebook/nlp/2017/08/24/text-classification-with-python.html",
            "relUrl": "/nlp/2017/08/24/text-classification-with-python.html",
            "date": " 2017 Aug 24"
        }
        
    
  
    
        ,"post9": {
            "title": "GraphQL and Django in 5 minutes",
            "content": "TL;DR Jump to the coding part or get the code here. . What is GraphQL? . GraphQL query is a string that is sent to a server to be interpreted and fulfilled, which then returns JSON back to the client. It was created by Facebook in 2012 and the first specification draft was made public in 2015. . In this tutorial I will cover the basics of working with GraphQL and Django. . Getting started . Before creating the project and all, make sure you have virtualenv installed, so that the packages used in this tutorial won’t be installed system-wide. . # Clone the repo git clone https://github.com/joaorafaelm/graphql-django-example; cd graphql-django-example; # Create virtualenv virtualenv venv &amp;&amp; source venv/bin/activate; # Install django and graphene pip install -r requirements.txt; # Setup db python manage.py migrate; . Run python manage.py loaddata books.json to populate the db, or run python manage.py createsuperuser and then add some data using the admin interface. . Models and GraphQL Schema . This example is going to use the models Author, Book and Publisher. . # bookstore/store/models.py from django.db import models class Publisher(models.Model): name = models.CharField(max_length=30) website = models.URLField() def __str__(self): return self.name class Author(models.Model): first_name = models.CharField(max_length=30) last_name = models.CharField(max_length=40) email = models.EmailField() def __str__(self): return &#39;%s %s&#39; % (self.first_name, self.last_name) class Book(models.Model): title = models.CharField(max_length=100) authors = models.ManyToManyField(Author) publisher = models.ForeignKey(Publisher) publication_date = models.DateField() def __str__(self): return self.title . After creating the models, the Schema should be created, which will be used to serve the API. . At the time of writing this post, graphene-django version is 1.3 and it does not handle ManyToMany fields properly, that is why the resolve_authors method was added. This issue has been resolved for the next release. . # bookstore/schema.py import graphene from graphene_django.types import DjangoObjectType from graphene_django.debug import DjangoDebug from bookstore.store.models import Author, Book, Publisher class AuthorType(DjangoObjectType): class Meta: model = Author class BookType(DjangoObjectType): authors = graphene.List(AuthorType) # Many To Many fix until next release. # https://github.com/graphql-python/graphene-django/issues/155 @graphene.resolve_only_args def resolve_authors(self): return self.authors.all() class Meta: model = Book class PublisherType(DjangoObjectType): class Meta: model = Publisher class Query(graphene.ObjectType): all_authors = graphene.List(AuthorType) all_books = graphene.List(BookType) all_publishers = graphene.List(PublisherType) # Debug field (rawSql, parameters etc). debug = graphene.Field(DjangoDebug, name=&#39;__debug&#39;) def resolve_all_authors(self, args, context, info): return Author.objects.all() def resolve_all_books(self, args, context, info): return Book.objects.select_related(&#39;publisher&#39;).all() def resolve_all_publishers(self, args, context, info): return Publisher.objects.all() schema = graphene.Schema(query=Query) . Last but not least, the GraphQL URL must be added into the urls.py file. . # bookstore/urls.py from django.conf.urls import url from django.contrib import admin from graphene_django.views import GraphQLView from bookstore.schema import schema urlpatterns = [ url(r&#39;^admin/&#39;, admin.site.urls), url(r&#39;^graphql&#39;, GraphQLView.as_view(graphiql=True, schema=schema)), ] . You can now run python manage.py runserver and start using the API at http://localhost:8000/graphql. . Querying and debugging . GraphiQL provides a graphical interactive in-browser GraphQL IDE, including some features such as syntax highlighting, real-time error reporting, automatic query completion etc. . I will show some query examples, but you can learn more about querying at graphql.org/learn/queries/. . Given the following query, we can retrieve all books registered along with their authors. . { allBooks { title, authors { firstName, lastName } } } . And the response… . { &quot;data&quot;: { &quot;allBooks&quot;: [ { &quot;title&quot;: &quot;Resurrection&quot;, &quot;authors&quot;: [ { &quot;firstName&quot;: &quot;Leo&quot;, &quot;lastName&quot;: &quot;Tolstoy&quot; } ] }, { &quot;title&quot;: &quot;Childhood&quot;, &quot;authors&quot;: [ { &quot;firstName&quot;: &quot;Leo&quot;, &quot;lastName&quot;: &quot;Tolstoy&quot; } ] } ] } } . Using the __debug field you can get information about the actual SQL query. . { allAuthors {lastName} __debug { sql {rawSql, duration} } } . Response: . { &quot;data&quot;: { &quot;allAuthors&quot;: [ { &quot;lastName&quot;: &quot;King&quot; }, { &quot;lastName&quot;: &quot;Tolstoy&quot; }, { &quot;lastName&quot;: &quot;Gaiman&quot; }, { &quot;lastName&quot;: &quot;Pratchett&quot; } ], &quot;__debug&quot;: { &quot;sql&quot;: [ { &quot;rawSql&quot;: &quot;SELECT &quot;store_author &quot;. &quot;id &quot;, &quot;store_author &quot;. &quot;first_name &quot;, &quot;store_author &quot;. &quot;last_name &quot;, &quot;store_author &quot;. &quot;email &quot; FROM &quot;store_author &quot;&quot;, &quot;duration&quot;: 0.0009260177612304688 } ] } } } . All this code is on my Github. Please do fork it and make pull requests regarding any issues or improvements you may have with my code. .",
            "url": "https://joaorafaelm.github.io/notebook/graphql/django/2017/08/05/graphql-and-django-in-5-minutes.html",
            "relUrl": "/graphql/django/2017/08/05/graphql-and-django-in-5-minutes.html",
            "date": " 2017 Aug 05"
        }
        
    
  

  
  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://joaorafaelm.github.io/notebook/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
