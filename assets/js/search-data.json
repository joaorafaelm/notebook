{
  
    
        "post0": {
            "title": "Automated data exploration with pandas profiling",
            "content": "pip install --upgrade pandas-profiling . Requirement already up-to-date: pandas-profiling in /usr/local/lib/python3.7/dist-packages (3.0.0) Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.0.1) Requirement already satisfied, skipping upgrade: tqdm&gt;=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (4.61.0) Requirement already satisfied, skipping upgrade: requests&gt;=2.24.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.25.1) Requirement already satisfied, skipping upgrade: matplotlib&gt;=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (3.2.2) Requirement already satisfied, skipping upgrade: jinja2&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.11.3) Requirement already satisfied, skipping upgrade: htmlmin&gt;=0.1.12 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.1.12) Requirement already satisfied, skipping upgrade: visions[type_image_path]==0.7.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.7.1) Requirement already satisfied, skipping upgrade: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.1.5) Requirement already satisfied, skipping upgrade: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.19.5) Requirement already satisfied, skipping upgrade: missingno&gt;=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.4.2) Requirement already satisfied, skipping upgrade: phik&gt;=0.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.11.2) Requirement already satisfied, skipping upgrade: tangled-up-in-unicode==0.1.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.1.0) Requirement already satisfied, skipping upgrade: seaborn&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.11.1) Requirement already satisfied, skipping upgrade: pydantic&gt;=1.8.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.8.2) Requirement already satisfied, skipping upgrade: scipy&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.4.1) Requirement already satisfied, skipping upgrade: PyYAML&gt;=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (5.4.1) Requirement already satisfied, skipping upgrade: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling) (1.24.3) Requirement already satisfied, skipping upgrade: chardet&lt;5,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling) (3.0.4) Requirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling) (2.10) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling) (2020.12.5) Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling) (2.4.7) Requirement already satisfied, skipping upgrade: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling) (0.10.0) Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling) (2.8.1) Requirement already satisfied, skipping upgrade: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling) (1.3.1) Requirement already satisfied, skipping upgrade: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2&gt;=2.11.1-&gt;pandas-profiling) (2.0.1) Requirement already satisfied, skipping upgrade: bottleneck in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (1.3.2) Requirement already satisfied, skipping upgrade: networkx&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (2.5.1) Requirement already satisfied, skipping upgrade: multimethod==1.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (1.4) Requirement already satisfied, skipping upgrade: attrs&gt;=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (21.2.0) Requirement already satisfied, skipping upgrade: imagehash; extra == &#34;type_image_path&#34; in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (4.2.0) Requirement already satisfied, skipping upgrade: Pillow; extra == &#34;type_image_path&#34; in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling) (7.1.2) Requirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3-&gt;pandas-profiling) (2018.9) Requirement already satisfied, skipping upgrade: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic&gt;=1.8.1-&gt;pandas-profiling) (3.7.4.3) Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from cycler&gt;=0.10-&gt;matplotlib&gt;=3.2.0-&gt;pandas-profiling) (1.15.0) Requirement already satisfied, skipping upgrade: decorator&lt;5,&gt;=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx&gt;=2.4-&gt;visions[type_image_path]==0.7.1-&gt;pandas-profiling) (4.4.2) Requirement already satisfied, skipping upgrade: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash; extra == &#34;type_image_path&#34;-&gt;visions[type_image_path]==0.7.1-&gt;pandas-profiling) (1.1.1) . . Importing libraries and Titanic dataset . import pandas as pd from pathlib import Path from ipywidgets import widgets from pandas_profiling import ProfileReport import warnings warnings.filterwarnings(&quot;ignore&quot;) file_name = &quot;https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv&quot; df = pd.read_csv(file_name) profile = ProfileReport(df, title=&quot;Titanic Dataset&quot;, html={&#39;style&#39;: {&#39;full_width&#39;: True}}, sort=None) . profile.to_widgets() . profile . .",
            "url": "https://joaorafaelm.github.io/notebook/pandas/2021/06/06/automated-data-exploration-with-pandas-profiling-and-titanic-dataset.html",
            "relUrl": "/pandas/2021/06/06/automated-data-exploration-with-pandas-profiling-and-titanic-dataset.html",
            "date": " 2021 Jun 06"
        }
        
    
  
    
        ,"post1": {
            "title": "Deploying a XGBoost model",
            "content": "import pandas as pd import numpy as np import xgboost as xgb import re from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error np.random.seed(42) . Load data and build a model . X, y = fetch_openml(&quot;titanic&quot;, version=1, as_frame=True, return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2) . X_y_train = xgb.DMatrix(data=X_train[[&quot;pclass&quot;, &quot;age&quot;, &quot;fare&quot;, &quot;sibsp&quot;, &quot;parch&quot;]], label=y_train.astype(int)) X_test = xgb.DMatrix(data=X_test[[&quot;pclass&quot;, &quot;age&quot;, &quot;fare&quot;, &quot;sibsp&quot;, &quot;parch&quot;]]) . X_train[[&quot;pclass&quot;, &quot;age&quot;, &quot;fare&quot;, &quot;sibsp&quot;, &quot;parch&quot;]].head() . pclass age fare sibsp parch . 662 3.0 | 40.0 | 7.2250 | 0.0 | 0.0 | . 164 1.0 | 35.0 | 26.5500 | 0.0 | 0.0 | . 871 3.0 | NaN | 7.7500 | 0.0 | 0.0 | . 1298 3.0 | 36.0 | 9.5000 | 0.0 | 0.0 | . 1004 3.0 | NaN | 7.7875 | 0.0 | 0.0 | . params = { &quot;base_score&quot;: np.mean(y_train.astype(int)), &quot;eta&quot;: 0.1, &quot;max_depth&quot;: 3, &quot;gamma&quot;: 3, &quot;objective&quot;: &quot;reg:squarederror&quot;, &quot;eval_metric&quot;: &quot;mae&quot; } model = xgb.train( params=params, dtrain=X_y_train, num_boost_round=3 ) . Visualization of model . xgb.to_graphviz(booster = model, num_trees=0) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 0 fare&lt;15.1729002 1 age&lt;16.5 0&#45;&gt;1 yes, missing 2 pclass&lt;2.5 0&#45;&gt;2 no 3 leaf=0.0223644935 1&#45;&gt;3 yes 4 leaf=&#45;0.0156309232 1&#45;&gt;4 no, missing 5 parch&lt;0.5 2&#45;&gt;5 yes, missing 6 fare&lt;23.3500004 2&#45;&gt;6 no 11 leaf=0.0153174726 5&#45;&gt;11 yes, missing 12 leaf=0.03650124 5&#45;&gt;12 no 13 leaf=0.00960917864 6&#45;&gt;13 yes 14 leaf=&#45;0.0235449504 6&#45;&gt;14 no, missing xgb.to_graphviz(booster = model, num_trees=1) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 0 fare&lt;15.1729002 1 age&lt;16.5 0&#45;&gt;1 yes, missing 2 pclass&lt;2.5 0&#45;&gt;2 no 3 leaf=0.0201979335 1&#45;&gt;3 yes 4 leaf=&#45;0.0140708359 1&#45;&gt;4 no, missing 5 leaf=0.0204214789 2&#45;&gt;5 yes, missing 6 fare&lt;23.3500004 2&#45;&gt;6 no 13 leaf=0.00866124686 6&#45;&gt;13 yes 14 leaf=&#45;0.0212272462 6&#45;&gt;14 no, missing xgb.to_graphviz(booster = model, num_trees=2) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 0 fare&lt;15.1729002 1 leaf=&#45;0.010894442 0&#45;&gt;1 yes, missing 2 pclass&lt;2.5 0&#45;&gt;2 no 5 leaf=0.0183849707 2&#45;&gt;5 yes, missing 6 leaf=&#45;0.00472340686 2&#45;&gt;6 no print(&quot; n&quot;.join(model.get_dump())) . 0:[fare&lt;15.1729002] yes=1,no=2,missing=1 1:[age&lt;16.5] yes=3,no=4,missing=4 3:leaf=0.0223644935 4:leaf=-0.0156309232 2:[pclass&lt;2.5] yes=5,no=6,missing=5 5:[parch&lt;0.5] yes=11,no=12,missing=11 11:leaf=0.0153174726 12:leaf=0.03650124 6:[fare&lt;23.3500004] yes=13,no=14,missing=14 13:leaf=0.00960917864 14:leaf=-0.0235449504 0:[fare&lt;15.1729002] yes=1,no=2,missing=1 1:[age&lt;16.5] yes=3,no=4,missing=4 3:leaf=0.0201979335 4:leaf=-0.0140708359 2:[pclass&lt;2.5] yes=5,no=6,missing=5 5:leaf=0.0204214789 6:[fare&lt;23.3500004] yes=13,no=14,missing=14 13:leaf=0.00866124686 14:leaf=-0.0212272462 0:[fare&lt;15.1729002] yes=1,no=2,missing=1 1:leaf=-0.010894442 2:[pclass&lt;2.5] yes=5,no=6,missing=5 5:leaf=0.0183849707 6:leaf=-0.00472340686 . Convert dump string to a .py file . def string_parser(s): if len(re.findall(r&quot;:leaf=&quot;, s)) == 0: out = re.findall(r&quot;[ w.-]+&quot;, s) tabs = re.findall(r&quot;[ t]+&quot;, s) if (out[4] == out[8]): missing_value_handling = (&quot; or np.isnan(x[&#39;&quot; + out[1] + &quot;&#39;]) &quot;) else: missing_value_handling = &quot;&quot; if len(tabs) &gt; 0: return (re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; if state == &#39; + out[0] + &#39;: n&#39; + re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; state = (&#39; + out[4] + &#39; if &#39; + &quot;x[&#39;&quot; + out[1] +&quot;&#39;]&lt;&quot; + out[2] + missing_value_handling + &#39; else &#39; + out[6] + &#39;) n&#39; ) else: return (&#39; if state == &#39; + out[0] + &#39;: n&#39; + &#39; state = (&#39; + out[4] + &#39; if &#39; + &quot;x[&#39;&quot; + out[1] +&quot;&#39;]&lt;&quot; + out[2] + missing_value_handling + &#39; else &#39; + out[6] + &#39;) n&#39; ) else: out = re.findall(r&quot;[ d.-]+&quot;, s) return (re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; if state == &#39; + out[0] + &#39;: n &#39; + re.findall(r&quot;[ t]+&quot;, s)[0].replace(&#39; t&#39;, &#39; &#39;) + &#39; return &#39; + out[1] + &#39; n&#39;) def tree_parser(tree, i): if i == 0: return (&#39; if num_booster == 0: n state = 0 n&#39; + &quot;&quot;.join([string_parser(tree.split(&#39; n&#39;)[i]) for i in range(len(tree.split(&#39; n&#39;))-1)])) else: return (&#39; elif num_booster == &#39;+str(i)+&#39;: n state = 0 n&#39; + &quot;&quot;.join([string_parser(tree.split(&#39; n&#39;)[i]) for i in range(len(tree.split(&#39; n&#39;))-1)])) def model_to_py(base_score, model, out_file): trees = model.get_dump() result = [&quot;import numpy as np n n&quot; +&quot;def xgb_tree(x, num_booster): n&quot;] for i in range(len(trees)): result.append(tree_parser(trees[i], i)) with open(out_file, &#39;w&#39;) as the_file: the_file.write(&quot;&quot;.join(result) + &quot; ndef xgb_predict(x): n predict = &quot; + str(base_score) + &quot; n&quot; + &quot;# initialize prediction with base score n&quot; + &quot; for i in range(&quot; + str(len(trees)) + &quot;): n predict = predict + xgb_tree(x, i)&quot; + &quot; n return predict&quot;) model_to_py(params[&#39;base_score&#39;], model, &#39;xgb_model.py&#39;) . Prediction using dump file . import xgb_model passenger_data_1 = {&#39;pclass&#39;:3, &#39;age&#39;:np.nan, &#39;sibsp&#39;:0, &#39;parch&#39;:0, &#39;fare&#39;:7.8958} passenger_data_2 = {&#39;pclass&#39;:1, &#39;age&#39;:46, &#39;sibsp&#39;:0, &#39;parch&#39;:0, &#39;fare&#39;:26} print(xgb_model.xgb_predict(passenger_data_1)) print(xgb_model.xgb_predict(passenger_data_2)) . 0.34144773395253103 0.43616785725253104 .",
            "url": "https://joaorafaelm.github.io/notebook/xgboost/2021/06/06/_deploying_xgboost_model.html",
            "relUrl": "/xgboost/2021/06/06/_deploying_xgboost_model.html",
            "date": " 2021 Jun 06"
        }
        
    
  
    
        ,"post2": {
            "title": "Incremental training with XGBoost",
            "content": "Install dependencies . pip install scikit-learn xgboost . Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1) Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1) Requirement already satisfied: numpy&gt;=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5) Requirement already satisfied: scipy&gt;=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1) . . Save your model after you train on the first batch. Then, on successive runs, provide the xgb.train method with the filepath of the saved model. . First, split the boston dataset into training and testing sets. Then split the training set into halves. Fit a model with the first half and get a score that will serve as a benchmark. Then fit two models with the second half; one model will have the additional parameter xgb_model. If passing in the extra parameter didn&#39;t make a difference, then we would expect their scores to be similar.. But, fortunately, the new model seems to perform much better than the first. . import xgboost as xgb from sklearn.model_selection import train_test_split from sklearn.datasets import load_boston from sklearn.metrics import mean_squared_error X = load_boston()[&#39;data&#39;] y = load_boston()[&#39;target&#39;] # split data into training and testing sets # then split training set in half X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0) X_train_1, X_train_2, y_train_1, y_train_2 = train_test_split( X_train, y_train, test_size=0.5, random_state=0 ) xg_train_1 = xgb.DMatrix(X_train_1, label=y_train_1) xg_train_2 = xgb.DMatrix(X_train_2, label=y_train_2) xg_test = xgb.DMatrix(X_test, label=y_test) params = {&#39;objective&#39;: &#39;reg:squarederror&#39;, &#39;verbose&#39;: False} model_1 = xgb.train(params, xg_train_1, 30) model_1.save_model(&#39;model_1.model&#39;) # ================= train two versions of the model =====================# model_2_v1 = xgb.train(params, xg_train_2, 30) model_2_v2 = xgb.train(params, xg_train_2, 30, xgb_model=&#39;model_1.model&#39;) print(mean_squared_error(model_1.predict(xg_test), y_test)) # benchmark print(mean_squared_error(model_2_v1.predict(xg_test), y_test)) # &quot;before&quot; print(mean_squared_error(model_2_v2.predict(xg_test), y_test)) # &quot;after&quot; . 21.988532050893138 39.677688213388755 23.092057209292484 .",
            "url": "https://joaorafaelm.github.io/notebook/xgboost/2021/05/31/incremental-training-xgboost.html",
            "relUrl": "/xgboost/2021/05/31/incremental-training-xgboost.html",
            "date": " 2021 May 31"
        }
        
    
  
    
        ,"post3": {
            "title": "Multilanguage topic modeling with BERT",
            "content": "!pip install contextualized_topic_models !pip uninstall transformers -y !pip install transformers==3.0.2 . Requirement already satisfied: contextualized_topic_models in /usr/local/lib/python3.6/dist-packages (1.4.2) Requirement already satisfied: torchvision==0.7.0 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (0.7.0+cu101) Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (3.8.3) Requirement already satisfied: wheel==0.33.6 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (0.33.6) Requirement already satisfied: pytest-runner==5.1 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (5.1) Requirement already satisfied: pytest==4.6.5 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (4.6.5) Requirement already satisfied: numpy==1.19.1 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (1.19.1) Requirement already satisfied: sentence-transformers==0.3.2 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (0.3.2) Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (1.6.0) Requirement already satisfied: pillow&gt;=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7.0-&gt;contextualized_topic_models) (7.0.0) Requirement already satisfied: six&gt;=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3-&gt;contextualized_topic_models) (1.15.0) Requirement already satisfied: scipy&gt;=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3-&gt;contextualized_topic_models) (1.4.1) Requirement already satisfied: smart-open&gt;=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3-&gt;contextualized_topic_models) (2.1.0) Requirement already satisfied: importlib-metadata&gt;=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (1.7.0) Requirement already satisfied: more-itertools&gt;=4.0.0; python_version &gt; &#34;2.7&#34; in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (8.4.0) Requirement already satisfied: atomicwrites&gt;=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (1.4.0) Requirement already satisfied: py&gt;=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (1.9.0) Requirement already satisfied: pluggy&lt;1.0,&gt;=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (0.13.1) Requirement already satisfied: attrs&gt;=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (20.1.0) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (20.4) Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5-&gt;contextualized_topic_models) (0.2.5) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.22.2.post1) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.2-&gt;contextualized_topic_models) (4.41.1) Requirement already satisfied: transformers&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.2-&gt;contextualized_topic_models) (3.1.0) Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.2-&gt;contextualized_topic_models) (3.2.5) Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0-&gt;contextualized_topic_models) (0.16.0) Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (2.49.0) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (2.23.0) Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (1.14.48) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata&gt;=0.12-&gt;pytest==4.6.5-&gt;contextualized_topic_models) (3.1.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;pytest==4.6.5-&gt;contextualized_topic_models) (2.4.7) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.16.0) Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.0.43) Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.8.1rc2) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (3.0.12) Requirement already satisfied: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.7) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (2019.12.20) Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (0.1.91) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (2020.6.20) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (3.0.4) Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (0.10.0) Requirement already satisfied: s3transfer&lt;0.4.0,&gt;=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (0.3.3) Requirement already satisfied: botocore&lt;1.18.0,&gt;=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (1.17.48) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers&gt;=3.0.2-&gt;sentence-transformers==0.3.2-&gt;contextualized_topic_models) (7.1.2) Requirement already satisfied: docutils&lt;0.16,&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore&lt;1.18.0,&gt;=1.17.48-&gt;boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (0.15.2) Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore&lt;1.18.0,&gt;=1.17.48-&gt;boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim==3.8.3-&gt;contextualized_topic_models) (2.8.1) Uninstalling transformers-3.1.0: Successfully uninstalled transformers-3.1.0 Collecting transformers==3.0.2 Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB) |████████████████████████████████| 778kB 3.4MB/s Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.0.43) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12) Requirement already satisfied: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.7) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.4) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.19.1) Collecting tokenizers==0.8.1.rc1 Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB) |████████████████████████████████| 3.0MB 17.9MB/s Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.1.91) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==3.0.2) (0.16.0) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==3.0.2) (7.1.2) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==3.0.2) (1.15.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;transformers==3.0.2) (2.4.7) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.0.2) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.0.2) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.0.2) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.0.2) (2020.6.20) Installing collected packages: tokenizers, transformers Found existing installation: tokenizers 0.8.1rc2 Uninstalling tokenizers-0.8.1rc2: Successfully uninstalled tokenizers-0.8.1rc2 Successfully installed tokenizers-0.8.1rc1 transformers-3.0.2 . . import os import numpy as np import pickle from contextualized_topic_models.models.ctm import CTM from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file, bert_embeddings_from_list from contextualized_topic_models.datasets.dataset import CTMDataset from contextualized_topic_models.utils.data_preparation import TextHandler . !curl -s https://raw.githubusercontent.com/MilaNLProc/contextualized-topic-models/master/contextualized_topic_models/data/gnews/GoogleNews.txt | head -n1000 &gt; googlenews.txt !head googlenews.txt !cat googlenews.txt | wc -l . centrepoint winter white gala london mourinho seek killer instinct roundup golden globe won seduced johansson voice travel disruption mount storm cold air sweep south florida wes welker blame costly turnover psalm book fetch record ny auction ktvn channel reno surface review comparison window powered tablet pitted scientist unreported fish trap space nokia lumia launch edward snowden latest leak nsa monitored online porn habit radicalizers 1000 . Load The Data . file_name = &quot;googlenews.txt&quot; handler = TextHandler(file_name) handler.prepare() # create vocabulary and training data . train_bert = bert_embeddings_from_file(file_name, &quot;distiluse-base-multilingual-cased&quot;) training_dataset = CTMDataset(handler.bow, train_bert, handler.idx2token) . Train the Fully Contextualized Topic Model . num_topics = 50 ctm = CTM(input_size=len(handler.vocab), bert_input_size=512, num_epochs=100, hidden_sizes = (100, ), inference_type=&quot;contextual&quot;, n_components=num_topics, num_data_loader_workers=0) ctm.fit(training_dataset) # run the model . ctm.get_topic_lists(5) # get the top-5 words lists . [[&#39;kim&#39;, &#39;west&#39;, &#39;kanye&#39;, &#39;kardashian&#39;, &#39;bound&#39;], [&#39;day&#39;, &#39;thanksgiving&#39;, &#39;parade&#39;, &#39;macy&#39;, &#39;packer&#39;], [&#39;patriot&#39;, &#39;bronco&#39;, &#39;pat&#39;, &#39;packer&#39;, &#39;loss&#39;], [&#39;xbox&#39;, &#39;microsoft&#39;, &#39;p&#39;, &#39;game&#39;, &#39;console&#39;], [&#39;government&#39;, &#39;political&#39;, &#39;thai&#39;, &#39;party&#39;, &#39;protest&#39;], [&#39;oldboy&#39;, &#39;brolin&#39;, &#39;josh&#39;, &#39;lee&#39;, &#39;spike&#39;], [&#39;google&#39;, &#39;chrome&#39;, &#39;search&#39;, &#39;extension&#39;, &#39;voice&#39;], [&#39;johansson&#39;, &#39;globe&#39;, &#39;golden&#39;, &#39;scarlett&#39;, &#39;ineligible&#39;], [&#39;star&#39;, &#39;dancing&#39;, &#39;amber&#39;, &#39;riley&#39;, &#39;win&#39;], [&#39;police&#39;, &#39;guilty&#39;, &#39;watkins&#39;, &#39;case&#39;, &#39;lostprophets&#39;], [&#39;san&#39;, &#39;andreas&#39;, &#39;gta&#39;, &#39;mobile&#39;, &#39;android&#39;], [&#39;flat&#39;, &#39;future&#39;, &#39;record&#39;, &#39;level&#39;, &#39;p&#39;], [&#39;thanksgiving&#39;, &#39;day&#39;, &#39;parade&#39;, &#39;thanksgivukkah&#39;, &#39;holiday&#39;], [&#39;jos&#39;, &#39;wearhouse&#39;, &#39;men&#39;, &#39;bank&#39;, &#39;baldwin&#39;], [&#39;prince&#39;, &#39;william&#39;, &#39;swift&#39;, &#39;jovi&#39;, &#39;bon&#39;], [&#39;porn&#39;, &#39;nsa&#39;, &#39;habit&#39;, &#39;radicalizers&#39;, &#39;spying&#39;], [&#39;pope&#39;, &#39;church&#39;, &#39;putin&#39;, &#39;issue&#39;, &#39;coalition&#39;], [&#39;report&#39;, &#39;benghazi&#39;, &#39;security&#39;, &#39;baldwin&#39;, &#39;alec&#39;], [&#39;china&#39;, &#39;zone&#39;, &#39;flight&#39;, &#39;airspace&#39;, &#39;disputed&#39;], [&#39;storm&#39;, &#39;parade&#39;, &#39;macy&#39;, &#39;balloon&#39;, &#39;travel&#39;], [&#39;bank&#39;, &#39;men&#39;, &#39;palestinian&#39;, &#39;jos&#39;, &#39;wearhouse&#39;], [&#39;review&#39;, &#39;homefront&#39;, &#39;frozen&#39;, &#39;inch&#39;, &#39;oldboy&#39;], [&#39;bronco&#39;, &#39;packer&#39;, &#39;seahawks&#39;, &#39;rodgers&#39;, &#39;patriot&#39;], [&#39;frozen&#39;, &#39;heart&#39;, &#39;review&#39;, &#39;homefront&#39;, &#39;detroit&#39;], [&#39;hiv&#39;, &#39;meningitis&#39;, &#39;flu&#39;, &#39;greece&#39;, &#39;health&#39;], [&#39;black&#39;, &#39;friday&#39;, &#39;nativity&#39;, &#39;deal&#39;, &#39;monday&#39;], [&#39;aarushi&#39;, &#39;hiv&#39;, &#39;killing&#39;, &#39;teen&#39;, &#39;murder&#39;], [&#39;west&#39;, &#39;kanye&#39;, &#39;kim&#39;, &#39;seth&#39;, &#39;bound&#39;], [&#39;cb&#39;, &#39;seahawks&#39;, &#39;dallas&#39;, &#39;chelsea&#39;, &#39;browner&#39;], [&#39;hp&#39;, &#39;revenue&#39;, &#39;raise&#39;, &#39;week&#39;, &#39;shopping&#39;], [&#39;lumia&#39;, &#39;nokia&#39;, &#39;price&#39;, &#39;power&#39;, &#39;uk&#39;], [&#39;typhoon&#39;, &#39;philippine&#39;, &#39;haiyan&#39;, &#39;climate&#39;, &#39;gain&#39;], [&#39;african&#39;, &#39;france&#39;, &#39;central&#39;, &#39;republic&#39;, &#39;troop&#39;], [&#39;parade&#39;, &#39;macy&#39;, &#39;carlos&#39;, &#39;beltran&#39;, &#39;york&#39;], [&#39;kim&#39;, &#39;kardashian&#39;, &#39;video&#39;, &#39;west&#39;, &#39;bound&#39;], [&#39;hewitt&#39;, &#39;love&#39;, &#39;star&#39;, &#39;jennifer&#39;, &#39;dancing&#39;], [&#39;swift&#39;, &#39;william&#39;, &#39;taylor&#39;, &#39;prince&#39;, &#39;jovi&#39;], [&#39;launch&#39;, &#39;microsoft&#39;, &#39;chrome&#39;, &#39;google&#39;, &#39;search&#39;], [&#39;pakistan&#39;, &#39;army&#39;, &#39;chief&#39;, &#39;sharif&#39;, &#39;pm&#39;], [&#39;air&#39;, &#39;china&#39;, &#39;zone&#39;, &#39;sea&#39;, &#39;disputed&#39;], [&#39;west&#39;, &#39;kanye&#39;, &#39;bound&#39;, &#39;kim&#39;, &#39;video&#39;], [&#39;ison&#39;, &#39;comet&#39;, &#39;raptor&#39;, &#39;sun&#39;, &#39;bonobo&#39;], [&#39;irs&#39;, &#39;google&#39;, &#39;tax&#39;, &#39;group&#39;, &#39;glass&#39;], [&#39;net&#39;, &#39;review&#39;, &#39;preview&#39;, &#39;disney&#39;, &#39;movie&#39;], [&#39;nokia&#39;, &#39;lumia&#39;, &#39;tablet&#39;, &#39;window&#39;, &#39;moto&#39;], [&#39;three&#39;, &#39;seahawks&#39;, &#39;year&#39;, &#39;burning&#39;, &#39;officer&#39;], [&#39;report&#39;, &#39;burning&#39;, &#39;officer&#39;, &#39;storm&#39;, &#39;truck&#39;], [&#39;girl&#39;, &#39;baby&#39;, &#39;guilty&#39;, &#39;lostprophets&#39;, &#39;hewitt&#39;], [&#39;black&#39;, &#39;friday&#39;, &#39;sale&#39;, &#39;deal&#39;, &#39;monday&#39;], [&#39;heart&#39;, &#39;woman&#39;, &#39;pill&#39;, &#39;frozen&#39;, &#39;crisis&#39;]] . !tail -n 5 googlenews.txt &gt; test.txt !cat test.txt . ray whitney return will dallas star huge boost offensively s relied intermediary probe spacex sept upper stage nokia lumia tablet kill surface lakers net preview neighbor helped save girl imprisoned year speaks . test_handler = TextHandler(&quot;test.txt&quot;) test_handler.prepare() # create vocabulary and training data # generate BERT data testing_bert = bert_embeddings_from_file(&quot;test.txt&quot;, &quot;distiluse-base-multilingual-cased&quot;) testing_dataset = CTMDataset(test_handler.bow, testing_bert, test_handler.idx2token) . # we sample n times and average to get a more accurate estimate of the document-topic distribution predicted_topics = [] thetas = np.zeros((len(testing_dataset), num_topics)) for a in range(0, 100): thetas = thetas + np.array(ctm.get_thetas(testing_dataset)) for idd in range(0, len(testing_dataset)): thetas[idd] = thetas[idd]/np.sum(thetas[idd]) predicted_topic = np.argmax(thetas[idd]) predicted_topics.append(predicted_topic) # document-topic distribution , list of the topic predicted for each testing document # thetas, predicted_topics . [22, 41, 44, 23, 47] . test_handler.load_text_file()[1] . &#39;s relied intermediary probe spacex sept upper stage n&#39; . ctm.get_topic_lists(20)[41] . [&#39;ison&#39;, &#39;comet&#39;, &#39;raptor&#39;, &#39;sun&#39;, &#39;bonobo&#39;, &#39;dna&#39;, &#39;flying&#39;, &#39;trouble&#39;, &#39;stereo&#39;, &#39;seahorse&#39;, &#39;researcher&#39;, &#39;preview&#39;, &#39;spacecraft&#39;, &#39;century&#39;, &#39;jellyfish&#39;, &#39;testing&#39;, &#39;minute&#39;, &#39;net&#39;, &#39;spectacular&#39;, &#39;congo&#39;] .",
            "url": "https://joaorafaelm.github.io/notebook/bert/topics/nlp/2021/04/17/multilang-topic-model.html",
            "relUrl": "/bert/topics/nlp/2021/04/17/multilang-topic-model.html",
            "date": " 2021 Apr 17"
        }
        
    
  
    
        ,"post4": {
            "title": "Cognitive complexity and python",
            "content": "Install dependencies . pip install cognitive_complexity astunparse tabulate . Requirement already satisfied: cognitive_complexity in /usr/local/lib/python3.7/dist-packages (1.2.0) Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (1.6.3) Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (0.8.9) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from cognitive_complexity) (57.0.0) Requirement already satisfied: six&lt;2.0,&gt;=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse) (1.15.0) Requirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse) (0.36.2) . . Import and define utils . import ast import astunparse from inspect import getsource from tabulate import tabulate from cognitive_complexity.api import get_cognitive_complexity_for_node from cognitive_complexity.utils.ast import has_recursive_calls, is_decorator, process_child_nodes, process_node_itself def get_cognitive_complexity(func): func = func if isinstance(func, str) else getsource(func) funcdef = ast.parse(func).body[0] if is_decorator(funcdef): return get_cognitive_complexity(funcdef.body[0]) details = [] complexity = 0 for node in funcdef.body: node_complexity = get_cognitive_complexity_for_node(node) complexity += node_complexity node_code = astunparse.unparse(node) if f&quot;{funcdef.name}(&quot; in node_code: # +1 for recursion node_complexity += 1 complexity += 1 details.append([node_complexity, node_code]) details.append([complexity, &quot;Total&quot;]) return complexity, details . Introduction . Formulated in a Fortran environment in 1976, Cyclomatic Complexity has long been the standard for measuring the complexity of a method’s control flow. It was originally intended “to identify software modules that will be difficult to test or maintain”, but while it accurately calculates the minimum number of test cases required to fully cover a method, it is not a satisfactory measure of understandability and it also doesn’t include modern language structures like try/catch, and lambdas. . -- Cognitive Complexity:A new way of measuring understandability, white paper by G. Ann Campbell . Basic criteria and methodology . As a remedy for these problems, Cognitive Complexity has been formulated to address modern language structures, and to produce values that are meaningful at the class and application levels. A Cognitive Complexity score is assessed according to three basic rules: . Ignore structures that allow multiple statements to be readably shorthanded into one | Increment (add one) for each break in the linear flow of the code | Increment when flow-breaking structures are nested Additionally, a complexity score is made up of four different types of increments: . A. Nesting - assessed for nesting control flow structures inside each other . B. Structural - assessed on control flow structures that are subject to a nesting increment, and that increase the nesting count . C. Fundamental - assessed on statements not subject to a nesting increment . D. Hybrid - assessed on control flow structures that are not subject to a nesting increment, but which do increase the nesting count . | -- Cognitive Complexity: A new way of measuring understandability, white paper by G. Ann Campbell . def f(n): if n &gt; 10: return True if n &lt; 5: return 20 else: return 2 return f(n) total, details = get_cognitive_complexity(f) print(tabulate(details, headers=[&quot;Complexity&quot;, &quot;Node&quot;], tablefmt=&quot;fancy_grid&quot;)) . ╒══════════════╤═════════════════╕ │ Complexity │ Node │ ╞══════════════╪═════════════════╡ │ 1 │ if (n &gt; 10): │ │ │ return True │ ├──────────────┼─────────────────┤ │ 2 │ if (n &lt; 5): │ │ │ return 20 │ │ │ else: │ │ │ return 2 │ ├──────────────┼─────────────────┤ │ 1 │ return f(n) │ ├──────────────┼─────────────────┤ │ 4 │ Total │ ╘══════════════╧═════════════════╛ . References . Cognitive Complexity, Because Testability != Understandability . | Cognitive Complexity: A new way of measuring understandability, white paper by G. Ann Campbell . | Cognitive Complexity: the New Guide to Refactoring for Maintainable Code . | Cognitive Complexity from CodeClimate docs . | Is Your Code Readable By Humans? Cognitive Complexity Tells You . | .",
            "url": "https://joaorafaelm.github.io/notebook/python/2021/04/10/cognitive-complexity.html",
            "relUrl": "/python/2021/04/10/cognitive-complexity.html",
            "date": " 2021 Apr 10"
        }
        
    
  
    
        ,"post5": {
            "title": "Training GPT2 with Colab and Google Drive",
            "content": "We&#39;ll be using aitextgen to finetune the model. . pip install aitextgen . Requirement already satisfied: aitextgen in /usr/local/lib/python3.7/dist-packages (0.5.2) Requirement already satisfied: pytorch-lightning&gt;=1.3.1 in /usr/local/lib/python3.7/dist-packages (from aitextgen) (1.3.4) Requirement already satisfied: fire&gt;=0.3.0 in /usr/local/lib/python3.7/dist-packages (from aitextgen) (0.4.0) Requirement already satisfied: transformers&gt;=4.5.1 in /usr/local/lib/python3.7/dist-packages (from aitextgen) (4.6.1) Requirement already satisfied: torch&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from aitextgen) (1.8.1+cu101) Requirement already satisfied: PyYAML&lt;=5.4.1,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (5.4.1) Requirement already satisfied: future&gt;=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.18.2) Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.3.0) Requirement already satisfied: fsspec[http]&gt;=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (2021.5.0) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (20.9) Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (2.4.1) Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (4.41.1) Requirement already satisfied: torchmetrics&gt;=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.3.2) Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.19.5) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire&gt;=0.3.0-&gt;aitextgen) (1.15.0) Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire&gt;=0.3.0-&gt;aitextgen) (1.1.0) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (3.0.12) Requirement already satisfied: tokenizers&lt;0.11,&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (0.10.3) Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (0.0.8) Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (0.0.45) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (2019.12.20) Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (4.0.1) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.5.1-&gt;aitextgen) (2.23.0) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch&gt;=1.6.0-&gt;aitextgen) (3.7.4.3) Requirement already satisfied: aiohttp; extra == &#34;http&#34; in /usr/local/lib/python3.7/dist-packages (from fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (3.7.4.post0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (2.4.7) Requirement already satisfied: google-auth&lt;2,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.30.0) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.34.1) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (3.12.4) Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.0.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (57.0.0) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.8.0) Requirement already satisfied: wheel&gt;=0.26; python_version &gt;= &#34;3&#34; in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.36.2) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.4.4) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (3.3.4) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.12.0) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (1.0.1) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (3.4.1) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (2020.12.5) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&gt;=4.5.1-&gt;aitextgen) (1.24.3) Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == &#34;http&#34;-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (5.1.0) Requirement already satisfied: async-timeout&lt;4.0,&gt;=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == &#34;http&#34;-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (3.0.1) Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == &#34;http&#34;-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.6.3) Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == &#34;http&#34;-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (21.2.0) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.2.8) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4; python_version &gt;= &#34;3.6&#34; in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (4.7.2) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (4.2.2) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (1.3.0) Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning&gt;=1.3.1-&gt;aitextgen) (3.1.0) . . Import modules and mount google drive . from aitextgen import aitextgen from aitextgen.colab import mount_gdrive, copy_file_from_gdrive from aitextgen.TokenDataset import TokenDataset, merge_datasets from aitextgen.utils import build_gpt2_config from aitextgen.tokenizers import train_tokenizer mount_gdrive() . !curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt &gt; input.txt !head input.txt . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1089k 100 1089k 0 0 9002k 0 --:--:-- --:--:-- --:--:-- 9002k First Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: You are all resolved rather to die than to famish? All: . Train tokenizer . file_name = &quot;input.txt&quot; project_name = &quot;project_name&quot; # copy_file_from_gdrive(file_name) train_tokenizer(file_name); . INFO:aitextgen.tokenizers:Saving aitextgen-vocab.json and aitextgen-merges.txt to the current directory. You will need both files to build the GPT2Tokenizer. . Training the model should take about 30 minutes . model = None config = None for _ in [&quot;pytorch_model.bin&quot;, &quot;config.json&quot;, &quot;aitextgen_vocab.json&quot;, &quot;aitextgen_merges.json&quot;]: try: copy_file_from_gdrive(_, project_name) model = &quot;pytorch_model.bin&quot; config = &quot;config.json&quot; except FileNotFoundError: pass config = config or build_gpt2_config( vocab_size=5000, max_length=200, dropout=0.0, n_embd=256, n_layer=8, n_head=8 ) ai = aitextgen( vocab_file=&quot;aitextgen-vocab.json&quot;, merges_file=&quot;aitextgen-merges.txt&quot;, config=config, model=model, to_gpu=True ) . INFO:aitextgen:Constructing GPT-2 model from provided config. INFO:aitextgen:Using a custom tokenizer. . ai.train( file_name, line_by_line=False, num_steps=10000, generate_every=1000, save_every=500, learning_rate=1e-4, batch_size=128, save_gdrive=True, run_id=project_name ) . INFO:aitextgen.TokenDataset:Encoding 40,000 sets of tokens from input.txt. GPU available: True, used: True INFO:lightning:GPU available: True, used: True TPU available: False, using: 0 TPU cores INFO:lightning:TPU available: False, using: 0 TPU cores CUDA_VISIBLE_DEVICES: [0] INFO:lightning:CUDA_VISIBLE_DEVICES: [0] . . Generating examples . ai.generate( n=5, batch_size=5, prompt=&quot;Speak:&quot;, temperature=1.0, top_p=0.9, ) .",
            "url": "https://joaorafaelm.github.io/notebook/gpt2/colab/drive/2020/06/02/gpt2-google-drive-sync.html",
            "relUrl": "/gpt2/colab/drive/2020/06/02/gpt2-google-drive-sync.html",
            "date": " 2020 Jun 02"
        }
        
    
  
    
        ,"post6": {
            "title": "Text Classification with Python",
            "content": "If you are already familiar with what text classification is, you might want to jump to this part, or get the code here. . What is Text Classification? . Document or text classification is used to classify information, that is, assign a category to a text; it can be a document, a tweet, a simple message, an email, and so on. In this article, I will show how you can classify retail products into categories. Although in this example the categories are structured in a hierarchy, to keep it simple I will consider all subcategories as top-level. . If you are looking for complex implementations of large scale hierarchical text classification, I will leave links to some really good papers and projects at the end of this post. . Getting started . Now, before you go any further, make sure you have installed Python3+ and virtualenv (optional, but I highly recommend you to use it). . Let’s break down the problem into steps: . Setting up the environment | Gathering the data | Extracting features from the dataset | Testing the algorithms | . Setting up the environment . The main packages used in this projects are: sklearn, nltk and dataset. Due to the size of the data-set, it might take some time to clone/download the repository; NLTK data is also considerably big. Run the following commands to setup the project structure and download the required packages: . # Clone the repo git clone https://github.com/joaorafaelm/text-classification-python; cd text-classification-python; # Create virtualenv; skip this one if you dont have virtualenv. virtualenv venv &amp;&amp; source venv/bin/activate; # Install all requirements pip install -r requirements.txt; # Download all data that NLTK uses python -m nltk.downloader all; . Gathering the data . The dataset that will be used was created by scraping some products from Amazon. Scraping might be fine for projects where only a small amount of data is required, but it can be a really slow process since it is very simple for a server to detect a robot, unless you are rotating over a list of proxies, which can slow the process even more. . Using this script, I downloaded information of over 22,000 products, organized into 42 top-level categories, and a total of 6233 subcategories. See the whole category tree structure here. . Again, to keep it simple I will be using only 3 top-level categories: Automotive, Home &amp; Kitchen and Industrial &amp; Scientific. Including the subcategories, there are 36 categories in total. . To extract the data from database, run the command: . # dump from db to dumps/all_products.json datafreeze .datafreeze.yaml; . Inside the project you will also find a file called data_prep.py, in this file you can set the categories you want to use, the minimum amount of samples per category and the depth of a category. As I said before, only 3 categories are going to be used: Home &amp; Kitchen, Industrial &amp; Scientific and Automotive. I did not specify the depth of the subcategories, but I did specify 50 as the minimum amount of samples (is this case, products) per category. To transform the data dumped from the database into this “filtered” data, just execute the file: . python data_prep.py . The script will create a new file called products.json at the root of the project, and print out the category tree structure. Change the value of the variables default_depth, min_samples and domain if you need more data. . Extracting features from the dataset . In order to run machine learning algorithms, we need to transform the text into numerical vectors. Bag-of-words is one of the most used models, it assigns a numerical value to a word, creating a list of numbers. It can also assign a value to a set of words, known as N-gram. . Scikit provides a vectorizer called TfidfVectorizer which transforms the text based on the bag-of-words/n-gram model, additionally, it computes term frequencies and evaluate each word using the tf-idf weighting scheme. . Counting terms frequencies might not be enough sometimes. Take the words ‘cars’ and ‘car’ for example, by only using tf-idf, they are considered different words. This problem can be solved using Stemming and/or Lemmatisation. And there is where NLTK comes into play. . NLTK offers some pretty useful tools for NLP. For this project I used it to perform Lemmatisation and Part-of-speech tagging. . With Lemmatisation we can group together the inflected forms of a word. For example, the words ‘walked’, ‘walks’ and ‘walking’, can be grouped into their base form, the verb ‘walk’. That is why we need to POS tag each word as a noun, verb, adverb, and so on. . It is also worth noting that some words despite the fact that they appear frequently, they do not really make any difference for classification, in fact they could even help misclassify a text. Words like ‘a’, ‘an’, ‘the’, ‘to’, ‘or’ etc, are known as stop-words. These words can be ignored during the tokenization process. . Testing the algorithms . Now that we have all the features and labels, it is time to train the classifiers. There are a number of algorithms you can use for this type of problem, for example: Multinomial Naive Bayes, Linear SVC, SGD Classifier, K-Neighbors Classifier, Random Forest Classifier. Inside the file classify.py you can find an example using the SGDClassifier. Run it yourself using the command: . python classify.py . It will print out the accuracy of each category, along with the confusion matrix. . Here is how it is implemented: load the dataset, initiate WordNetLemmatizer and PerceptronTagger from NLTK. As I was only interested in nouns, verbs, adverbs and adjectives, I created a lookup dict to quicken up the process. Although NLTK is great, its aim is not performance, so I also implemented python’s LRU Cache for both lemmatize and tagger functions. . # Load data dataset = json.load(open(&#39;products.json&#39;, encoding=&#39;utf-8&#39;)) # Initiate lemmatizer wnl = WordNetLemmatizer() # Load tagger pickle tagger = PerceptronTagger() # Lookup if tag is noun, verb, adverb or an adjective tags = {&#39;N&#39;: wn.NOUN, &#39;V&#39;: wn.VERB, &#39;R&#39;: wn.ADV, &#39;J&#39;: wn.ADJ} # Memoization of POS tagging and Lemmatizer lemmatize_mem = lru_cache(maxsize=10000)(wnl.lemmatize) tagger_mem = lru_cache(maxsize=10000)(tagger.tag) . Next, the tokenizer function was created. It breaks the text into words and iterate over them, ignoring the stop-words and POS-tagging/Lemmatising the rest. This function will receive all documents from the dataset. . # POS tag sentences and lemmatize each word def tokenizer(text): for token in wordpunct_tokenize(text): if token not in ENGLISH_STOP_WORDS: tag = tagger_mem(frozenset({token})) yield lemmatize_mem(token, tags.get(tag[0][1], wn.NOUN)) . At last the pipeline is defined; the first step is to call TfidfVectorizer, with the tokenizer function preprocessing each document, and then pass through the SGDClassifier. The classifier is trained and tested using 10-fold Cross-Validation provided by the cross_val_predict method from scikit-learn. . # Pipeline definition pipeline = Pipeline([ (&#39;vectorizer&#39;, TfidfVectorizer( tokenizer=tokenizer, ngram_range=(1, 2), stop_words=ENGLISH_STOP_WORDS, sublinear_tf=True, min_df=0.00009 )), (&#39;classifier&#39;, SGDClassifier( alpha=1e-4, n_jobs=-1 )), ]) # Cross validate using k-fold y_pred = cross_val_predict( pipeline, dataset.get(&#39;data&#39;), y=dataset.get(&#39;target&#39;), cv=10, n_jobs=-1, verbose=20 ) # Print out precison, recall and f1 scode. print(classification_report( dataset.get(&#39;target&#39;), y_pred, target_names=dataset.get(&#39;target_names&#39;), digits=3 )) . And here are the accuracy results for each algorithm I tested (all algorithms were tested with their default parameters): . Algorithms Precision Recall . SGDClassifier | 0.975 | 0.975 | . LinearSVC | 0.972 | 0.971 | . RandomForest | 0.938 | 0.936 | . MultinomialNB | 0.882 | 0.851 | . The precision is the percentage of the test samples that were classified to the category and actually belonged to the category. . The recall is the percentage of all the test samples that originally belonged to the category and in the evaluation process were correctly classified to the category. . Conclusion . As the category tree gets bigger, and you have more and more data to classify, you cannot use a model as simple as the one above (well, you can but its precision will be very low, not to mention the computational cost). Another important thing to notice, is how you structure the categories, in amazon category structure, a lot of subcategories are so confused that I doubt even humans could correctly classify products to them. The full code of this post can be found here. . If you noticed something wrong, or you know something that can make the algorithms better, please do comment bellow. Thanks for reading! . Further reading . Classifier Statistics . | A Meta-Top-Down Method for Large-Scale Hierarchical Classification . | A survey of hierarchical classification across different application domains . | Hierarchical Text Categorization and Its Application to Bioinformatics . | Comparing Several Approaches for Hierarchical Classification of Proteins with Decision Trees . | Tokenizing Words and Sentences with NLTK . | Natural Language Processing with Deep Learning . | Document Classification using Multinomial Naive Bayes Classifier . | .",
            "url": "https://joaorafaelm.github.io/notebook/nlp/2017/08/24/text-classification-with-python.html",
            "relUrl": "/nlp/2017/08/24/text-classification-with-python.html",
            "date": " 2017 Aug 24"
        }
        
    
  
    
        ,"post7": {
            "title": "GraphQL and Django in 5 minutes",
            "content": "TL;DR Jump to the coding part or get the code here. . What is GraphQL? . GraphQL query is a string that is sent to a server to be interpreted and fulfilled, which then returns JSON back to the client. It was created by Facebook in 2012 and the first specification draft was made public in 2015. . In this tutorial I will cover the basics of working with GraphQL and Django. . Getting started . Before creating the project and all, make sure you have virtualenv installed, so that the packages used in this tutorial won’t be installed system-wide. . # Clone the repo git clone https://github.com/joaorafaelm/graphql-django-example; cd graphql-django-example; # Create virtualenv virtualenv venv &amp;&amp; source venv/bin/activate; # Install django and graphene pip install -r requirements.txt; # Setup db python manage.py migrate; . Run python manage.py loaddata books.json to populate the db, or run python manage.py createsuperuser and then add some data using the admin interface. . Models and GraphQL Schema . This example is going to use the models Author, Book and Publisher. . # bookstore/store/models.py from django.db import models class Publisher(models.Model): name = models.CharField(max_length=30) website = models.URLField() def __str__(self): return self.name class Author(models.Model): first_name = models.CharField(max_length=30) last_name = models.CharField(max_length=40) email = models.EmailField() def __str__(self): return &#39;%s %s&#39; % (self.first_name, self.last_name) class Book(models.Model): title = models.CharField(max_length=100) authors = models.ManyToManyField(Author) publisher = models.ForeignKey(Publisher) publication_date = models.DateField() def __str__(self): return self.title . After creating the models, the Schema should be created, which will be used to serve the API. . At the time of writing this post, graphene-django version is 1.3 and it does not handle ManyToMany fields properly, that is why the resolve_authors method was added. This issue has been resolved for the next release. . # bookstore/schema.py import graphene from graphene_django.types import DjangoObjectType from graphene_django.debug import DjangoDebug from bookstore.store.models import Author, Book, Publisher class AuthorType(DjangoObjectType): class Meta: model = Author class BookType(DjangoObjectType): authors = graphene.List(AuthorType) # Many To Many fix until next release. # https://github.com/graphql-python/graphene-django/issues/155 @graphene.resolve_only_args def resolve_authors(self): return self.authors.all() class Meta: model = Book class PublisherType(DjangoObjectType): class Meta: model = Publisher class Query(graphene.ObjectType): all_authors = graphene.List(AuthorType) all_books = graphene.List(BookType) all_publishers = graphene.List(PublisherType) # Debug field (rawSql, parameters etc). debug = graphene.Field(DjangoDebug, name=&#39;__debug&#39;) def resolve_all_authors(self, args, context, info): return Author.objects.all() def resolve_all_books(self, args, context, info): return Book.objects.select_related(&#39;publisher&#39;).all() def resolve_all_publishers(self, args, context, info): return Publisher.objects.all() schema = graphene.Schema(query=Query) . Last but not least, the GraphQL URL must be added into the urls.py file. . # bookstore/urls.py from django.conf.urls import url from django.contrib import admin from graphene_django.views import GraphQLView from bookstore.schema import schema urlpatterns = [ url(r&#39;^admin/&#39;, admin.site.urls), url(r&#39;^graphql&#39;, GraphQLView.as_view(graphiql=True, schema=schema)), ] . You can now run python manage.py runserver and start using the API at http://localhost:8000/graphql. . Querying and debugging . GraphiQL provides a graphical interactive in-browser GraphQL IDE, including some features such as syntax highlighting, real-time error reporting, automatic query completion etc. . I will show some query examples, but you can learn more about querying at graphql.org/learn/queries/. . Given the following query, we can retrieve all books registered along with their authors. . { allBooks { title, authors { firstName, lastName } } } . And the response… . { &quot;data&quot;: { &quot;allBooks&quot;: [ { &quot;title&quot;: &quot;Resurrection&quot;, &quot;authors&quot;: [ { &quot;firstName&quot;: &quot;Leo&quot;, &quot;lastName&quot;: &quot;Tolstoy&quot; } ] }, { &quot;title&quot;: &quot;Childhood&quot;, &quot;authors&quot;: [ { &quot;firstName&quot;: &quot;Leo&quot;, &quot;lastName&quot;: &quot;Tolstoy&quot; } ] } ] } } . Using the __debug field you can get information about the actual SQL query. . { allAuthors {lastName} __debug { sql {rawSql, duration} } } . Response: . { &quot;data&quot;: { &quot;allAuthors&quot;: [ { &quot;lastName&quot;: &quot;King&quot; }, { &quot;lastName&quot;: &quot;Tolstoy&quot; }, { &quot;lastName&quot;: &quot;Gaiman&quot; }, { &quot;lastName&quot;: &quot;Pratchett&quot; } ], &quot;__debug&quot;: { &quot;sql&quot;: [ { &quot;rawSql&quot;: &quot;SELECT &quot;store_author &quot;. &quot;id &quot;, &quot;store_author &quot;. &quot;first_name &quot;, &quot;store_author &quot;. &quot;last_name &quot;, &quot;store_author &quot;. &quot;email &quot; FROM &quot;store_author &quot;&quot;, &quot;duration&quot;: 0.0009260177612304688 } ] } } } . All this code is on my Github. Please do fork it and make pull requests regarding any issues or improvements you may have with my code. .",
            "url": "https://joaorafaelm.github.io/notebook/graphql/django/2017/08/05/graphql-and-django-in-5-minutes.html",
            "relUrl": "/graphql/django/2017/08/05/graphql-and-django-in-5-minutes.html",
            "date": " 2017 Aug 05"
        }
        
    
  

  
  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://joaorafaelm.github.io/notebook/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
